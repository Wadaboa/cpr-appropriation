{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ec6af5",
   "metadata": {},
   "source": [
    "# CPR appropriation with policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba419115",
   "metadata": {},
   "source": [
    "This notebook contains actual Harvest trainings for each implemented policy gradient method. The environment in use is a custom implementation of Harvest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca81e4",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc8123",
   "metadata": {},
   "source": [
    "The cells down below install and import the necessary libraries to successfully run the notebook examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a2bf688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50c4b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r ../init/requirements.txt\n",
    "!pip install ../src/gym_cpr_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96a5b2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of gym_cpr_grid failed: Traceback (most recent call last):\n",
      "  File \"/Users/jobs/Github/cpr-appropriation/venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/jobs/Github/cpr-appropriation/venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 613, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 855, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"/Users/jobs/Github/cpr-appropriation/venv/lib/python3.9/site-packages/gym_cpr_grid/__init__.py\", line 3, in <module>\n",
      "    register(id=\"CPRGridEnv-v0\", entry_point=\"gym_cpr_grid.cpr_grid:CPRGridEnv\")\n",
      "  File \"/Users/jobs/Github/cpr-appropriation/venv/lib/python3.9/site-packages/gym/envs/registration.py\", line 142, in register\n",
      "    return registry.register(id, **kwargs)\n",
      "  File \"/Users/jobs/Github/cpr-appropriation/venv/lib/python3.9/site-packages/gym/envs/registration.py\", line 135, in register\n",
      "    raise error.Error('Cannot re-register id: {}'.format(id))\n",
      "gym.error.Error: Cannot re-register id: CPRGridEnv-v0\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from src import memory, models, policies\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5cabd3",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb50f2f",
   "metadata": {},
   "source": [
    "The cell down below defines the environment, along with common variables to be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb8048cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    'gym_cpr_grid:CPRGridEnv-v0', \n",
    "    n_agents=11, \n",
    "    grid_width=39, \n",
    "    grid_height=19,\n",
    "    tagging_ability=True,\n",
    "    gifting_mechanism=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "365ab924",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space_size = env.observation_space_size()\n",
    "action_space_size = env.action_space_size()\n",
    "epochs = 4000\n",
    "steps_per_epoch = 4000\n",
    "save_every = 500\n",
    "hidden_sizes = [32, 32]\n",
    "checkpoints_path = \"../checkpoints\"\n",
    "wandb_config = {\n",
    "    \"api_key\": open(\"../wandb_api_key_file\", \"r\").read().strip(),\n",
    "    \"project\": \"cpr-appropriation\",\n",
    "    \"entity\": \"wadaboa\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4277e979",
   "metadata": {},
   "source": [
    "## VPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac3d6ab",
   "metadata": {},
   "source": [
    "This section deals with training a set of Harvest agents using our custom Vanilla Policy Gradient implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "babe08a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-25 13:04:12.486 | INFO     | src.policies:train:103 - Epoch 1 / 4000\n",
      "2021-08-25 13:04:12.487 | INFO     | src.policies:train:110 - Episode 1\n",
      "2021-08-25 13:04:32.495 | DEBUG    | src.policies:execute_episode:270 - Early stopping, all agents done\n",
      "2021-08-25 13:04:32.497 | INFO     | src.policies:train:117 - Episode infos: {'efficiency': 281.1818181818182, 'equality': 0.9623783910889343, 'sustainability': 510.8224459964027, 'peace': 592.2727272727273}\n",
      "2021-08-25 13:04:32.498 | INFO     | src.policies:train:122 - Mean episode return: 281.1818181818182\n",
      "2021-08-25 13:04:32.498 | INFO     | src.policies:train:123 - Last 100 episodes mean return: 281.1818181818182\n",
      "2021-08-25 13:04:32.499 | WARNING  | src.policies:train:133 - The actual batch size is 11000, instead of 4000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pp/wfj1kcws58g7pxksj9k8zdhw0000gn/T/ipykernel_22958/3969307400.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvpg_baseline_nn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_space_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_softmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvpg_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVPGPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvpg_policy_nn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_nn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvpg_baseline_nn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m vpg_policy.train(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Github/cpr-appropriation/notebooks/../src/policies.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, steps_per_epoch, policy_lr, baseline_lr, discount, save_every, checkpoints_path, enable_wandb, wandb_config, max_episodes, std_returns, episodes_mean_return)\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0;34mf\"The actual batch size is {actual_batch_size}, instead of {steps_per_epoch}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 )\n\u001b[0;32m--> 136\u001b[0;31m             states, actions, old_log_probs, returns, _ = trajectories.tensorify(\n\u001b[0m\u001b[1;32m    137\u001b[0m                 \u001b[0mdiscount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             )\n",
      "\u001b[0;32m~/Github/cpr-appropriation/notebooks/../src/memory.py\u001b[0m in \u001b[0;36mtensorify\u001b[0;34m(self, discount)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mtrajectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_returns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_go\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_torch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             )\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mnext_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_torch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         return (\n",
      "\u001b[0;32m~/Github/cpr-appropriation/notebooks/../src/memory.py\u001b[0m in \u001b[0;36mget_next_states\u001b[0;34m(self, as_torch)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mas_torch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         )\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vpg_policy_nn = models.MLP(observation_space_size, hidden_sizes, action_space_size)\n",
    "vpg_baseline_nn = models.MLP(observation_space_size, hidden_sizes, 1, log_softmax=False)\n",
    "vpg_policy = policies.VPGPolicy(env, vpg_policy_nn, baseline_nn=vpg_baseline_nn)\n",
    "vpg_policy.train(\n",
    "    epochs,\n",
    "    steps_per_epoch,\n",
    "    enable_wandb=True,\n",
    "    wandb_config={**wandb_config, \"group\": \"VPG\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39957d7",
   "metadata": {},
   "source": [
    "## TRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff0e5f",
   "metadata": {},
   "source": [
    "This section deals with training a set of Harvest agents using our custom Trust Region Policy Optimization implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d6713",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1.0\n",
    "kl_target = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce99179",
   "metadata": {},
   "outputs": [],
   "source": [
    "trpo_policy_nn = models.MLP(observation_space_size, hidden_sizes, action_space_size)\n",
    "trpo_baseline_nn = models.MLP(observation_space_size, hidden_sizes, 1, log_softmax=False)\n",
    "trpo_policy = policies.TRPOPolicy(env, trpo_policy_nn, trpo_baseline_nn, beta=beta, kl_target=kl_target)\n",
    "trpo_policy.train(\n",
    "    epochs,\n",
    "    steps_per_epoch,\n",
    "    enable_wandb=True,\n",
    "    wandb_config={**wandb_config, \"group\": \"TRPO\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293840af",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5610cf7",
   "metadata": {},
   "source": [
    "This section deals with training a set of Harvest agents using our custom Proximal Policy Optimization implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1=1.0\n",
    "c2=0.01\n",
    "eps=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f87451",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_policy_nn = models.MLP(observation_space_size, hidden_sizes, action_space_size)\n",
    "ppo_baseline_nn = models.MLP(observation_space_size, hidden_sizes, 1, log_softmax=False)\n",
    "ppo_policy = policies.PPOPolicy(env, ppo_policy_nn, ppo_baseline_nn, c1=c1, c2=c2, eps=eps)\n",
    "ppo_policy.train(\n",
    "    epochs,\n",
    "    steps_per_epoch,\n",
    "    enable_wandb=True,\n",
    "    wandb_config={**wandb_config, \"group\": \"PPO\"}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
