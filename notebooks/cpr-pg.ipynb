{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e24b61",
   "metadata": {},
   "source": [
    "# CPR appropriation with policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8123d6",
   "metadata": {},
   "source": [
    "This notebook contains actual Harvest trainings for each implemented policy gradient method. The environment in use is a custom implementation of Harvest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359ae1fd",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3f3c6",
   "metadata": {},
   "source": [
    "The cells down below install and import the necessary libraries to successfully run the notebook examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dba4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee82033",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r ../init/requirements.txt\n",
    "!pip install ../src/gym_cpr_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "from src import memory, models, policies\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ddcded",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef0e00",
   "metadata": {},
   "source": [
    "The cell down below defines the environment, along with common variables to be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8048cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    'gym_cpr_grid:CPRGridEnv-v0', \n",
    "    n_agents=1, \n",
    "    grid_width=25, \n",
    "    grid_height=7,\n",
    "    tagging_ability=False,\n",
    "    gifting_mechanism=None,\n",
    "    initial_resource_probability=0.2,\n",
    "    fov_squares_front=21,\n",
    "    global_obs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365ab924",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space_size = env.observation_space_size()\n",
    "action_space_size = env.action_space_size()\n",
    "epochs = 1000\n",
    "steps_per_epoch = 1000\n",
    "minibatch_size = 100\n",
    "save_every = 200\n",
    "hidden_sizes = [32, 32]\n",
    "checkpoints_path = \"../checkpoints\"\n",
    "render_every = None\n",
    "std_returns = True\n",
    "std_advs = False\n",
    "clip_gradient_norm = None\n",
    "policy_lr = 3e-4\n",
    "baseline_lr = 1e-3\n",
    "wandb_config = {\n",
    "    \"api_key\": open(\"../wandb_api_key_file\", \"r\").read().strip(),\n",
    "    \"project\": \"cpr-appropriation\",\n",
    "    \"entity\": \"wadaboa\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec04f2",
   "metadata": {},
   "source": [
    "## VPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541b9cca",
   "metadata": {},
   "source": [
    "This section deals with training a set of Harvest agents using our custom Vanilla Policy Gradient implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d3a71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vpg_policy_nn = models.MLP(observation_space_size, hidden_sizes, action_space_size)\n",
    "vpg_baseline_nn = models.MLP(observation_space_size, hidden_sizes, 1, log_softmax=False)\n",
    "vpg_policy = policies.VPGPolicy(env, vpg_policy_nn, baseline_nn=vpg_baseline_nn)\n",
    "vpg_policy.train(\n",
    "    epochs,\n",
    "    steps_per_epoch,\n",
    "    minibatch_size,\n",
    "    enable_wandb=True,\n",
    "    wandb_config={**wandb_config, \"group\": \"VPG\"},\n",
    "    save_every=save_every,\n",
    "    checkpoints_path=checkpoints_path,\n",
    "    render_every=render_every,\n",
    "    std_returns=std_returns,\n",
    "    std_advs=std_advs,\n",
    "    clip_gradient_norm=clip_gradient_norm,\n",
    "    policy_lr=policy_lr,\n",
    "    baseline_lr=baseline_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f905e97",
   "metadata": {},
   "source": [
    "## TRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc84f4",
   "metadata": {},
   "source": [
    "This section deals with training a set of Harvest agents using our custom Trust Region Policy Optimization implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008057e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1.0\n",
    "kl_target = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5642d94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trpo_policy_nn = models.MLP(observation_space_size, hidden_sizes, action_space_size)\n",
    "trpo_baseline_nn = models.MLP(observation_space_size, hidden_sizes, 1, log_softmax=False)\n",
    "trpo_policy = policies.TRPOPolicy(env, trpo_policy_nn, trpo_baseline_nn, beta=beta, kl_target=kl_target)\n",
    "trpo_policy.train(\n",
    "    epochs,\n",
    "    steps_per_epoch,\n",
    "    minibatch_size,\n",
    "    enable_wandb=True,\n",
    "    wandb_config={**wandb_config, \"group\": \"TRPO\"},\n",
    "    save_every=save_every,\n",
    "    checkpoints_path=checkpoints_path,\n",
    "    render_every=render_every,\n",
    "    std_returns=std_returns,\n",
    "    std_advs=std_advs,\n",
    "    clip_gradient_norm=clip_gradient_norm,\n",
    "    policy_lr=policy_lr,\n",
    "    baseline_lr=baseline_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e8a3c",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf0a281",
   "metadata": {},
   "source": [
    "This section deals with training a set of Harvest agents using our custom Proximal Policy Optimization implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba6b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "beta = 0.01\n",
    "eps = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e359b6f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ppo_policy_nn = models.MLP(observation_space_size, hidden_sizes, action_space_size)\n",
    "ppo_baseline_nn = models.MLP(observation_space_size, hidden_sizes, 1, log_softmax=False)\n",
    "ppo_policy = policies.PPOPolicy(env, ppo_policy_nn, ppo_baseline_nn, alpha=alpha, beta=beta, eps=eps)\n",
    "ppo_policy.train(\n",
    "    epochs,\n",
    "    steps_per_epoch,\n",
    "    minibatch_size,\n",
    "    enable_wandb=True,\n",
    "    wandb_config={**wandb_config, \"group\": \"PPO\"},\n",
    "    save_every=save_every,\n",
    "    checkpoints_path=checkpoints_path,\n",
    "    render_every=render_every,\n",
    "    std_returns=std_returns,\n",
    "    std_advs=std_advs,\n",
    "    clip_gradient_norm=clip_gradient_norm,\n",
    "    policy_lr=policy_lr,\n",
    "    baseline_lr=baseline_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe05f592",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500b0348",
   "metadata": {},
   "source": [
    "In this section we are evaluating one of the trained models on the Harvest environment and visualizing results through custom rendering functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ppo_policy\n",
    "checkpoint = \"\"\n",
    "policy.load(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4caf1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.policy_nn.eval()\n",
    "policy.baseline_nn.eval()\n",
    "observations = env.reset()\n",
    "prev_actions = {h: None for h in range(env.n_agents)}\n",
    "for _ in range(env._max_episode_steps):\n",
    "    env.render()\n",
    "    action_dict = dict()\n",
    "    for agent_handle in range(env.n_agents):\n",
    "        legal_actions = env.get_legal_actions(agent_handle)\n",
    "        probs = np.exp(\n",
    "            ppo_policy.policy_nn(\n",
    "                torch.tensor(observations[agent_handle], dtype=torch.float32), \n",
    "                torch.tensor(legal_actions, dtype=torch.int64)\n",
    "            ).cpu().detach().numpy()\n",
    "        )\n",
    "        action = np.argmax(probs)\n",
    "        if prev_actions[agent_handle] == 0 and action == 1:\n",
    "            probs[1] = 0\n",
    "        if prev_actions[agent_handle] == 1 and action == 0:\n",
    "            probs[0] = 0\n",
    "        if prev_actions[agent_handle] == 2 and action == 3:\n",
    "            probs[3] = 0\n",
    "        if prev_actions[agent_handle] == 3 and action == 2:\n",
    "            probs[2] = 0\n",
    "        if prev_actions[agent_handle] == 4 and action == 5:\n",
    "            probs[5] = 0\n",
    "        if prev_actions[agent_handle] == 5 and action == 4:\n",
    "            probs[4] = 0\n",
    "        if prev_actions[agent_handle] == action:\n",
    "            probs[action] == 0\n",
    "        action = np.argmax(probs)\n",
    "        prev_actions[agent_handle] = action\n",
    "\n",
    "    print(env.agent_positions)\n",
    "    action_dict = {0: action}\n",
    "    print(action_dict)\n",
    "    observations, rewards, dones, infos = env.step(action_dict)\n",
    "    print(infos)\n",
    "    display.clear_output(wait=True)\n",
    "    #time.sleep(3)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
