{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7a1479f",
   "metadata": {},
   "source": [
    "# Cartpole tests with policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be42462",
   "metadata": {},
   "source": [
    "This notebook contains a simple test for each implemented policy gradient method. In order to test if they function properly, we rely on the [Cartpole](https://gym.openai.com/envs/CartPole-v0/) environment, provided out-of-the-box in OpenAI Gym. As stated in Gym's documentation, the problem is considered \"solved\" if the agent is able to obtain a mean return of 195 in the last 100 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14860860",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5ccae8",
   "metadata": {},
   "source": [
    "The cells down below install and import the necessary libraries to successfully run the notebook examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dce2ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48a7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r ../init/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e221fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from src import models, policies\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a94d40",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1749819c",
   "metadata": {},
   "source": [
    "The cell down below defines the environment, along with common variables to be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ff50867",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e9ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space_size = 4\n",
    "action_space_size = 2\n",
    "hidden_sizes = [32, 32]\n",
    "epochs = 800\n",
    "steps_per_epoch = 200\n",
    "minibatch_size = 100\n",
    "episodes_mean_return = 100\n",
    "wandb_config = {\n",
    "    \"api_key\": open(\"../wandb_api_key_file\", \"r\").read().strip(),\n",
    "    \"project\": \"cpr-appropriation\",\n",
    "    \"entity\": \"wadaboa\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b0b49",
   "metadata": {},
   "source": [
    "## VPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29665cb2",
   "metadata": {},
   "source": [
    "This section deals with training a Cartpole agent using our custom Vanilla Policy Gradient implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d831219c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:49.160 | DEBUG    | src.models:__init__:56 - Model summary: MLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=32, out_features=2, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (out): LogSoftmax(dim=-1)\n",
      ")\n",
      "2021-09-07 17:17:49.162 | DEBUG    | src.models:__init__:56 - Model summary: MLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (out): Identity()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1ra5ow1i) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 39839<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/Users/jobs/Github/cpr-appropriation/notebooks/wandb/run-20210907_171556-1ra5ow1i/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/Users/jobs/Github/cpr-appropriation/notebooks/wandb/run-20210907_171556-1ra5ow1i/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>0.03829</td></tr><tr><td>mean_return</td><td>200.0</td></tr><tr><td>_runtime</td><td>113</td></tr><tr><td>_timestamp</td><td>1631027869</td></tr><tr><td>_step</td><td>728</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>▆▄▄▃▂▃▂▃▃▆▂▂▃▂▂▂▂▂█▃▂▂▂▁▂▃▂▁▁▁▂▁▁▂▁▂▃▃▃▃</td></tr><tr><td>mean_return</td><td>▁▂▂▂▄▂▆▅█▇█▇████████████████▇████▅▆▄████</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">winter-donkey-88</strong>: <a href=\"https://wandb.ai/wadaboa/cpr-appropriation/runs/1ra5ow1i\" target=\"_blank\">https://wandb.ai/wadaboa/cpr-appropriation/runs/1ra5ow1i</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:1ra5ow1i). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">peach-salad-89</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/wadaboa/cpr-appropriation\" target=\"_blank\">https://wandb.ai/wadaboa/cpr-appropriation</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/wadaboa/cpr-appropriation/runs/fotj8n3u\" target=\"_blank\">https://wandb.ai/wadaboa/cpr-appropriation/runs/fotj8n3u</a><br/>\n",
       "                Run data is saved locally in <code>/Users/jobs/Github/cpr-appropriation/notebooks/wandb/run-20210907_171749-fotj8n3u</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:56.889 | INFO     | src.policies:train:123 - Epoch 1 / 800\n",
      "2021-09-07 17:17:56.889 | INFO     | src.policies:collect_trajectories:221 - Episode 1\n",
      "2021-09-07 17:17:56.897 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:56.898 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 34.0\n",
      "2021-09-07 17:17:56.898 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 34.0\n",
      "2021-09-07 17:17:56.899 | INFO     | src.policies:collect_trajectories:221 - Episode 2\n",
      "2021-09-07 17:17:56.904 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:56.904 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:17:56.905 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.5\n",
      "2021-09-07 17:17:56.905 | INFO     | src.policies:collect_trajectories:221 - Episode 3\n",
      "2021-09-07 17:17:56.911 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:56.912 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:17:56.912 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.333333333333332\n",
      "2021-09-07 17:17:56.913 | INFO     | src.policies:collect_trajectories:221 - Episode 4\n",
      "2021-09-07 17:17:56.920 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:56.921 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 32.0\n",
      "2021-09-07 17:17:56.921 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 27.0\n",
      "2021-09-07 17:17:56.922 | INFO     | src.policies:collect_trajectories:221 - Episode 5\n",
      "2021-09-07 17:17:56.931 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:56.932 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 45.0\n",
      "2021-09-07 17:17:56.933 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 30.6\n",
      "2021-09-07 17:17:56.933 | INFO     | src.policies:collect_trajectories:221 - Episode 6\n",
      "2021-09-07 17:17:56.939 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:56.940 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:17:56.940 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 29.5\n",
      "2021-09-07 17:17:56.940 | INFO     | src.policies:collect_trajectories:221 - Episode 7\n",
      "2021-09-07 17:17:56.950 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:56.951 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 47.0\n",
      "2021-09-07 17:17:56.952 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 32.0\n",
      "2021-09-07 17:17:56.952 | WARNING  | src.policies:train:144 - The actual batch size is 224, instead of 200\n",
      "2021-09-07 17:17:56.955 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:56.957 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.17567262053489685, 'baseline_loss': 1.2977690696716309, 'total_loss': 0.4732119143009186}\n",
      "2021-09-07 17:17:56.958 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.26033201813697815\n",
      "2021-09-07 17:17:56.958 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3840964138507843\n",
      "2021-09-07 17:17:56.960 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.26033201813697815\n",
      "2021-09-07 17:17:56.961 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3840964138507843\n",
      "2021-09-07 17:17:56.963 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:56.964 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1298218071460724, 'baseline_loss': 1.1797136068344116, 'total_loss': 0.4600349962711334}\n",
      "2021-09-07 17:17:56.965 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2257009744644165\n",
      "2021-09-07 17:17:56.966 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.36613357067108154\n",
      "2021-09-07 17:17:56.967 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2257009744644165\n",
      "2021-09-07 17:17:56.968 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.36613357067108154\n",
      "2021-09-07 17:17:56.970 | INFO     | src.policies:train:123 - Epoch 2 / 800\n",
      "2021-09-07 17:17:56.970 | INFO     | src.policies:collect_trajectories:221 - Episode 8\n",
      "2021-09-07 17:17:56.975 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:56.976 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:17:56.977 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.0\n",
      "2021-09-07 17:17:56.977 | INFO     | src.policies:collect_trajectories:221 - Episode 9\n",
      "2021-09-07 17:17:56.984 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:56.985 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:17:56.985 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.5\n",
      "2021-09-07 17:17:56.986 | INFO     | src.policies:collect_trajectories:221 - Episode 10\n",
      "2021-09-07 17:17:56.990 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:56.990 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:17:56.991 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.0\n",
      "2021-09-07 17:17:56.991 | INFO     | src.policies:collect_trajectories:221 - Episode 11\n",
      "2021-09-07 17:17:56.998 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:56.998 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 32.0\n",
      "2021-09-07 17:17:56.999 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.5\n",
      "2021-09-07 17:17:57.000 | INFO     | src.policies:collect_trajectories:221 - Episode 12\n",
      "2021-09-07 17:17:57.010 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.011 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:17:57.011 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.8\n",
      "2021-09-07 17:17:57.012 | INFO     | src.policies:collect_trajectories:221 - Episode 13\n",
      "2021-09-07 17:17:57.019 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.020 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:17:57.021 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.5\n",
      "2021-09-07 17:17:57.021 | INFO     | src.policies:collect_trajectories:221 - Episode 14\n",
      "2021-09-07 17:17:57.028 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.028 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 28.0\n",
      "2021-09-07 17:17:57.029 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.571428571428573\n",
      "2021-09-07 17:17:57.029 | INFO     | src.policies:collect_trajectories:221 - Episode 15\n",
      "2021-09-07 17:17:57.036 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.036 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 26.0\n",
      "2021-09-07 17:17:57.037 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.125\n",
      "2021-09-07 17:17:57.037 | INFO     | src.policies:collect_trajectories:221 - Episode 16\n",
      "2021-09-07 17:17:57.042 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:57.043 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:17:57.043 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.555555555555557\n",
      "2021-09-07 17:17:57.044 | INFO     | src.policies:collect_trajectories:221 - Episode 17\n",
      "2021-09-07 17:17:57.048 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.048 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 10.0\n",
      "2021-09-07 17:17:57.049 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.4\n",
      "2021-09-07 17:17:57.049 | WARNING  | src.policies:train:144 - The actual batch size is 204, instead of 200\n",
      "2021-09-07 17:17:57.053 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:57.054 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1984850913286209, 'baseline_loss': 1.553328275680542, 'total_loss': 0.5781790614128113}\n",
      "2021-09-07 17:17:57.055 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2549288272857666\n",
      "2021-09-07 17:17:57.056 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6651554703712463\n",
      "2021-09-07 17:17:57.057 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2549288272857666\n",
      "2021-09-07 17:17:57.058 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:17:57.060 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:57.061 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.17864175140857697, 'baseline_loss': 1.5712342262268066, 'total_loss': 0.6069753766059875}\n",
      "2021-09-07 17:17:57.063 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13561265170574188\n",
      "2021-09-07 17:17:57.064 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8261432647705078\n",
      "2021-09-07 17:17:57.065 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13561265170574188\n",
      "2021-09-07 17:17:57.066 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:17:57.068 | INFO     | src.policies:train:123 - Epoch 3 / 800\n",
      "2021-09-07 17:17:57.069 | INFO     | src.policies:collect_trajectories:221 - Episode 18\n",
      "2021-09-07 17:17:57.073 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.074 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:17:57.074 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.0\n",
      "2021-09-07 17:17:57.075 | INFO     | src.policies:collect_trajectories:221 - Episode 19\n",
      "2021-09-07 17:17:57.081 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.082 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:17:57.082 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.5\n",
      "2021-09-07 17:17:57.083 | INFO     | src.policies:collect_trajectories:221 - Episode 20\n",
      "2021-09-07 17:17:57.088 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.088 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 15.0\n",
      "2021-09-07 17:17:57.089 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.666666666666668\n",
      "2021-09-07 17:17:57.089 | INFO     | src.policies:collect_trajectories:221 - Episode 21\n",
      "2021-09-07 17:17:57.095 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.096 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 28.0\n",
      "2021-09-07 17:17:57.096 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.0\n",
      "2021-09-07 17:17:57.097 | INFO     | src.policies:collect_trajectories:221 - Episode 22\n",
      "2021-09-07 17:17:57.103 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.104 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:17:57.105 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.6\n",
      "2021-09-07 17:17:57.105 | INFO     | src.policies:collect_trajectories:221 - Episode 23\n",
      "2021-09-07 17:17:57.111 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.111 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:17:57.112 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.0\n",
      "2021-09-07 17:17:57.112 | INFO     | src.policies:collect_trajectories:221 - Episode 24\n",
      "2021-09-07 17:17:57.116 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.117 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:17:57.118 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.285714285714285\n",
      "2021-09-07 17:17:57.118 | INFO     | src.policies:collect_trajectories:221 - Episode 25\n",
      "2021-09-07 17:17:57.125 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.125 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:17:57.126 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.75\n",
      "2021-09-07 17:17:57.126 | INFO     | src.policies:collect_trajectories:221 - Episode 26\n",
      "2021-09-07 17:17:57.130 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.130 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:17:57.130 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.555555555555557\n",
      "2021-09-07 17:17:57.131 | INFO     | src.policies:collect_trajectories:221 - Episode 27\n",
      "2021-09-07 17:17:57.136 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.136 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:17:57.136 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.4\n",
      "2021-09-07 17:17:57.137 | WARNING  | src.policies:train:144 - The actual batch size is 204, instead of 200\n",
      "2021-09-07 17:17:57.141 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:57.143 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.14691193401813507, 'baseline_loss': 1.5459661483764648, 'total_loss': 0.6260711550712585}\n",
      "2021-09-07 17:17:57.144 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.599511981010437\n",
      "2021-09-07 17:17:57.145 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9251968264579773\n",
      "2021-09-07 17:17:57.146 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:17:57.147 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:17:57.148 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:57.149 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.16162066161632538, 'baseline_loss': 1.3879156112670898, 'total_loss': 0.5323371291160583}\n",
      "2021-09-07 17:17:57.150 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10196549445390701\n",
      "2021-09-07 17:17:57.151 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6863385438919067\n",
      "2021-09-07 17:17:57.152 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10196549445390701\n",
      "2021-09-07 17:17:57.153 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:57.155 | INFO     | src.policies:train:123 - Epoch 4 / 800\n",
      "2021-09-07 17:17:57.155 | INFO     | src.policies:collect_trajectories:221 - Episode 28\n",
      "2021-09-07 17:17:57.160 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.161 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:17:57.162 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.0\n",
      "2021-09-07 17:17:57.162 | INFO     | src.policies:collect_trajectories:221 - Episode 29\n",
      "2021-09-07 17:17:57.168 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.169 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:17:57.169 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.5\n",
      "2021-09-07 17:17:57.170 | INFO     | src.policies:collect_trajectories:221 - Episode 30\n",
      "2021-09-07 17:17:57.175 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.175 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:17:57.176 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.666666666666668\n",
      "2021-09-07 17:17:57.176 | INFO     | src.policies:collect_trajectories:221 - Episode 31\n",
      "2021-09-07 17:17:57.181 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.182 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:17:57.182 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.0\n",
      "2021-09-07 17:17:57.183 | INFO     | src.policies:collect_trajectories:221 - Episode 32\n",
      "2021-09-07 17:17:57.188 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.188 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:17:57.189 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.0\n",
      "2021-09-07 17:17:57.189 | INFO     | src.policies:collect_trajectories:221 - Episode 33\n",
      "2021-09-07 17:17:57.194 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.195 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:17:57.195 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.333333333333332\n",
      "2021-09-07 17:17:57.196 | INFO     | src.policies:collect_trajectories:221 - Episode 34\n",
      "2021-09-07 17:17:57.200 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.201 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:17:57.201 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.857142857142858\n",
      "2021-09-07 17:17:57.202 | INFO     | src.policies:collect_trajectories:221 - Episode 35\n",
      "2021-09-07 17:17:57.208 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.208 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:17:57.209 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.875\n",
      "2021-09-07 17:17:57.209 | INFO     | src.policies:collect_trajectories:221 - Episode 36\n",
      "2021-09-07 17:17:57.215 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.216 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 27.0\n",
      "2021-09-07 17:17:57.216 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.77777777777778\n",
      "2021-09-07 17:17:57.216 | INFO     | src.policies:collect_trajectories:221 - Episode 37\n",
      "2021-09-07 17:17:57.223 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.224 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:17:57.224 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.5\n",
      "2021-09-07 17:17:57.225 | INFO     | src.policies:collect_trajectories:221 - Episode 38\n",
      "2021-09-07 17:17:57.228 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.229 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:17:57.229 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.818181818181817\n",
      "2021-09-07 17:17:57.230 | WARNING  | src.policies:train:144 - The actual batch size is 207, instead of 200\n",
      "2021-09-07 17:17:57.233 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:57.234 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.16043119132518768, 'baseline_loss': 1.404262661933899, 'total_loss': 0.5417001247406006}\n",
      "2021-09-07 17:17:57.235 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09071327745914459\n",
      "2021-09-07 17:17:57.236 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6876624226570129\n",
      "2021-09-07 17:17:57.237 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09071327745914459\n",
      "2021-09-07 17:17:57.238 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:17:57.240 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:57.241 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2213454395532608, 'baseline_loss': 1.5581883192062378, 'total_loss': 0.5577487349510193}\n",
      "2021-09-07 17:17:57.242 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2703312933444977\n",
      "2021-09-07 17:17:57.243 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8638285994529724\n",
      "2021-09-07 17:17:57.245 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2703312933444977\n",
      "2021-09-07 17:17:57.245 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:17:57.247 | INFO     | src.policies:train:123 - Epoch 5 / 800\n",
      "2021-09-07 17:17:57.247 | INFO     | src.policies:collect_trajectories:221 - Episode 39\n",
      "2021-09-07 17:17:57.253 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.253 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 27.0\n",
      "2021-09-07 17:17:57.254 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 27.0\n",
      "2021-09-07 17:17:57.254 | INFO     | src.policies:collect_trajectories:221 - Episode 40\n",
      "2021-09-07 17:17:57.258 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.259 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:17:57.260 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.5\n",
      "2021-09-07 17:17:57.261 | INFO     | src.policies:collect_trajectories:221 - Episode 41\n",
      "2021-09-07 17:17:57.267 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.267 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:17:57.268 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.666666666666668\n",
      "2021-09-07 17:17:57.268 | INFO     | src.policies:collect_trajectories:221 - Episode 42\n",
      "2021-09-07 17:17:57.272 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.272 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:17:57.273 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.0\n",
      "2021-09-07 17:17:57.273 | INFO     | src.policies:collect_trajectories:221 - Episode 43\n",
      "2021-09-07 17:17:57.278 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:57.279 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:17:57.279 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.0\n",
      "2021-09-07 17:17:57.280 | INFO     | src.policies:collect_trajectories:221 - Episode 44\n",
      "2021-09-07 17:17:57.285 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.286 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:17:57.286 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.333333333333332\n",
      "2021-09-07 17:17:57.287 | INFO     | src.policies:collect_trajectories:221 - Episode 45\n",
      "2021-09-07 17:17:57.292 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.292 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:17:57.293 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.142857142857142\n",
      "2021-09-07 17:17:57.293 | INFO     | src.policies:collect_trajectories:221 - Episode 46\n",
      "2021-09-07 17:17:57.299 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.300 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 28.0\n",
      "2021-09-07 17:17:57.300 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.0\n",
      "2021-09-07 17:17:57.301 | INFO     | src.policies:collect_trajectories:221 - Episode 47\n",
      "2021-09-07 17:17:57.305 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.305 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:17:57.306 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.444444444444443\n",
      "2021-09-07 17:17:57.306 | INFO     | src.policies:collect_trajectories:221 - Episode 48\n",
      "2021-09-07 17:17:57.310 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.310 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:17:57.311 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.6\n",
      "2021-09-07 17:17:57.311 | WARNING  | src.policies:train:144 - The actual batch size is 206, instead of 200\n",
      "2021-09-07 17:17:57.314 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:57.315 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.072895348072052, 'baseline_loss': 1.1735508441925049, 'total_loss': 0.5138800740242004}\n",
      "2021-09-07 17:17:57.316 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08210945874452591\n",
      "2021-09-07 17:17:57.317 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6295064091682434\n",
      "2021-09-07 17:17:57.318 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08210945874452591\n",
      "2021-09-07 17:17:57.320 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:17:57.321 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:57.322 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.09182094782590866, 'baseline_loss': 1.2000725269317627, 'total_loss': 0.5082153081893921}\n",
      "2021-09-07 17:17:57.324 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1662529706954956\n",
      "2021-09-07 17:17:57.324 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8436146974563599\n",
      "2021-09-07 17:17:57.326 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1662529706954956\n",
      "2021-09-07 17:17:57.327 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:17:57.328 | INFO     | src.policies:train:123 - Epoch 6 / 800\n",
      "2021-09-07 17:17:57.328 | INFO     | src.policies:collect_trajectories:221 - Episode 49\n",
      "2021-09-07 17:17:57.332 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.333 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:17:57.333 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 16.0\n",
      "2021-09-07 17:17:57.333 | INFO     | src.policies:collect_trajectories:221 - Episode 50\n",
      "2021-09-07 17:17:57.338 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.338 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:17:57.339 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 16.5\n",
      "2021-09-07 17:17:57.340 | INFO     | src.policies:collect_trajectories:221 - Episode 51\n",
      "2021-09-07 17:17:57.345 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.346 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:17:57.347 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 17.333333333333332\n",
      "2021-09-07 17:17:57.347 | INFO     | src.policies:collect_trajectories:221 - Episode 52\n",
      "2021-09-07 17:17:57.352 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.353 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 26.0\n",
      "2021-09-07 17:17:57.353 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.5\n",
      "2021-09-07 17:17:57.354 | INFO     | src.policies:collect_trajectories:221 - Episode 53\n",
      "2021-09-07 17:17:57.361 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.362 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 40.0\n",
      "2021-09-07 17:17:57.363 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.6\n",
      "2021-09-07 17:17:57.363 | INFO     | src.policies:collect_trajectories:221 - Episode 54\n",
      "2021-09-07 17:17:57.367 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.368 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:17:57.368 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.0\n",
      "2021-09-07 17:17:57.369 | INFO     | src.policies:collect_trajectories:221 - Episode 55\n",
      "2021-09-07 17:17:57.374 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.375 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 22.0\n",
      "2021-09-07 17:17:57.375 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.0\n",
      "2021-09-07 17:17:57.375 | INFO     | src.policies:collect_trajectories:221 - Episode 56\n",
      "2021-09-07 17:17:57.379 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.380 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:17:57.381 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.25\n",
      "2021-09-07 17:17:57.382 | INFO     | src.policies:collect_trajectories:221 - Episode 57\n",
      "2021-09-07 17:17:57.386 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.387 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:17:57.387 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.0\n",
      "2021-09-07 17:17:57.388 | INFO     | src.policies:collect_trajectories:221 - Episode 58\n",
      "2021-09-07 17:17:57.482 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.482 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:17:57.483 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.7\n",
      "2021-09-07 17:17:57.483 | WARNING  | src.policies:train:144 - The actual batch size is 207, instead of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:57.488 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:57.489 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.17285577952861786, 'baseline_loss': 1.4730218648910522, 'total_loss': 0.5636551380157471}\n",
      "2021-09-07 17:17:57.490 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18745467066764832\n",
      "2021-09-07 17:17:57.491 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7886437177658081\n",
      "2021-09-07 17:17:57.492 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18745467066764832\n",
      "2021-09-07 17:17:57.493 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:17:57.495 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:57.496 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2402968555688858, 'baseline_loss': 1.5870705842971802, 'total_loss': 0.5532384514808655}\n",
      "2021-09-07 17:17:57.497 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2322685718536377\n",
      "2021-09-07 17:17:57.498 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0103459358215332\n",
      "2021-09-07 17:17:57.499 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2322685718536377\n",
      "2021-09-07 17:17:57.500 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:17:57.501 | INFO     | src.policies:train:123 - Epoch 7 / 800\n",
      "2021-09-07 17:17:57.502 | INFO     | src.policies:collect_trajectories:221 - Episode 59\n",
      "2021-09-07 17:17:57.506 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.507 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 10.0\n",
      "2021-09-07 17:17:57.507 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 10.0\n",
      "2021-09-07 17:17:57.508 | INFO     | src.policies:collect_trajectories:221 - Episode 60\n",
      "2021-09-07 17:17:57.511 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.512 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 10.0\n",
      "2021-09-07 17:17:57.512 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 10.0\n",
      "2021-09-07 17:17:57.512 | INFO     | src.policies:collect_trajectories:221 - Episode 61\n",
      "2021-09-07 17:17:57.517 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.518 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:17:57.518 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 12.333333333333334\n",
      "2021-09-07 17:17:57.518 | INFO     | src.policies:collect_trajectories:221 - Episode 62\n",
      "2021-09-07 17:17:57.527 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.527 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 31.0\n",
      "2021-09-07 17:17:57.528 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 17.0\n",
      "2021-09-07 17:17:57.529 | INFO     | src.policies:collect_trajectories:221 - Episode 63\n",
      "2021-09-07 17:17:57.540 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.541 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 63.0\n",
      "2021-09-07 17:17:57.541 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 26.2\n",
      "2021-09-07 17:17:57.542 | INFO     | src.policies:collect_trajectories:221 - Episode 64\n",
      "2021-09-07 17:17:57.547 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.548 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:17:57.548 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.166666666666668\n",
      "2021-09-07 17:17:57.549 | INFO     | src.policies:collect_trajectories:221 - Episode 65\n",
      "2021-09-07 17:17:57.561 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.562 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 61.0\n",
      "2021-09-07 17:17:57.562 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 29.428571428571427\n",
      "2021-09-07 17:17:57.563 | WARNING  | src.policies:train:144 - The actual batch size is 206, instead of 200\n",
      "2021-09-07 17:17:57.566 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:57.568 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.10911665111780167, 'baseline_loss': 1.0812450647354126, 'total_loss': 0.43150588870048523}\n",
      "2021-09-07 17:17:57.569 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11431492865085602\n",
      "2021-09-07 17:17:57.570 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8009454011917114\n",
      "2021-09-07 17:17:57.571 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11431492865085602\n",
      "2021-09-07 17:17:57.572 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:17:57.573 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:57.574 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.10984795540571213, 'baseline_loss': 1.155902624130249, 'total_loss': 0.4681033492088318}\n",
      "2021-09-07 17:17:57.575 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21569029986858368\n",
      "2021-09-07 17:17:57.576 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.736595094203949\n",
      "2021-09-07 17:17:57.578 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21569029986858368\n",
      "2021-09-07 17:17:57.579 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:17:57.581 | INFO     | src.policies:train:123 - Epoch 8 / 800\n",
      "2021-09-07 17:17:57.581 | INFO     | src.policies:collect_trajectories:221 - Episode 66\n",
      "2021-09-07 17:17:57.586 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.587 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 22.0\n",
      "2021-09-07 17:17:57.588 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.0\n",
      "2021-09-07 17:17:57.588 | INFO     | src.policies:collect_trajectories:221 - Episode 67\n",
      "2021-09-07 17:17:57.592 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.593 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:17:57.593 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.5\n",
      "2021-09-07 17:17:57.593 | INFO     | src.policies:collect_trajectories:221 - Episode 68\n",
      "2021-09-07 17:17:57.598 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.598 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:17:57.599 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.333333333333332\n",
      "2021-09-07 17:17:57.600 | INFO     | src.policies:collect_trajectories:221 - Episode 69\n",
      "2021-09-07 17:17:57.607 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.608 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 31.0\n",
      "2021-09-07 17:17:57.608 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.25\n",
      "2021-09-07 17:17:57.609 | INFO     | src.policies:collect_trajectories:221 - Episode 70\n",
      "2021-09-07 17:17:57.612 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:57.613 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:17:57.613 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.4\n",
      "2021-09-07 17:17:57.614 | INFO     | src.policies:collect_trajectories:221 - Episode 71\n",
      "2021-09-07 17:17:57.618 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.618 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:17:57.619 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.0\n",
      "2021-09-07 17:17:57.619 | INFO     | src.policies:collect_trajectories:221 - Episode 72\n",
      "2021-09-07 17:17:57.625 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.625 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:17:57.626 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.142857142857142\n",
      "2021-09-07 17:17:57.626 | INFO     | src.policies:collect_trajectories:221 - Episode 73\n",
      "2021-09-07 17:17:57.631 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.631 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:17:57.632 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.0\n",
      "2021-09-07 17:17:57.632 | INFO     | src.policies:collect_trajectories:221 - Episode 74\n",
      "2021-09-07 17:17:57.636 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.636 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:17:57.637 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.666666666666668\n",
      "2021-09-07 17:17:57.637 | INFO     | src.policies:collect_trajectories:221 - Episode 75\n",
      "2021-09-07 17:17:57.644 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.644 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:17:57.645 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.0\n",
      "2021-09-07 17:17:57.648 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:57.649 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.10932460427284241, 'baseline_loss': 1.1597824096679688, 'total_loss': 0.47056660056114197}\n",
      "2021-09-07 17:17:57.649 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13762371242046356\n",
      "2021-09-07 17:17:57.650 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7189208269119263\n",
      "2021-09-07 17:17:57.651 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13762371242046356\n",
      "2021-09-07 17:17:57.652 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:17:57.653 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:57.654 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.15620681643486023, 'baseline_loss': 1.2039052248001099, 'total_loss': 0.4457457959651947}\n",
      "2021-09-07 17:17:57.655 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11512799561023712\n",
      "2021-09-07 17:17:57.656 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6713036298751831\n",
      "2021-09-07 17:17:57.658 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11512799561023712\n",
      "2021-09-07 17:17:57.659 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:17:57.661 | INFO     | src.policies:train:123 - Epoch 9 / 800\n",
      "2021-09-07 17:17:57.662 | INFO     | src.policies:collect_trajectories:221 - Episode 76\n",
      "2021-09-07 17:17:57.667 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.667 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:17:57.668 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.0\n",
      "2021-09-07 17:17:57.668 | INFO     | src.policies:collect_trajectories:221 - Episode 77\n",
      "2021-09-07 17:17:57.678 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.678 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 51.0\n",
      "2021-09-07 17:17:57.679 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 35.0\n",
      "2021-09-07 17:17:57.680 | INFO     | src.policies:collect_trajectories:221 - Episode 78\n",
      "2021-09-07 17:17:57.689 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.690 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 37.0\n",
      "2021-09-07 17:17:57.690 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 35.666666666666664\n",
      "2021-09-07 17:17:57.690 | INFO     | src.policies:collect_trajectories:221 - Episode 79\n",
      "2021-09-07 17:17:57.698 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.699 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 27.0\n",
      "2021-09-07 17:17:57.699 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 33.5\n",
      "2021-09-07 17:17:57.700 | INFO     | src.policies:collect_trajectories:221 - Episode 80\n",
      "2021-09-07 17:17:57.707 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.708 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 26.0\n",
      "2021-09-07 17:17:57.708 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 32.0\n",
      "2021-09-07 17:17:57.709 | INFO     | src.policies:collect_trajectories:221 - Episode 81\n",
      "2021-09-07 17:17:57.714 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.714 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:17:57.715 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 30.166666666666668\n",
      "2021-09-07 17:17:57.715 | INFO     | src.policies:collect_trajectories:221 - Episode 82\n",
      "2021-09-07 17:17:57.718 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.719 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:17:57.720 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 27.428571428571427\n",
      "2021-09-07 17:17:57.720 | INFO     | src.policies:collect_trajectories:221 - Episode 83\n",
      "2021-09-07 17:17:57.725 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.726 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:17:57.727 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 26.0\n",
      "2021-09-07 17:17:57.727 | WARNING  | src.policies:train:144 - The actual batch size is 208, instead of 200\n",
      "2021-09-07 17:17:57.731 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:57.732 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.13520698249340057, 'baseline_loss': 1.2165013551712036, 'total_loss': 0.47304368019104004}\n",
      "2021-09-07 17:17:57.733 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07785740494728088\n",
      "2021-09-07 17:17:57.734 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7927244305610657\n",
      "2021-09-07 17:17:57.735 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07785740494728088\n",
      "2021-09-07 17:17:57.736 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:17:57.738 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:57.739 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.11859670281410217, 'baseline_loss': 1.0739201307296753, 'total_loss': 0.4183633625507355}\n",
      "2021-09-07 17:17:57.740 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18032552301883698\n",
      "2021-09-07 17:17:57.741 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8120718598365784\n",
      "2021-09-07 17:17:57.742 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18032552301883698\n",
      "2021-09-07 17:17:57.744 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:17:57.745 | INFO     | src.policies:train:123 - Epoch 10 / 800\n",
      "2021-09-07 17:17:57.746 | INFO     | src.policies:collect_trajectories:221 - Episode 84\n",
      "2021-09-07 17:17:57.751 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.752 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:17:57.752 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.0\n",
      "2021-09-07 17:17:57.753 | INFO     | src.policies:collect_trajectories:221 - Episode 85\n",
      "2021-09-07 17:17:57.768 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.768 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 83.0\n",
      "2021-09-07 17:17:57.769 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 53.5\n",
      "2021-09-07 17:17:57.769 | INFO     | src.policies:collect_trajectories:221 - Episode 86\n",
      "2021-09-07 17:17:57.779 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.779 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 46.0\n",
      "2021-09-07 17:17:57.780 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 51.0\n",
      "2021-09-07 17:17:57.780 | INFO     | src.policies:collect_trajectories:221 - Episode 87\n",
      "2021-09-07 17:17:57.785 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.786 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:17:57.786 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 42.25\n",
      "2021-09-07 17:17:57.787 | INFO     | src.policies:collect_trajectories:221 - Episode 88\n",
      "2021-09-07 17:17:57.792 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.792 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:17:57.793 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 38.0\n",
      "2021-09-07 17:17:57.793 | INFO     | src.policies:collect_trajectories:221 - Episode 89\n",
      "2021-09-07 17:17:57.799 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.800 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:17:57.800 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 35.833333333333336\n",
      "2021-09-07 17:17:57.801 | WARNING  | src.policies:train:144 - The actual batch size is 215, instead of 200\n",
      "2021-09-07 17:17:57.804 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:57.806 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.14702853560447693, 'baseline_loss': 1.1993088722229004, 'total_loss': 0.45262590050697327}\n",
      "2021-09-07 17:17:57.807 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17217491567134857\n",
      "2021-09-07 17:17:57.808 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7890628576278687\n",
      "2021-09-07 17:17:57.809 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17217491567134857\n",
      "2021-09-07 17:17:57.810 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:17:57.811 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:57.813 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.10800096392631531, 'baseline_loss': 1.0679059028625488, 'total_loss': 0.4259519875049591}\n",
      "2021-09-07 17:17:57.814 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10371693968772888\n",
      "2021-09-07 17:17:57.815 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8052491545677185\n",
      "2021-09-07 17:17:57.816 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10371693968772888\n",
      "2021-09-07 17:17:57.817 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:17:57.819 | INFO     | src.policies:train:123 - Epoch 11 / 800\n",
      "2021-09-07 17:17:57.819 | INFO     | src.policies:collect_trajectories:221 - Episode 90\n",
      "2021-09-07 17:17:57.825 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.825 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 22.0\n",
      "2021-09-07 17:17:57.826 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.0\n",
      "2021-09-07 17:17:57.826 | INFO     | src.policies:collect_trajectories:221 - Episode 91\n",
      "2021-09-07 17:17:57.832 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.833 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:17:57.833 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.0\n",
      "2021-09-07 17:17:57.833 | INFO     | src.policies:collect_trajectories:221 - Episode 92\n",
      "2021-09-07 17:17:57.843 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.844 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 50.0\n",
      "2021-09-07 17:17:57.845 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 32.0\n",
      "2021-09-07 17:17:57.846 | INFO     | src.policies:collect_trajectories:221 - Episode 93\n",
      "2021-09-07 17:17:57.850 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.850 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:17:57.851 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 27.5\n",
      "2021-09-07 17:17:57.851 | INFO     | src.policies:collect_trajectories:221 - Episode 94\n",
      "2021-09-07 17:17:57.857 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.858 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:17:57.858 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 26.6\n",
      "2021-09-07 17:17:57.859 | INFO     | src.policies:collect_trajectories:221 - Episode 95\n",
      "2021-09-07 17:17:57.865 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.865 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 26.0\n",
      "2021-09-07 17:17:57.866 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 26.5\n",
      "2021-09-07 17:17:57.867 | INFO     | src.policies:collect_trajectories:221 - Episode 96\n",
      "2021-09-07 17:17:57.871 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.872 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:17:57.872 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.571428571428573\n",
      "2021-09-07 17:17:57.872 | INFO     | src.policies:collect_trajectories:221 - Episode 97\n",
      "2021-09-07 17:17:57.876 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.877 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:57.877 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.0\n",
      "2021-09-07 17:17:57.877 | INFO     | src.policies:collect_trajectories:221 - Episode 98\n",
      "2021-09-07 17:17:57.882 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.882 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:17:57.883 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.0\n",
      "2021-09-07 17:17:57.883 | INFO     | src.policies:collect_trajectories:221 - Episode 99\n",
      "2021-09-07 17:17:57.887 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.888 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 10.0\n",
      "2021-09-07 17:17:57.888 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.8\n",
      "2021-09-07 17:17:57.889 | WARNING  | src.policies:train:144 - The actual batch size is 208, instead of 200\n",
      "2021-09-07 17:17:57.892 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:57.894 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.197603240609169, 'baseline_loss': 1.2424041032791138, 'total_loss': 0.4235988259315491}\n",
      "2021-09-07 17:17:57.895 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1524161398410797\n",
      "2021-09-07 17:17:57.896 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7569067478179932\n",
      "2021-09-07 17:17:57.897 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1524161398410797\n",
      "2021-09-07 17:17:57.898 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:17:57.899 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:57.901 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1831769347190857, 'baseline_loss': 1.1853082180023193, 'total_loss': 0.409477174282074}\n",
      "2021-09-07 17:17:57.902 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07384726405143738\n",
      "2021-09-07 17:17:57.903 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8100045919418335\n",
      "2021-09-07 17:17:57.904 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07384726405143738\n",
      "2021-09-07 17:17:57.906 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:17:57.908 | INFO     | src.policies:train:123 - Epoch 12 / 800\n",
      "2021-09-07 17:17:57.909 | INFO     | src.policies:collect_trajectories:221 - Episode 100\n",
      "2021-09-07 17:17:57.911 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.912 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:17:57.913 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 12.0\n",
      "2021-09-07 17:17:57.913 | INFO     | src.policies:collect_trajectories:221 - Episode 101\n",
      "2021-09-07 17:17:57.921 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.921 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 32.0\n",
      "2021-09-07 17:17:57.922 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.0\n",
      "2021-09-07 17:17:57.923 | INFO     | src.policies:collect_trajectories:221 - Episode 102\n",
      "2021-09-07 17:17:57.927 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.928 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 15.0\n",
      "2021-09-07 17:17:57.928 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.666666666666668\n",
      "2021-09-07 17:17:57.929 | INFO     | src.policies:collect_trajectories:221 - Episode 103\n",
      "2021-09-07 17:17:57.932 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.933 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:17:57.933 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 17.75\n",
      "2021-09-07 17:17:57.934 | INFO     | src.policies:collect_trajectories:221 - Episode 104\n",
      "2021-09-07 17:17:57.938 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.939 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:17:57.939 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.0\n",
      "2021-09-07 17:17:57.939 | INFO     | src.policies:collect_trajectories:221 - Episode 105\n",
      "2021-09-07 17:17:57.943 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.944 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:17:57.944 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 16.833333333333332\n",
      "2021-09-07 17:17:57.945 | INFO     | src.policies:collect_trajectories:221 - Episode 106\n",
      "2021-09-07 17:17:57.948 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.948 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:17:57.949 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 16.0\n",
      "2021-09-07 17:17:57.949 | INFO     | src.policies:collect_trajectories:221 - Episode 107\n",
      "2021-09-07 17:17:57.953 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.954 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:17:57.954 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 16.125\n",
      "2021-09-07 17:17:57.955 | INFO     | src.policies:collect_trajectories:221 - Episode 108\n",
      "2021-09-07 17:17:57.958 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.959 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:17:57.959 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 15.666666666666666\n",
      "2021-09-07 17:17:57.960 | INFO     | src.policies:collect_trajectories:221 - Episode 109\n",
      "2021-09-07 17:17:57.964 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.964 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:17:57.965 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 15.5\n",
      "2021-09-07 17:17:57.965 | INFO     | src.policies:collect_trajectories:221 - Episode 110\n",
      "2021-09-07 17:17:57.969 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.969 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:17:57.970 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 15.181818181818182\n",
      "2021-09-07 17:17:57.970 | INFO     | src.policies:collect_trajectories:221 - Episode 111\n",
      "2021-09-07 17:17:57.975 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.975 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:17:57.976 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 15.833333333333334\n",
      "2021-09-07 17:17:57.976 | INFO     | src.policies:collect_trajectories:221 - Episode 112\n",
      "2021-09-07 17:17:57.983 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:57.984 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 32.0\n",
      "2021-09-07 17:17:58.035 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 17.076923076923077\n",
      "2021-09-07 17:17:58.074 | WARNING  | src.policies:train:144 - The actual batch size is 222, instead of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:58.078 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:58.079 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.15498928725719452, 'baseline_loss': 1.1203265190124512, 'total_loss': 0.4051739573478699}\n",
      "2021-09-07 17:17:58.080 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10220279544591904\n",
      "2021-09-07 17:17:58.082 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.806701123714447\n",
      "2021-09-07 17:17:58.083 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10220279544591904\n",
      "2021-09-07 17:17:58.084 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:17:58.086 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:58.087 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.16340167820453644, 'baseline_loss': 1.1611295938491821, 'total_loss': 0.4171631336212158}\n",
      "2021-09-07 17:17:58.089 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11253401637077332\n",
      "2021-09-07 17:17:58.089 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8281474113464355\n",
      "2021-09-07 17:17:58.090 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11253401637077332\n",
      "2021-09-07 17:17:58.091 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:17:58.093 | INFO     | src.policies:train:123 - Epoch 13 / 800\n",
      "2021-09-07 17:17:58.093 | INFO     | src.policies:collect_trajectories:221 - Episode 113\n",
      "2021-09-07 17:17:58.097 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.098 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:17:58.098 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.0\n",
      "2021-09-07 17:17:58.098 | INFO     | src.policies:collect_trajectories:221 - Episode 114\n",
      "2021-09-07 17:17:58.103 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.104 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:17:58.104 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 16.0\n",
      "2021-09-07 17:17:58.104 | INFO     | src.policies:collect_trajectories:221 - Episode 115\n",
      "2021-09-07 17:17:58.108 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.109 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:17:58.109 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 14.666666666666666\n",
      "2021-09-07 17:17:58.110 | INFO     | src.policies:collect_trajectories:221 - Episode 116\n",
      "2021-09-07 17:17:58.117 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.118 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 37.0\n",
      "2021-09-07 17:17:58.119 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.25\n",
      "2021-09-07 17:17:58.119 | INFO     | src.policies:collect_trajectories:221 - Episode 117\n",
      "2021-09-07 17:17:58.127 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.128 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 27.0\n",
      "2021-09-07 17:17:58.128 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.6\n",
      "2021-09-07 17:17:58.129 | INFO     | src.policies:collect_trajectories:221 - Episode 118\n",
      "2021-09-07 17:17:58.135 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.135 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 28.0\n",
      "2021-09-07 17:17:58.136 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.666666666666668\n",
      "2021-09-07 17:17:58.136 | INFO     | src.policies:collect_trajectories:221 - Episode 119\n",
      "2021-09-07 17:17:58.141 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.142 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 15.0\n",
      "2021-09-07 17:17:58.142 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.571428571428573\n",
      "2021-09-07 17:17:58.143 | INFO     | src.policies:collect_trajectories:221 - Episode 120\n",
      "2021-09-07 17:17:58.148 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.149 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 26.0\n",
      "2021-09-07 17:17:58.149 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.125\n",
      "2021-09-07 17:17:58.150 | INFO     | src.policies:collect_trajectories:221 - Episode 121\n",
      "2021-09-07 17:17:58.157 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.158 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 26.0\n",
      "2021-09-07 17:17:58.158 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.555555555555557\n",
      "2021-09-07 17:17:58.159 | WARNING  | src.policies:train:144 - The actual batch size is 203, instead of 200\n",
      "2021-09-07 17:17:58.164 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:58.165 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1629982888698578, 'baseline_loss': 1.1335809230804443, 'total_loss': 0.4037921726703644}\n",
      "2021-09-07 17:17:58.166 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1881224662065506\n",
      "2021-09-07 17:17:58.167 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7903924584388733\n",
      "2021-09-07 17:17:58.169 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1881224662065506\n",
      "2021-09-07 17:17:58.170 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:17:58.171 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:58.173 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.17326736450195312, 'baseline_loss': 1.11879301071167, 'total_loss': 0.38612914085388184}\n",
      "2021-09-07 17:17:58.174 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12441854923963547\n",
      "2021-09-07 17:17:58.175 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7780370712280273\n",
      "2021-09-07 17:17:58.176 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12441854923963547\n",
      "2021-09-07 17:17:58.177 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:17:58.179 | INFO     | src.policies:train:123 - Epoch 14 / 800\n",
      "2021-09-07 17:17:58.180 | INFO     | src.policies:collect_trajectories:221 - Episode 122\n",
      "2021-09-07 17:17:58.187 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.188 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 38.0\n",
      "2021-09-07 17:17:58.188 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 38.0\n",
      "2021-09-07 17:17:58.189 | INFO     | src.policies:collect_trajectories:221 - Episode 123\n",
      "2021-09-07 17:17:58.194 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.194 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:17:58.195 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 30.5\n",
      "2021-09-07 17:17:58.195 | INFO     | src.policies:collect_trajectories:221 - Episode 124\n",
      "2021-09-07 17:17:58.198 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:58.199 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:17:58.200 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.333333333333332\n",
      "2021-09-07 17:17:58.200 | INFO     | src.policies:collect_trajectories:221 - Episode 125\n",
      "2021-09-07 17:17:58.206 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.207 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:17:58.207 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.75\n",
      "2021-09-07 17:17:58.208 | INFO     | src.policies:collect_trajectories:221 - Episode 126\n",
      "2021-09-07 17:17:58.212 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.213 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:17:58.213 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.8\n",
      "2021-09-07 17:17:58.213 | INFO     | src.policies:collect_trajectories:221 - Episode 127\n",
      "2021-09-07 17:17:58.218 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.218 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:17:58.219 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.0\n",
      "2021-09-07 17:17:58.219 | INFO     | src.policies:collect_trajectories:221 - Episode 128\n",
      "2021-09-07 17:17:58.224 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.225 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 15.0\n",
      "2021-09-07 17:17:58.226 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.142857142857142\n",
      "2021-09-07 17:17:58.227 | INFO     | src.policies:collect_trajectories:221 - Episode 129\n",
      "2021-09-07 17:17:58.233 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.234 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 31.0\n",
      "2021-09-07 17:17:58.234 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.5\n",
      "2021-09-07 17:17:58.235 | INFO     | src.policies:collect_trajectories:221 - Episode 130\n",
      "2021-09-07 17:17:58.240 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.240 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:17:58.241 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.0\n",
      "2021-09-07 17:17:58.241 | INFO     | src.policies:collect_trajectories:221 - Episode 131\n",
      "2021-09-07 17:17:58.245 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.246 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:17:58.246 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.2\n",
      "2021-09-07 17:17:58.247 | WARNING  | src.policies:train:144 - The actual batch size is 202, instead of 200\n",
      "2021-09-07 17:17:58.250 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:58.251 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.17106640338897705, 'baseline_loss': 1.1506798267364502, 'total_loss': 0.40427350997924805}\n",
      "2021-09-07 17:17:58.252 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.04052986577153206\n",
      "2021-09-07 17:17:58.252 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8165451288223267\n",
      "2021-09-07 17:17:58.253 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.04052986577153206\n",
      "2021-09-07 17:17:58.254 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:17:58.256 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:58.257 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1839333474636078, 'baseline_loss': 1.1162102222442627, 'total_loss': 0.37417176365852356}\n",
      "2021-09-07 17:17:58.258 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.05577799677848816\n",
      "2021-09-07 17:17:58.259 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7793018817901611\n",
      "2021-09-07 17:17:58.260 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.05577799677848816\n",
      "2021-09-07 17:17:58.261 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:17:58.263 | INFO     | src.policies:train:123 - Epoch 15 / 800\n",
      "2021-09-07 17:17:58.264 | INFO     | src.policies:collect_trajectories:221 - Episode 132\n",
      "2021-09-07 17:17:58.268 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.268 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:17:58.269 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.0\n",
      "2021-09-07 17:17:58.269 | INFO     | src.policies:collect_trajectories:221 - Episode 133\n",
      "2021-09-07 17:17:58.273 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.274 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:17:58.274 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.5\n",
      "2021-09-07 17:17:58.274 | INFO     | src.policies:collect_trajectories:221 - Episode 134\n",
      "2021-09-07 17:17:58.279 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.279 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:17:58.280 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.333333333333332\n",
      "2021-09-07 17:17:58.280 | INFO     | src.policies:collect_trajectories:221 - Episode 135\n",
      "2021-09-07 17:17:58.286 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.287 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:17:58.287 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.5\n",
      "2021-09-07 17:17:58.288 | INFO     | src.policies:collect_trajectories:221 - Episode 136\n",
      "2021-09-07 17:17:58.296 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.296 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 42.0\n",
      "2021-09-07 17:17:58.297 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.0\n",
      "2021-09-07 17:17:58.297 | INFO     | src.policies:collect_trajectories:221 - Episode 137\n",
      "2021-09-07 17:17:58.303 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.304 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:17:58.304 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.166666666666668\n",
      "2021-09-07 17:17:58.304 | INFO     | src.policies:collect_trajectories:221 - Episode 138\n",
      "2021-09-07 17:17:58.310 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.310 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:17:58.311 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.142857142857142\n",
      "2021-09-07 17:17:58.311 | INFO     | src.policies:collect_trajectories:221 - Episode 139\n",
      "2021-09-07 17:17:58.315 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.316 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:17:58.316 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:58.316 | INFO     | src.policies:collect_trajectories:221 - Episode 140\n",
      "2021-09-07 17:17:58.323 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.324 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 29.0\n",
      "2021-09-07 17:17:58.325 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.11111111111111\n",
      "2021-09-07 17:17:58.325 | WARNING  | src.policies:train:144 - The actual batch size is 208, instead of 200\n",
      "2021-09-07 17:17:58.328 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:58.330 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1983831524848938, 'baseline_loss': 1.1767024993896484, 'total_loss': 0.3899680972099304}\n",
      "2021-09-07 17:17:58.330 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.04361369088292122\n",
      "2021-09-07 17:17:58.331 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8305467963218689\n",
      "2021-09-07 17:17:58.332 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.04361369088292122\n",
      "2021-09-07 17:17:58.333 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:17:58.335 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:58.336 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.21711678802967072, 'baseline_loss': 1.1698966026306152, 'total_loss': 0.3678315281867981}\n",
      "2021-09-07 17:17:58.337 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.27056702971458435\n",
      "2021-09-07 17:17:58.337 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7853307127952576\n",
      "2021-09-07 17:17:58.338 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.27056702971458435\n",
      "2021-09-07 17:17:58.340 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:17:58.342 | INFO     | src.policies:train:123 - Epoch 16 / 800\n",
      "2021-09-07 17:17:58.342 | INFO     | src.policies:collect_trajectories:221 - Episode 141\n",
      "2021-09-07 17:17:58.345 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.346 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:17:58.346 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 12.0\n",
      "2021-09-07 17:17:58.347 | INFO     | src.policies:collect_trajectories:221 - Episode 142\n",
      "2021-09-07 17:17:58.350 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.351 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:17:58.351 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 13.0\n",
      "2021-09-07 17:17:58.352 | INFO     | src.policies:collect_trajectories:221 - Episode 143\n",
      "2021-09-07 17:17:58.356 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.357 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:17:58.357 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 15.666666666666666\n",
      "2021-09-07 17:17:58.358 | INFO     | src.policies:collect_trajectories:221 - Episode 144\n",
      "2021-09-07 17:17:58.363 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.363 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:17:58.364 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 15.25\n",
      "2021-09-07 17:17:58.364 | INFO     | src.policies:collect_trajectories:221 - Episode 145\n",
      "2021-09-07 17:17:58.369 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.370 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:17:58.370 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 17.0\n",
      "2021-09-07 17:17:58.371 | INFO     | src.policies:collect_trajectories:221 - Episode 146\n",
      "2021-09-07 17:17:58.376 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.376 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:17:58.377 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.0\n",
      "2021-09-07 17:17:58.377 | INFO     | src.policies:collect_trajectories:221 - Episode 147\n",
      "2021-09-07 17:17:58.382 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.382 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:17:58.383 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 17.428571428571427\n",
      "2021-09-07 17:17:58.383 | INFO     | src.policies:collect_trajectories:221 - Episode 148\n",
      "2021-09-07 17:17:58.389 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.389 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:17:58.389 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.375\n",
      "2021-09-07 17:17:58.390 | INFO     | src.policies:collect_trajectories:221 - Episode 149\n",
      "2021-09-07 17:17:58.395 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.395 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:17:58.396 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.666666666666668\n",
      "2021-09-07 17:17:58.396 | INFO     | src.policies:collect_trajectories:221 - Episode 150\n",
      "2021-09-07 17:17:58.404 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.405 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 43.0\n",
      "2021-09-07 17:17:58.405 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.1\n",
      "2021-09-07 17:17:58.406 | WARNING  | src.policies:train:144 - The actual batch size is 211, instead of 200\n",
      "2021-09-07 17:17:58.409 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:58.411 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20508445799350739, 'baseline_loss': 1.1632877588272095, 'total_loss': 0.37655943632125854}\n",
      "2021-09-07 17:17:58.412 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.0907507911324501\n",
      "2021-09-07 17:17:58.413 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7598833441734314\n",
      "2021-09-07 17:17:58.414 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.0907507911324501\n",
      "2021-09-07 17:17:58.415 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:17:58.416 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:58.417 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2155662178993225, 'baseline_loss': 1.1935855150222778, 'total_loss': 0.3812265396118164}\n",
      "2021-09-07 17:17:58.418 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.04561663046479225\n",
      "2021-09-07 17:17:58.420 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7834697961807251\n",
      "2021-09-07 17:17:58.421 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.04561663046479225\n",
      "2021-09-07 17:17:58.422 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:17:58.424 | INFO     | src.policies:train:123 - Epoch 17 / 800\n",
      "2021-09-07 17:17:58.425 | INFO     | src.policies:collect_trajectories:221 - Episode 151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:58.428 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.428 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:17:58.429 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 11.0\n",
      "2021-09-07 17:17:58.429 | INFO     | src.policies:collect_trajectories:221 - Episode 152\n",
      "2021-09-07 17:17:58.432 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.433 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:17:58.433 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 11.5\n",
      "2021-09-07 17:17:58.434 | INFO     | src.policies:collect_trajectories:221 - Episode 153\n",
      "2021-09-07 17:17:58.437 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.438 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:17:58.438 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 11.666666666666666\n",
      "2021-09-07 17:17:58.439 | INFO     | src.policies:collect_trajectories:221 - Episode 154\n",
      "2021-09-07 17:17:58.447 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.447 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 31.0\n",
      "2021-09-07 17:17:58.448 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 16.5\n",
      "2021-09-07 17:17:58.448 | INFO     | src.policies:collect_trajectories:221 - Episode 155\n",
      "2021-09-07 17:17:58.456 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.457 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 38.0\n",
      "2021-09-07 17:17:58.458 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.8\n",
      "2021-09-07 17:17:58.458 | INFO     | src.policies:collect_trajectories:221 - Episode 156\n",
      "2021-09-07 17:17:58.463 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.464 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:17:58.464 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.666666666666668\n",
      "2021-09-07 17:17:58.464 | INFO     | src.policies:collect_trajectories:221 - Episode 157\n",
      "2021-09-07 17:17:58.469 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.470 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 22.0\n",
      "2021-09-07 17:17:58.470 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.0\n",
      "2021-09-07 17:17:58.471 | INFO     | src.policies:collect_trajectories:221 - Episode 158\n",
      "2021-09-07 17:17:58.475 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.475 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 15.0\n",
      "2021-09-07 17:17:58.476 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.375\n",
      "2021-09-07 17:17:58.476 | INFO     | src.policies:collect_trajectories:221 - Episode 159\n",
      "2021-09-07 17:17:58.481 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.482 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 15.0\n",
      "2021-09-07 17:17:58.483 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.88888888888889\n",
      "2021-09-07 17:17:58.483 | INFO     | src.policies:collect_trajectories:221 - Episode 160\n",
      "2021-09-07 17:17:58.489 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.490 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 28.0\n",
      "2021-09-07 17:17:58.490 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.8\n",
      "2021-09-07 17:17:58.490 | INFO     | src.policies:collect_trajectories:221 - Episode 161\n",
      "2021-09-07 17:17:58.494 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.494 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:17:58.495 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.09090909090909\n",
      "2021-09-07 17:17:58.495 | WARNING  | src.policies:train:144 - The actual batch size is 210, instead of 200\n",
      "2021-09-07 17:17:58.499 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:58.501 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23289859294891357, 'baseline_loss': 1.1889127492904663, 'total_loss': 0.3615577816963196}\n",
      "2021-09-07 17:17:58.503 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1692403107881546\n",
      "2021-09-07 17:17:58.504 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7441222667694092\n",
      "2021-09-07 17:17:58.505 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1692403107881546\n",
      "2021-09-07 17:17:58.507 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:17:58.508 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:58.509 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23228755593299866, 'baseline_loss': 1.1894060373306274, 'total_loss': 0.36241546273231506}\n",
      "2021-09-07 17:17:58.510 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.055266428738832474\n",
      "2021-09-07 17:17:58.511 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7371801137924194\n",
      "2021-09-07 17:17:58.512 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.055266428738832474\n",
      "2021-09-07 17:17:58.514 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:17:58.515 | INFO     | src.policies:train:123 - Epoch 18 / 800\n",
      "2021-09-07 17:17:58.515 | INFO     | src.policies:collect_trajectories:221 - Episode 162\n",
      "2021-09-07 17:17:58.521 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.522 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:17:58.522 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.0\n",
      "2021-09-07 17:17:58.523 | INFO     | src.policies:collect_trajectories:221 - Episode 163\n",
      "2021-09-07 17:17:58.527 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.527 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 10.0\n",
      "2021-09-07 17:17:58.528 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 15.5\n",
      "2021-09-07 17:17:58.528 | INFO     | src.policies:collect_trajectories:221 - Episode 164\n",
      "2021-09-07 17:17:58.534 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.534 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:17:58.535 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.666666666666668\n",
      "2021-09-07 17:17:58.535 | INFO     | src.policies:collect_trajectories:221 - Episode 165\n",
      "2021-09-07 17:17:58.541 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.542 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:17:58.542 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.75\n",
      "2021-09-07 17:17:58.543 | INFO     | src.policies:collect_trajectories:221 - Episode 166\n",
      "2021-09-07 17:17:58.548 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.548 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:58.548 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.8\n",
      "2021-09-07 17:17:58.549 | INFO     | src.policies:collect_trajectories:221 - Episode 167\n",
      "2021-09-07 17:17:58.555 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.556 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 33.0\n",
      "2021-09-07 17:17:58.556 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.0\n",
      "2021-09-07 17:17:58.557 | INFO     | src.policies:collect_trajectories:221 - Episode 168\n",
      "2021-09-07 17:17:58.568 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.569 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 56.0\n",
      "2021-09-07 17:17:58.569 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 26.857142857142858\n",
      "2021-09-07 17:17:58.569 | INFO     | src.policies:collect_trajectories:221 - Episode 169\n",
      "2021-09-07 17:17:58.573 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.574 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 15.0\n",
      "2021-09-07 17:17:58.574 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.375\n",
      "2021-09-07 17:17:58.575 | WARNING  | src.policies:train:144 - The actual batch size is 203, instead of 200\n",
      "2021-09-07 17:17:58.754 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:58.779 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20113323628902435, 'baseline_loss': 1.0607322454452515, 'total_loss': 0.3292328715324402}\n",
      "2021-09-07 17:17:58.780 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.06757985055446625\n",
      "2021-09-07 17:17:58.781 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7353671193122864\n",
      "2021-09-07 17:17:58.782 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.06757985055446625\n",
      "2021-09-07 17:17:58.783 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:17:58.785 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:58.786 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.21156302094459534, 'baseline_loss': 1.1154941320419312, 'total_loss': 0.34618404507637024}\n",
      "2021-09-07 17:17:58.787 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09039134532213211\n",
      "2021-09-07 17:17:58.788 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7373865842819214\n",
      "2021-09-07 17:17:58.789 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09039134532213211\n",
      "2021-09-07 17:17:58.790 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:17:58.791 | INFO     | src.policies:train:123 - Epoch 19 / 800\n",
      "2021-09-07 17:17:58.791 | INFO     | src.policies:collect_trajectories:221 - Episode 170\n",
      "2021-09-07 17:17:58.800 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.800 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 46.0\n",
      "2021-09-07 17:17:58.801 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 46.0\n",
      "2021-09-07 17:17:58.801 | INFO     | src.policies:collect_trajectories:221 - Episode 171\n",
      "2021-09-07 17:17:58.809 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.810 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 34.0\n",
      "2021-09-07 17:17:58.810 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 40.0\n",
      "2021-09-07 17:17:58.811 | INFO     | src.policies:collect_trajectories:221 - Episode 172\n",
      "2021-09-07 17:17:58.819 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.820 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 42.0\n",
      "2021-09-07 17:17:58.820 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 40.666666666666664\n",
      "2021-09-07 17:17:58.821 | INFO     | src.policies:collect_trajectories:221 - Episode 173\n",
      "2021-09-07 17:17:58.826 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.826 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:17:58.827 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 33.75\n",
      "2021-09-07 17:17:58.827 | INFO     | src.policies:collect_trajectories:221 - Episode 174\n",
      "2021-09-07 17:17:58.832 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.832 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:17:58.832 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 29.4\n",
      "2021-09-07 17:17:58.833 | INFO     | src.policies:collect_trajectories:221 - Episode 175\n",
      "2021-09-07 17:17:58.839 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.840 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 33.0\n",
      "2021-09-07 17:17:58.841 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 30.0\n",
      "2021-09-07 17:17:58.841 | INFO     | src.policies:collect_trajectories:221 - Episode 176\n",
      "2021-09-07 17:17:58.847 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.847 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 22.0\n",
      "2021-09-07 17:17:58.848 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 28.857142857142858\n",
      "2021-09-07 17:17:58.849 | WARNING  | src.policies:train:144 - The actual batch size is 202, instead of 200\n",
      "2021-09-07 17:17:58.852 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:58.853 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.21992698311805725, 'baseline_loss': 1.0875623226165771, 'total_loss': 0.3238541781902313}\n",
      "2021-09-07 17:17:58.854 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.124711774289608\n",
      "2021-09-07 17:17:58.855 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7306958436965942\n",
      "2021-09-07 17:17:58.856 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.124711774289608\n",
      "2021-09-07 17:17:58.858 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:17:58.859 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:58.860 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.22287026047706604, 'baseline_loss': 1.110325813293457, 'total_loss': 0.3322926461696625}\n",
      "2021-09-07 17:17:58.861 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08363111317157745\n",
      "2021-09-07 17:17:58.862 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6832490563392639\n",
      "2021-09-07 17:17:58.863 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08363111317157745\n",
      "2021-09-07 17:17:58.865 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:17:58.867 | INFO     | src.policies:train:123 - Epoch 20 / 800\n",
      "2021-09-07 17:17:58.867 | INFO     | src.policies:collect_trajectories:221 - Episode 177\n",
      "2021-09-07 17:17:58.872 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.872 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 22.0\n",
      "2021-09-07 17:17:58.873 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:58.873 | INFO     | src.policies:collect_trajectories:221 - Episode 178\n",
      "2021-09-07 17:17:58.880 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.880 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 31.0\n",
      "2021-09-07 17:17:58.881 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 26.5\n",
      "2021-09-07 17:17:58.881 | INFO     | src.policies:collect_trajectories:221 - Episode 179\n",
      "2021-09-07 17:17:58.885 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:58.886 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:17:58.887 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.333333333333332\n",
      "2021-09-07 17:17:58.887 | INFO     | src.policies:collect_trajectories:221 - Episode 180\n",
      "2021-09-07 17:17:59.032 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.033 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 61.0\n",
      "2021-09-07 17:17:59.033 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 32.0\n",
      "2021-09-07 17:17:59.034 | INFO     | src.policies:collect_trajectories:221 - Episode 181\n",
      "2021-09-07 17:17:59.043 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.044 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 30.0\n",
      "2021-09-07 17:17:59.044 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 31.6\n",
      "2021-09-07 17:17:59.045 | INFO     | src.policies:collect_trajectories:221 - Episode 182\n",
      "2021-09-07 17:17:59.049 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.050 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:17:59.050 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 29.0\n",
      "2021-09-07 17:17:59.050 | INFO     | src.policies:collect_trajectories:221 - Episode 183\n",
      "2021-09-07 17:17:59.055 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.055 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:17:59.056 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 27.285714285714285\n",
      "2021-09-07 17:17:59.056 | INFO     | src.policies:collect_trajectories:221 - Episode 184\n",
      "2021-09-07 17:17:59.063 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.063 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:17:59.064 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 26.25\n",
      "2021-09-07 17:17:59.064 | WARNING  | src.policies:train:144 - The actual batch size is 210, instead of 200\n",
      "2021-09-07 17:17:59.067 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:59.069 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24018503725528717, 'baseline_loss': 1.1028116941452026, 'total_loss': 0.31122082471847534}\n",
      "2021-09-07 17:17:59.070 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14857038855552673\n",
      "2021-09-07 17:17:59.071 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6892991662025452\n",
      "2021-09-07 17:17:59.072 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14857038855552673\n",
      "2021-09-07 17:17:59.073 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:17:59.074 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:59.075 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2298126220703125, 'baseline_loss': 1.1087872982025146, 'total_loss': 0.3245810270309448}\n",
      "2021-09-07 17:17:59.076 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.06439366936683655\n",
      "2021-09-07 17:17:59.076 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6401573419570923\n",
      "2021-09-07 17:17:59.078 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.06439366936683655\n",
      "2021-09-07 17:17:59.079 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:17:59.082 | INFO     | src.policies:train:123 - Epoch 21 / 800\n",
      "2021-09-07 17:17:59.083 | INFO     | src.policies:collect_trajectories:221 - Episode 185\n",
      "2021-09-07 17:17:59.091 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.092 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 46.0\n",
      "2021-09-07 17:17:59.092 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 46.0\n",
      "2021-09-07 17:17:59.093 | INFO     | src.policies:collect_trajectories:221 - Episode 186\n",
      "2021-09-07 17:17:59.100 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.101 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 27.0\n",
      "2021-09-07 17:17:59.102 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 36.5\n",
      "2021-09-07 17:17:59.103 | INFO     | src.policies:collect_trajectories:221 - Episode 187\n",
      "2021-09-07 17:17:59.108 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.109 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:17:59.109 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 32.0\n",
      "2021-09-07 17:17:59.110 | INFO     | src.policies:collect_trajectories:221 - Episode 188\n",
      "2021-09-07 17:17:59.115 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.116 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 29.0\n",
      "2021-09-07 17:17:59.116 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 31.25\n",
      "2021-09-07 17:17:59.117 | INFO     | src.policies:collect_trajectories:221 - Episode 189\n",
      "2021-09-07 17:17:59.122 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.123 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:17:59.123 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 27.2\n",
      "2021-09-07 17:17:59.124 | INFO     | src.policies:collect_trajectories:221 - Episode 190\n",
      "2021-09-07 17:17:59.130 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.130 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 27.0\n",
      "2021-09-07 17:17:59.131 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 27.166666666666668\n",
      "2021-09-07 17:17:59.131 | INFO     | src.policies:collect_trajectories:221 - Episode 191\n",
      "2021-09-07 17:17:59.138 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.139 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 35.0\n",
      "2021-09-07 17:17:59.140 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 28.285714285714285\n",
      "2021-09-07 17:17:59.141 | INFO     | src.policies:collect_trajectories:221 - Episode 192\n",
      "2021-09-07 17:17:59.148 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.148 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:17:59.149 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 27.875\n",
      "2021-09-07 17:17:59.150 | WARNING  | src.policies:train:144 - The actual batch size is 223, instead of 200\n",
      "2021-09-07 17:17:59.153 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:59.154 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.25347790122032166, 'baseline_loss': 1.1516668796539307, 'total_loss': 0.3223555386066437}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:59.155 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1210079938173294\n",
      "2021-09-07 17:17:59.156 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7229351997375488\n",
      "2021-09-07 17:17:59.157 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1210079938173294\n",
      "2021-09-07 17:17:59.158 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:17:59.160 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:59.162 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23609676957130432, 'baseline_loss': 1.1550917625427246, 'total_loss': 0.341449111700058}\n",
      "2021-09-07 17:17:59.163 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09787659347057343\n",
      "2021-09-07 17:17:59.164 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7556661367416382\n",
      "2021-09-07 17:17:59.165 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09787659347057343\n",
      "2021-09-07 17:17:59.167 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:17:59.168 | INFO     | src.policies:train:123 - Epoch 22 / 800\n",
      "2021-09-07 17:17:59.169 | INFO     | src.policies:collect_trajectories:221 - Episode 193\n",
      "2021-09-07 17:17:59.172 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.173 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:17:59.173 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 14.0\n",
      "2021-09-07 17:17:59.174 | INFO     | src.policies:collect_trajectories:221 - Episode 194\n",
      "2021-09-07 17:17:59.177 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.177 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 10.0\n",
      "2021-09-07 17:17:59.178 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 12.0\n",
      "2021-09-07 17:17:59.178 | INFO     | src.policies:collect_trajectories:221 - Episode 195\n",
      "2021-09-07 17:17:59.183 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.183 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:17:59.184 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 12.333333333333334\n",
      "2021-09-07 17:17:59.184 | INFO     | src.policies:collect_trajectories:221 - Episode 196\n",
      "2021-09-07 17:17:59.190 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.190 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 29.0\n",
      "2021-09-07 17:17:59.191 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 16.5\n",
      "2021-09-07 17:17:59.191 | INFO     | src.policies:collect_trajectories:221 - Episode 197\n",
      "2021-09-07 17:17:59.195 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.196 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:17:59.196 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 15.8\n",
      "2021-09-07 17:17:59.196 | INFO     | src.policies:collect_trajectories:221 - Episode 198\n",
      "2021-09-07 17:17:59.210 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.211 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 73.0\n",
      "2021-09-07 17:17:59.211 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.333333333333332\n",
      "2021-09-07 17:17:59.211 | INFO     | src.policies:collect_trajectories:221 - Episode 199\n",
      "2021-09-07 17:17:59.218 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.219 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 32.0\n",
      "2021-09-07 17:17:59.220 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 26.285714285714285\n",
      "2021-09-07 17:17:59.221 | INFO     | src.policies:collect_trajectories:221 - Episode 200\n",
      "2021-09-07 17:17:59.229 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.229 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 31.0\n",
      "2021-09-07 17:17:59.230 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 26.875\n",
      "2021-09-07 17:17:59.231 | WARNING  | src.policies:train:144 - The actual batch size is 215, instead of 200\n",
      "2021-09-07 17:17:59.234 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:59.235 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2608310580253601, 'baseline_loss': 1.0905951261520386, 'total_loss': 0.2844665050506592}\n",
      "2021-09-07 17:17:59.236 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16233164072036743\n",
      "2021-09-07 17:17:59.237 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6900354027748108\n",
      "2021-09-07 17:17:59.238 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16233164072036743\n",
      "2021-09-07 17:17:59.239 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:17:59.241 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:59.242 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24976560473442078, 'baseline_loss': 1.1062332391738892, 'total_loss': 0.3033510148525238}\n",
      "2021-09-07 17:17:59.243 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10383771359920502\n",
      "2021-09-07 17:17:59.245 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7295836210250854\n",
      "2021-09-07 17:17:59.246 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10383771359920502\n",
      "2021-09-07 17:17:59.248 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:17:59.249 | INFO     | src.policies:train:123 - Epoch 23 / 800\n",
      "2021-09-07 17:17:59.250 | INFO     | src.policies:collect_trajectories:221 - Episode 201\n",
      "2021-09-07 17:17:59.254 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.254 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:17:59.255 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.0\n",
      "2021-09-07 17:17:59.255 | INFO     | src.policies:collect_trajectories:221 - Episode 202\n",
      "2021-09-07 17:17:59.260 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.261 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:17:59.262 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 16.5\n",
      "2021-09-07 17:17:59.262 | INFO     | src.policies:collect_trajectories:221 - Episode 203\n",
      "2021-09-07 17:17:59.267 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.268 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:17:59.268 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 17.333333333333332\n",
      "2021-09-07 17:17:59.269 | INFO     | src.policies:collect_trajectories:221 - Episode 204\n",
      "2021-09-07 17:17:59.274 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.275 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:17:59.275 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.0\n",
      "2021-09-07 17:17:59.276 | INFO     | src.policies:collect_trajectories:221 - Episode 205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:59.349 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.350 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 48.0\n",
      "2021-09-07 17:17:59.350 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.8\n",
      "2021-09-07 17:17:59.351 | INFO     | src.policies:collect_trajectories:221 - Episode 206\n",
      "2021-09-07 17:17:59.355 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.356 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:17:59.356 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.0\n",
      "2021-09-07 17:17:59.357 | INFO     | src.policies:collect_trajectories:221 - Episode 207\n",
      "2021-09-07 17:17:59.362 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.362 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:17:59.363 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.285714285714285\n",
      "2021-09-07 17:17:59.363 | INFO     | src.policies:collect_trajectories:221 - Episode 208\n",
      "2021-09-07 17:17:59.368 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.369 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:17:59.369 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.5\n",
      "2021-09-07 17:17:59.369 | INFO     | src.policies:collect_trajectories:221 - Episode 209\n",
      "2021-09-07 17:17:59.373 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.373 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:17:59.374 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.444444444444443\n",
      "2021-09-07 17:17:59.374 | INFO     | src.policies:collect_trajectories:221 - Episode 210\n",
      "2021-09-07 17:17:59.378 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.379 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:17:59.379 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.1\n",
      "2021-09-07 17:17:59.380 | WARNING  | src.policies:train:144 - The actual batch size is 201, instead of 200\n",
      "2021-09-07 17:17:59.383 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:59.385 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.26948627829551697, 'baseline_loss': 1.103780746459961, 'total_loss': 0.2824040949344635}\n",
      "2021-09-07 17:17:59.386 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1616390496492386\n",
      "2021-09-07 17:17:59.387 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6854537129402161\n",
      "2021-09-07 17:17:59.388 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1616390496492386\n",
      "2021-09-07 17:17:59.389 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:17:59.390 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:59.391 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29144078493118286, 'baseline_loss': 1.1130503416061401, 'total_loss': 0.2650843858718872}\n",
      "2021-09-07 17:17:59.392 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2907591462135315\n",
      "2021-09-07 17:17:59.393 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6653929948806763\n",
      "2021-09-07 17:17:59.394 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2907591462135315\n",
      "2021-09-07 17:17:59.395 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:17:59.396 | INFO     | src.policies:train:123 - Epoch 24 / 800\n",
      "2021-09-07 17:17:59.397 | INFO     | src.policies:collect_trajectories:221 - Episode 211\n",
      "2021-09-07 17:17:59.404 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.404 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 38.0\n",
      "2021-09-07 17:17:59.405 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 38.0\n",
      "2021-09-07 17:17:59.405 | INFO     | src.policies:collect_trajectories:221 - Episode 212\n",
      "2021-09-07 17:17:59.410 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.410 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:17:59.411 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.5\n",
      "2021-09-07 17:17:59.411 | INFO     | src.policies:collect_trajectories:221 - Episode 213\n",
      "2021-09-07 17:17:59.417 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.417 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:17:59.418 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.0\n",
      "2021-09-07 17:17:59.418 | INFO     | src.policies:collect_trajectories:221 - Episode 214\n",
      "2021-09-07 17:17:59.424 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.425 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:17:59.425 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.25\n",
      "2021-09-07 17:17:59.426 | INFO     | src.policies:collect_trajectories:221 - Episode 215\n",
      "2021-09-07 17:17:59.429 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.430 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:17:59.430 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.2\n",
      "2021-09-07 17:17:59.431 | INFO     | src.policies:collect_trajectories:221 - Episode 216\n",
      "2021-09-07 17:17:59.435 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.435 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:17:59.436 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.833333333333332\n",
      "2021-09-07 17:17:59.436 | INFO     | src.policies:collect_trajectories:221 - Episode 217\n",
      "2021-09-07 17:17:59.442 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.443 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 26.0\n",
      "2021-09-07 17:17:59.444 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.571428571428573\n",
      "2021-09-07 17:17:59.444 | INFO     | src.policies:collect_trajectories:221 - Episode 218\n",
      "2021-09-07 17:17:59.453 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.454 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 49.0\n",
      "2021-09-07 17:17:59.455 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.0\n",
      "2021-09-07 17:17:59.458 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:59.460 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2799007296562195, 'baseline_loss': 1.0803756713867188, 'total_loss': 0.2602871060371399}\n",
      "2021-09-07 17:17:59.461 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16683374345302582\n",
      "2021-09-07 17:17:59.462 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6860885620117188\n",
      "2021-09-07 17:17:59.463 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16683374345302582\n",
      "2021-09-07 17:17:59.465 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:59.466 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:59.468 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.26167404651641846, 'baseline_loss': 1.0578150749206543, 'total_loss': 0.2672334909439087}\n",
      "2021-09-07 17:17:59.469 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08644621074199677\n",
      "2021-09-07 17:17:59.470 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.672928512096405\n",
      "2021-09-07 17:17:59.471 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08644621074199677\n",
      "2021-09-07 17:17:59.472 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:17:59.474 | INFO     | src.policies:train:123 - Epoch 25 / 800\n",
      "2021-09-07 17:17:59.474 | INFO     | src.policies:collect_trajectories:221 - Episode 219\n",
      "2021-09-07 17:17:59.481 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.481 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 34.0\n",
      "2021-09-07 17:17:59.482 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 34.0\n",
      "2021-09-07 17:17:59.482 | INFO     | src.policies:collect_trajectories:221 - Episode 220\n",
      "2021-09-07 17:17:59.486 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.487 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:17:59.488 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.5\n",
      "2021-09-07 17:17:59.488 | INFO     | src.policies:collect_trajectories:221 - Episode 221\n",
      "2021-09-07 17:17:59.496 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.497 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 46.0\n",
      "2021-09-07 17:17:59.497 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 31.0\n",
      "2021-09-07 17:17:59.498 | INFO     | src.policies:collect_trajectories:221 - Episode 222\n",
      "2021-09-07 17:17:59.507 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.508 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 33.0\n",
      "2021-09-07 17:17:59.508 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 31.5\n",
      "2021-09-07 17:17:59.508 | INFO     | src.policies:collect_trajectories:221 - Episode 223\n",
      "2021-09-07 17:17:59.513 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.513 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:17:59.514 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 29.0\n",
      "2021-09-07 17:17:59.514 | INFO     | src.policies:collect_trajectories:221 - Episode 224\n",
      "2021-09-07 17:17:59.524 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.525 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 45.0\n",
      "2021-09-07 17:17:59.525 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 31.666666666666668\n",
      "2021-09-07 17:17:59.526 | INFO     | src.policies:collect_trajectories:221 - Episode 225\n",
      "2021-09-07 17:17:59.533 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.533 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 35.0\n",
      "2021-09-07 17:17:59.534 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 32.142857142857146\n",
      "2021-09-07 17:17:59.534 | WARNING  | src.policies:train:144 - The actual batch size is 225, instead of 200\n",
      "2021-09-07 17:17:59.538 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:59.540 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2513252794742584, 'baseline_loss': 1.0886672735214233, 'total_loss': 0.29300835728645325}\n",
      "2021-09-07 17:17:59.541 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.05560683086514473\n",
      "2021-09-07 17:17:59.542 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6534596085548401\n",
      "2021-09-07 17:17:59.543 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.05560683086514473\n",
      "2021-09-07 17:17:59.545 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:17:59.546 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:59.548 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2961972951889038, 'baseline_loss': 1.108644962310791, 'total_loss': 0.2581251859664917}\n",
      "2021-09-07 17:17:59.549 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18361544609069824\n",
      "2021-09-07 17:17:59.550 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5996753573417664\n",
      "2021-09-07 17:17:59.551 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18361544609069824\n",
      "2021-09-07 17:17:59.552 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:17:59.553 | INFO     | src.policies:train:123 - Epoch 26 / 800\n",
      "2021-09-07 17:17:59.554 | INFO     | src.policies:collect_trajectories:221 - Episode 226\n",
      "2021-09-07 17:17:59.561 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.562 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 38.0\n",
      "2021-09-07 17:17:59.562 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 38.0\n",
      "2021-09-07 17:17:59.563 | INFO     | src.policies:collect_trajectories:221 - Episode 227\n",
      "2021-09-07 17:17:59.577 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.578 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 77.0\n",
      "2021-09-07 17:17:59.579 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 57.5\n",
      "2021-09-07 17:17:59.579 | INFO     | src.policies:collect_trajectories:221 - Episode 228\n",
      "2021-09-07 17:17:59.586 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.587 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:17:59.587 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 45.333333333333336\n",
      "2021-09-07 17:17:59.588 | INFO     | src.policies:collect_trajectories:221 - Episode 229\n",
      "2021-09-07 17:17:59.592 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.593 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:17:59.593 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 37.25\n",
      "2021-09-07 17:17:59.594 | INFO     | src.policies:collect_trajectories:221 - Episode 230\n",
      "2021-09-07 17:17:59.599 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.600 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 22.0\n",
      "2021-09-07 17:17:59.600 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 34.2\n",
      "2021-09-07 17:17:59.601 | INFO     | src.policies:collect_trajectories:221 - Episode 231\n",
      "2021-09-07 17:17:59.607 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.608 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:17:59.608 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 32.0\n",
      "2021-09-07 17:17:59.609 | INFO     | src.policies:collect_trajectories:221 - Episode 232\n",
      "2021-09-07 17:17:59.622 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:59.622 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 72.0\n",
      "2021-09-07 17:17:59.623 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 37.714285714285715\n",
      "2021-09-07 17:17:59.624 | WARNING  | src.policies:train:144 - The actual batch size is 264, instead of 200\n",
      "2021-09-07 17:17:59.628 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:59.630 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.25840359926223755, 'baseline_loss': 1.1044288873672485, 'total_loss': 0.2938108444213867}\n",
      "2021-09-07 17:17:59.631 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.06333320587873459\n",
      "2021-09-07 17:17:59.631 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5550678968429565\n",
      "2021-09-07 17:17:59.633 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.06333320587873459\n",
      "2021-09-07 17:17:59.634 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:17:59.635 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:59.636 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27395835518836975, 'baseline_loss': 1.1120271682739258, 'total_loss': 0.28205522894859314}\n",
      "2021-09-07 17:17:59.637 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08411728590726852\n",
      "2021-09-07 17:17:59.638 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5988552570343018\n",
      "2021-09-07 17:17:59.639 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08411728590726852\n",
      "2021-09-07 17:17:59.640 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:17:59.642 | INFO     | src.policies:train:123 - Epoch 27 / 800\n",
      "2021-09-07 17:17:59.643 | INFO     | src.policies:collect_trajectories:221 - Episode 233\n",
      "2021-09-07 17:17:59.647 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.648 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:17:59.648 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 13.0\n",
      "2021-09-07 17:17:59.648 | INFO     | src.policies:collect_trajectories:221 - Episode 234\n",
      "2021-09-07 17:17:59.653 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.654 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:17:59.654 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 14.5\n",
      "2021-09-07 17:17:59.655 | INFO     | src.policies:collect_trajectories:221 - Episode 235\n",
      "2021-09-07 17:17:59.659 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.660 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:17:59.660 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 15.666666666666666\n",
      "2021-09-07 17:17:59.662 | INFO     | src.policies:collect_trajectories:221 - Episode 236\n",
      "2021-09-07 17:17:59.667 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.668 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:17:59.668 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 16.75\n",
      "2021-09-07 17:17:59.668 | INFO     | src.policies:collect_trajectories:221 - Episode 237\n",
      "2021-09-07 17:17:59.674 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.674 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:17:59.675 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.0\n",
      "2021-09-07 17:17:59.675 | INFO     | src.policies:collect_trajectories:221 - Episode 238\n",
      "2021-09-07 17:17:59.682 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.683 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 35.0\n",
      "2021-09-07 17:17:59.684 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.833333333333332\n",
      "2021-09-07 17:17:59.685 | INFO     | src.policies:collect_trajectories:221 - Episode 239\n",
      "2021-09-07 17:17:59.691 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.692 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 32.0\n",
      "2021-09-07 17:17:59.692 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.428571428571427\n",
      "2021-09-07 17:17:59.692 | INFO     | src.policies:collect_trajectories:221 - Episode 240\n",
      "2021-09-07 17:17:59.698 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.699 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:17:59.699 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.625\n",
      "2021-09-07 17:17:59.700 | INFO     | src.policies:collect_trajectories:221 - Episode 241\n",
      "2021-09-07 17:17:59.709 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.709 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 40.0\n",
      "2021-09-07 17:17:59.710 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.555555555555557\n",
      "2021-09-07 17:17:59.710 | WARNING  | src.policies:train:144 - The actual batch size is 221, instead of 200\n",
      "2021-09-07 17:17:59.714 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:59.715 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2648862302303314, 'baseline_loss': 1.0525158643722534, 'total_loss': 0.2613717019557953}\n",
      "2021-09-07 17:17:59.716 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10964687913656235\n",
      "2021-09-07 17:17:59.717 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.678730845451355\n",
      "2021-09-07 17:17:59.718 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10964687913656235\n",
      "2021-09-07 17:17:59.719 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:17:59.721 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:59.722 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.25445297360420227, 'baseline_loss': 1.0683752298355103, 'total_loss': 0.27973464131355286}\n",
      "2021-09-07 17:17:59.723 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.06121216341853142\n",
      "2021-09-07 17:17:59.725 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.679212749004364\n",
      "2021-09-07 17:17:59.726 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.06121216341853142\n",
      "2021-09-07 17:17:59.727 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:17:59.729 | INFO     | src.policies:train:123 - Epoch 28 / 800\n",
      "2021-09-07 17:17:59.729 | INFO     | src.policies:collect_trajectories:221 - Episode 242\n",
      "2021-09-07 17:17:59.737 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.738 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 46.0\n",
      "2021-09-07 17:17:59.739 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 46.0\n",
      "2021-09-07 17:17:59.739 | INFO     | src.policies:collect_trajectories:221 - Episode 243\n",
      "2021-09-07 17:17:59.746 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.746 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 26.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:59.747 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 36.0\n",
      "2021-09-07 17:17:59.747 | INFO     | src.policies:collect_trajectories:221 - Episode 244\n",
      "2021-09-07 17:17:59.754 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.755 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 34.0\n",
      "2021-09-07 17:17:59.756 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 35.333333333333336\n",
      "2021-09-07 17:17:59.756 | INFO     | src.policies:collect_trajectories:221 - Episode 245\n",
      "2021-09-07 17:17:59.762 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.763 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:17:59.763 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 32.5\n",
      "2021-09-07 17:17:59.764 | INFO     | src.policies:collect_trajectories:221 - Episode 246\n",
      "2021-09-07 17:17:59.770 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.770 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:17:59.771 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 29.2\n",
      "2021-09-07 17:17:59.771 | INFO     | src.policies:collect_trajectories:221 - Episode 247\n",
      "2021-09-07 17:17:59.779 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.780 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 37.0\n",
      "2021-09-07 17:17:59.781 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 30.5\n",
      "2021-09-07 17:17:59.781 | INFO     | src.policies:collect_trajectories:221 - Episode 248\n",
      "2021-09-07 17:17:59.790 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.791 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 39.0\n",
      "2021-09-07 17:17:59.791 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 31.714285714285715\n",
      "2021-09-07 17:17:59.792 | WARNING  | src.policies:train:144 - The actual batch size is 222, instead of 200\n",
      "2021-09-07 17:17:59.795 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:59.797 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27167561650276184, 'baseline_loss': 1.080031156539917, 'total_loss': 0.26833996176719666}\n",
      "2021-09-07 17:17:59.798 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1367035061120987\n",
      "2021-09-07 17:17:59.799 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6570720672607422\n",
      "2021-09-07 17:17:59.800 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1367035061120987\n",
      "2021-09-07 17:17:59.801 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:17:59.803 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:59.804 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.28169718384742737, 'baseline_loss': 1.064212441444397, 'total_loss': 0.2504090368747711}\n",
      "2021-09-07 17:17:59.805 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.06937573850154877\n",
      "2021-09-07 17:17:59.806 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6103039383888245\n",
      "2021-09-07 17:17:59.807 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.06937573850154877\n",
      "2021-09-07 17:17:59.808 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:17:59.810 | INFO     | src.policies:train:123 - Epoch 29 / 800\n",
      "2021-09-07 17:17:59.811 | INFO     | src.policies:collect_trajectories:221 - Episode 249\n",
      "2021-09-07 17:17:59.814 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.815 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:17:59.815 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 14.0\n",
      "2021-09-07 17:17:59.815 | INFO     | src.policies:collect_trajectories:221 - Episode 250\n",
      "2021-09-07 17:17:59.819 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.820 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:17:59.820 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 13.0\n",
      "2021-09-07 17:17:59.821 | INFO     | src.policies:collect_trajectories:221 - Episode 251\n",
      "2021-09-07 17:17:59.826 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.827 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:17:59.827 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 14.666666666666666\n",
      "2021-09-07 17:17:59.828 | INFO     | src.policies:collect_trajectories:221 - Episode 252\n",
      "2021-09-07 17:17:59.834 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.834 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:17:59.835 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 17.25\n",
      "2021-09-07 17:17:59.835 | INFO     | src.policies:collect_trajectories:221 - Episode 253\n",
      "2021-09-07 17:17:59.840 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.841 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:17:59.842 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 17.0\n",
      "2021-09-07 17:17:59.842 | INFO     | src.policies:collect_trajectories:221 - Episode 254\n",
      "2021-09-07 17:17:59.932 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.933 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:17:59.933 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 17.333333333333332\n",
      "2021-09-07 17:17:59.934 | INFO     | src.policies:collect_trajectories:221 - Episode 255\n",
      "2021-09-07 17:17:59.940 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.940 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:17:59.941 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 17.714285714285715\n",
      "2021-09-07 17:17:59.942 | INFO     | src.policies:collect_trajectories:221 - Episode 256\n",
      "2021-09-07 17:17:59.950 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.951 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 43.0\n",
      "2021-09-07 17:17:59.952 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.875\n",
      "2021-09-07 17:17:59.952 | INFO     | src.policies:collect_trajectories:221 - Episode 257\n",
      "2021-09-07 17:17:59.957 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.957 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:17:59.957 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.333333333333332\n",
      "2021-09-07 17:17:59.958 | INFO     | src.policies:collect_trajectories:221 - Episode 258\n",
      "2021-09-07 17:17:59.967 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.968 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 40.0\n",
      "2021-09-07 17:17:59.969 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.3\n",
      "2021-09-07 17:17:59.969 | WARNING  | src.policies:train:144 - The actual batch size is 223, instead of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:17:59.973 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:17:59.974 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2889930009841919, 'baseline_loss': 1.1442428827285767, 'total_loss': 0.28312844038009644}\n",
      "2021-09-07 17:17:59.975 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09033088386058807\n",
      "2021-09-07 17:17:59.976 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.648177981376648\n",
      "2021-09-07 17:17:59.977 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09033088386058807\n",
      "2021-09-07 17:17:59.978 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:17:59.980 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:17:59.981 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2852475643157959, 'baseline_loss': 1.1129636764526367, 'total_loss': 0.27123427391052246}\n",
      "2021-09-07 17:17:59.983 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08258066326379776\n",
      "2021-09-07 17:17:59.983 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6124104261398315\n",
      "2021-09-07 17:17:59.985 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08258066326379776\n",
      "2021-09-07 17:17:59.987 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:17:59.988 | INFO     | src.policies:train:123 - Epoch 30 / 800\n",
      "2021-09-07 17:17:59.988 | INFO     | src.policies:collect_trajectories:221 - Episode 259\n",
      "2021-09-07 17:17:59.992 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.993 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:17:59.993 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.0\n",
      "2021-09-07 17:17:59.994 | INFO     | src.policies:collect_trajectories:221 - Episode 260\n",
      "2021-09-07 17:17:59.998 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:17:59.999 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 15.0\n",
      "2021-09-07 17:17:59.999 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 17.0\n",
      "2021-09-07 17:18:00.000 | INFO     | src.policies:collect_trajectories:221 - Episode 261\n",
      "2021-09-07 17:18:00.006 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.007 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:00.007 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.0\n",
      "2021-09-07 17:18:00.007 | INFO     | src.policies:collect_trajectories:221 - Episode 262\n",
      "2021-09-07 17:18:00.013 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.013 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:18:00.014 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.25\n",
      "2021-09-07 17:18:00.014 | INFO     | src.policies:collect_trajectories:221 - Episode 263\n",
      "2021-09-07 17:18:00.018 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.018 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:18:00.019 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.0\n",
      "2021-09-07 17:18:00.020 | INFO     | src.policies:collect_trajectories:221 - Episode 264\n",
      "2021-09-07 17:18:00.026 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.027 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:18:00.027 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.166666666666668\n",
      "2021-09-07 17:18:00.028 | INFO     | src.policies:collect_trajectories:221 - Episode 265\n",
      "2021-09-07 17:18:00.032 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.033 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:18:00.033 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.857142857142858\n",
      "2021-09-07 17:18:00.033 | INFO     | src.policies:collect_trajectories:221 - Episode 266\n",
      "2021-09-07 17:18:00.040 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.040 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 29.0\n",
      "2021-09-07 17:18:00.041 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.125\n",
      "2021-09-07 17:18:00.042 | INFO     | src.policies:collect_trajectories:221 - Episode 267\n",
      "2021-09-07 17:18:00.047 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.047 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:18:00.047 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.333333333333332\n",
      "2021-09-07 17:18:00.048 | INFO     | src.policies:collect_trajectories:221 - Episode 268\n",
      "2021-09-07 17:18:00.055 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.056 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 40.0\n",
      "2021-09-07 17:18:00.056 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.4\n",
      "2021-09-07 17:18:00.057 | WARNING  | src.policies:train:144 - The actual batch size is 214, instead of 200\n",
      "2021-09-07 17:18:00.061 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:00.063 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2911515533924103, 'baseline_loss': 1.1132643222808838, 'total_loss': 0.2654806077480316}\n",
      "2021-09-07 17:18:00.064 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09118398278951645\n",
      "2021-09-07 17:18:00.065 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.48698747158050537\n",
      "2021-09-07 17:18:00.066 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09118398278951645\n",
      "2021-09-07 17:18:00.067 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.48698747158050537\n",
      "2021-09-07 17:18:00.068 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:00.069 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31234171986579895, 'baseline_loss': 1.026338815689087, 'total_loss': 0.2008276879787445}\n",
      "2021-09-07 17:18:00.070 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09163106232881546\n",
      "2021-09-07 17:18:00.071 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7128400206565857\n",
      "2021-09-07 17:18:00.072 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09163106232881546\n",
      "2021-09-07 17:18:00.073 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:00.074 | INFO     | src.policies:train:123 - Epoch 31 / 800\n",
      "2021-09-07 17:18:00.075 | INFO     | src.policies:collect_trajectories:221 - Episode 269\n",
      "2021-09-07 17:18:00.078 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.078 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:18:00.079 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 12.0\n",
      "2021-09-07 17:18:00.079 | INFO     | src.policies:collect_trajectories:221 - Episode 270\n",
      "2021-09-07 17:18:00.086 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:00.086 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:18:00.087 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.5\n",
      "2021-09-07 17:18:00.087 | INFO     | src.policies:collect_trajectories:221 - Episode 271\n",
      "2021-09-07 17:18:00.094 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.095 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 36.0\n",
      "2021-09-07 17:18:00.095 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.333333333333332\n",
      "2021-09-07 17:18:00.096 | INFO     | src.policies:collect_trajectories:221 - Episode 272\n",
      "2021-09-07 17:18:00.102 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.103 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 30.0\n",
      "2021-09-07 17:18:00.103 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.75\n",
      "2021-09-07 17:18:00.104 | INFO     | src.policies:collect_trajectories:221 - Episode 273\n",
      "2021-09-07 17:18:00.108 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.108 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:18:00.109 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.8\n",
      "2021-09-07 17:18:00.109 | INFO     | src.policies:collect_trajectories:221 - Episode 274\n",
      "2021-09-07 17:18:00.114 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.115 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:18:00.115 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.0\n",
      "2021-09-07 17:18:00.115 | INFO     | src.policies:collect_trajectories:221 - Episode 275\n",
      "2021-09-07 17:18:00.118 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.119 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 9.0\n",
      "2021-09-07 17:18:00.120 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.0\n",
      "2021-09-07 17:18:00.120 | INFO     | src.policies:collect_trajectories:221 - Episode 276\n",
      "2021-09-07 17:18:00.127 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.128 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 34.0\n",
      "2021-09-07 17:18:00.129 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.625\n",
      "2021-09-07 17:18:00.129 | INFO     | src.policies:collect_trajectories:221 - Episode 277\n",
      "2021-09-07 17:18:00.135 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.135 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:18:00.136 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.88888888888889\n",
      "2021-09-07 17:18:00.136 | WARNING  | src.policies:train:144 - The actual batch size is 206, instead of 200\n",
      "2021-09-07 17:18:00.140 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:00.142 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29421278834342957, 'baseline_loss': 1.002938985824585, 'total_loss': 0.20725670456886292}\n",
      "2021-09-07 17:18:00.143 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.03782060742378235\n",
      "2021-09-07 17:18:00.144 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6196215748786926\n",
      "2021-09-07 17:18:00.145 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.03782060742378235\n",
      "2021-09-07 17:18:00.146 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:00.147 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:00.148 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3136691451072693, 'baseline_loss': 1.022108554840088, 'total_loss': 0.19738513231277466}\n",
      "2021-09-07 17:18:00.149 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.05058688670396805\n",
      "2021-09-07 17:18:00.150 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6147794127464294\n",
      "2021-09-07 17:18:00.151 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.05058688670396805\n",
      "2021-09-07 17:18:00.153 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:00.155 | INFO     | src.policies:train:123 - Epoch 32 / 800\n",
      "2021-09-07 17:18:00.155 | INFO     | src.policies:collect_trajectories:221 - Episode 278\n",
      "2021-09-07 17:18:00.163 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.164 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 31.0\n",
      "2021-09-07 17:18:00.164 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 31.0\n",
      "2021-09-07 17:18:00.165 | INFO     | src.policies:collect_trajectories:221 - Episode 279\n",
      "2021-09-07 17:18:00.173 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.173 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 35.0\n",
      "2021-09-07 17:18:00.174 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 33.0\n",
      "2021-09-07 17:18:00.174 | INFO     | src.policies:collect_trajectories:221 - Episode 280\n",
      "2021-09-07 17:18:00.188 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.189 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 70.0\n",
      "2021-09-07 17:18:00.189 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 45.333333333333336\n",
      "2021-09-07 17:18:00.190 | INFO     | src.policies:collect_trajectories:221 - Episode 281\n",
      "2021-09-07 17:18:00.195 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.196 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:18:00.196 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 40.25\n",
      "2021-09-07 17:18:00.196 | INFO     | src.policies:collect_trajectories:221 - Episode 282\n",
      "2021-09-07 17:18:00.203 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.203 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:18:00.204 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 36.8\n",
      "2021-09-07 17:18:00.204 | INFO     | src.policies:collect_trajectories:221 - Episode 283\n",
      "2021-09-07 17:18:00.209 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.210 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:18:00.210 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 34.5\n",
      "2021-09-07 17:18:00.211 | WARNING  | src.policies:train:144 - The actual batch size is 207, instead of 200\n",
      "2021-09-07 17:18:00.213 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:00.215 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32015421986579895, 'baseline_loss': 1.1623427867889404, 'total_loss': 0.26101717352867126}\n",
      "2021-09-07 17:18:00.216 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.0836060494184494\n",
      "2021-09-07 17:18:00.216 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6020011901855469\n",
      "2021-09-07 17:18:00.218 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.0836060494184494\n",
      "2021-09-07 17:18:00.219 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:00.221 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:00.222 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3240794241428375, 'baseline_loss': 1.1539690494537354, 'total_loss': 0.25290510058403015}\n",
      "2021-09-07 17:18:00.223 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.050189897418022156\n",
      "2021-09-07 17:18:00.224 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5918150544166565\n",
      "2021-09-07 17:18:00.226 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.050189897418022156\n",
      "2021-09-07 17:18:00.227 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:00.228 | INFO     | src.policies:train:123 - Epoch 33 / 800\n",
      "2021-09-07 17:18:00.229 | INFO     | src.policies:collect_trajectories:221 - Episode 284\n",
      "2021-09-07 17:18:00.232 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.232 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:18:00.233 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 11.0\n",
      "2021-09-07 17:18:00.233 | INFO     | src.policies:collect_trajectories:221 - Episode 285\n",
      "2021-09-07 17:18:00.237 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.238 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:18:00.238 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 14.0\n",
      "2021-09-07 17:18:00.239 | INFO     | src.policies:collect_trajectories:221 - Episode 286\n",
      "2021-09-07 17:18:00.246 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.247 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 33.0\n",
      "2021-09-07 17:18:00.247 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.333333333333332\n",
      "2021-09-07 17:18:00.248 | INFO     | src.policies:collect_trajectories:221 - Episode 287\n",
      "2021-09-07 17:18:00.254 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.255 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 33.0\n",
      "2021-09-07 17:18:00.256 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.5\n",
      "2021-09-07 17:18:00.256 | INFO     | src.policies:collect_trajectories:221 - Episode 288\n",
      "2021-09-07 17:18:00.260 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.261 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:18:00.262 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.2\n",
      "2021-09-07 17:18:00.262 | INFO     | src.policies:collect_trajectories:221 - Episode 289\n",
      "2021-09-07 17:18:00.267 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.268 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 22.0\n",
      "2021-09-07 17:18:00.268 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.333333333333332\n",
      "2021-09-07 17:18:00.269 | INFO     | src.policies:collect_trajectories:221 - Episode 290\n",
      "2021-09-07 17:18:00.274 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.275 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 27.0\n",
      "2021-09-07 17:18:00.275 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.142857142857142\n",
      "2021-09-07 17:18:00.276 | INFO     | src.policies:collect_trajectories:221 - Episode 291\n",
      "2021-09-07 17:18:00.282 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.283 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:18:00.283 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.375\n",
      "2021-09-07 17:18:00.283 | INFO     | src.policies:collect_trajectories:221 - Episode 292\n",
      "2021-09-07 17:18:00.291 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.292 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 40.0\n",
      "2021-09-07 17:18:00.292 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.333333333333332\n",
      "2021-09-07 17:18:00.293 | WARNING  | src.policies:train:144 - The actual batch size is 219, instead of 200\n",
      "2021-09-07 17:18:00.295 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:00.297 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.37243616580963135, 'baseline_loss': 1.064288854598999, 'total_loss': 0.15970826148986816}\n",
      "2021-09-07 17:18:00.298 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25919628143310547\n",
      "2021-09-07 17:18:00.300 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5758275985717773\n",
      "2021-09-07 17:18:00.301 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25919628143310547\n",
      "2021-09-07 17:18:00.302 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:00.304 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:00.305 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3366737365722656, 'baseline_loss': 1.1486945152282715, 'total_loss': 0.23767352104187012}\n",
      "2021-09-07 17:18:00.306 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10568266361951828\n",
      "2021-09-07 17:18:00.307 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4942627251148224\n",
      "2021-09-07 17:18:00.308 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10568266361951828\n",
      "2021-09-07 17:18:00.309 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4942627251148224\n",
      "2021-09-07 17:18:00.310 | INFO     | src.policies:train:123 - Epoch 34 / 800\n",
      "2021-09-07 17:18:00.311 | INFO     | src.policies:collect_trajectories:221 - Episode 293\n",
      "2021-09-07 17:18:00.315 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.315 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:18:00.316 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 17.0\n",
      "2021-09-07 17:18:00.316 | INFO     | src.policies:collect_trajectories:221 - Episode 294\n",
      "2021-09-07 17:18:00.321 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.322 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:18:00.322 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 16.5\n",
      "2021-09-07 17:18:00.323 | INFO     | src.policies:collect_trajectories:221 - Episode 295\n",
      "2021-09-07 17:18:00.330 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.330 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 31.0\n",
      "2021-09-07 17:18:00.330 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.333333333333332\n",
      "2021-09-07 17:18:00.331 | INFO     | src.policies:collect_trajectories:221 - Episode 296\n",
      "2021-09-07 17:18:00.344 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.345 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 78.0\n",
      "2021-09-07 17:18:00.345 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 35.5\n",
      "2021-09-07 17:18:00.346 | INFO     | src.policies:collect_trajectories:221 - Episode 297\n",
      "2021-09-07 17:18:00.350 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:00.351 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:18:00.351 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 31.6\n",
      "2021-09-07 17:18:00.351 | INFO     | src.policies:collect_trajectories:221 - Episode 298\n",
      "2021-09-07 17:18:00.358 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.358 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 33.0\n",
      "2021-09-07 17:18:00.359 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 31.833333333333332\n",
      "2021-09-07 17:18:00.360 | INFO     | src.policies:collect_trajectories:221 - Episode 299\n",
      "2021-09-07 17:18:00.364 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.365 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:18:00.365 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 29.142857142857142\n",
      "2021-09-07 17:18:00.366 | WARNING  | src.policies:train:144 - The actual batch size is 204, instead of 200\n",
      "2021-09-07 17:18:00.369 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:00.370 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.35319146513938904, 'baseline_loss': 1.082013726234436, 'total_loss': 0.18781539797782898}\n",
      "2021-09-07 17:18:00.371 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08165641129016876\n",
      "2021-09-07 17:18:00.372 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.47975611686706543\n",
      "2021-09-07 17:18:00.373 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08165641129016876\n",
      "2021-09-07 17:18:00.374 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.47975611686706543\n",
      "2021-09-07 17:18:00.375 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:00.376 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3561578094959259, 'baseline_loss': 1.0570809841156006, 'total_loss': 0.1723826825618744}\n",
      "2021-09-07 17:18:00.377 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09979049861431122\n",
      "2021-09-07 17:18:00.378 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4973618686199188\n",
      "2021-09-07 17:18:00.380 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09979049861431122\n",
      "2021-09-07 17:18:00.381 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4973618686199188\n",
      "2021-09-07 17:18:00.383 | INFO     | src.policies:train:123 - Epoch 35 / 800\n",
      "2021-09-07 17:18:00.384 | INFO     | src.policies:collect_trajectories:221 - Episode 300\n",
      "2021-09-07 17:18:00.391 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.392 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 41.0\n",
      "2021-09-07 17:18:00.392 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 41.0\n",
      "2021-09-07 17:18:00.392 | INFO     | src.policies:collect_trajectories:221 - Episode 301\n",
      "2021-09-07 17:18:00.399 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.400 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 29.0\n",
      "2021-09-07 17:18:00.400 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 35.0\n",
      "2021-09-07 17:18:00.400 | INFO     | src.policies:collect_trajectories:221 - Episode 302\n",
      "2021-09-07 17:18:00.412 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.413 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 53.0\n",
      "2021-09-07 17:18:00.413 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 41.0\n",
      "2021-09-07 17:18:00.413 | INFO     | src.policies:collect_trajectories:221 - Episode 303\n",
      "2021-09-07 17:18:00.421 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.422 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 36.0\n",
      "2021-09-07 17:18:00.422 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 39.75\n",
      "2021-09-07 17:18:00.423 | INFO     | src.policies:collect_trajectories:221 - Episode 304\n",
      "2021-09-07 17:18:00.429 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.429 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 28.0\n",
      "2021-09-07 17:18:00.430 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 37.4\n",
      "2021-09-07 17:18:00.430 | INFO     | src.policies:collect_trajectories:221 - Episode 305\n",
      "2021-09-07 17:18:00.604 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.605 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 38.0\n",
      "2021-09-07 17:18:00.605 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 37.5\n",
      "2021-09-07 17:18:00.606 | WARNING  | src.policies:train:144 - The actual batch size is 225, instead of 200\n",
      "2021-09-07 17:18:00.609 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:00.611 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3748456537723541, 'baseline_loss': 1.194826602935791, 'total_loss': 0.22256764769554138}\n",
      "2021-09-07 17:18:00.613 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13990069925785065\n",
      "2021-09-07 17:18:00.614 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.32358303666114807\n",
      "2021-09-07 17:18:00.615 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13990069925785065\n",
      "2021-09-07 17:18:00.616 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.32358303666114807\n",
      "2021-09-07 17:18:00.618 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:00.619 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4037536382675171, 'baseline_loss': 1.1830121278762817, 'total_loss': 0.18775242567062378}\n",
      "2021-09-07 17:18:00.620 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.06808950006961823\n",
      "2021-09-07 17:18:00.621 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3241906762123108\n",
      "2021-09-07 17:18:00.622 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.06808950006961823\n",
      "2021-09-07 17:18:00.624 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3241906762123108\n",
      "2021-09-07 17:18:00.626 | INFO     | src.policies:train:123 - Epoch 36 / 800\n",
      "2021-09-07 17:18:00.626 | INFO     | src.policies:collect_trajectories:221 - Episode 306\n",
      "2021-09-07 17:18:00.629 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.630 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:18:00.630 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 12.0\n",
      "2021-09-07 17:18:00.631 | INFO     | src.policies:collect_trajectories:221 - Episode 307\n",
      "2021-09-07 17:18:00.637 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.637 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 30.0\n",
      "2021-09-07 17:18:00.638 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.0\n",
      "2021-09-07 17:18:00.638 | INFO     | src.policies:collect_trajectories:221 - Episode 308\n",
      "2021-09-07 17:18:00.646 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.646 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 32.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:00.647 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.666666666666668\n",
      "2021-09-07 17:18:00.647 | INFO     | src.policies:collect_trajectories:221 - Episode 309\n",
      "2021-09-07 17:18:00.652 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.653 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:18:00.653 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.75\n",
      "2021-09-07 17:18:00.654 | INFO     | src.policies:collect_trajectories:221 - Episode 310\n",
      "2021-09-07 17:18:00.658 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.658 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:18:00.659 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.4\n",
      "2021-09-07 17:18:00.659 | INFO     | src.policies:collect_trajectories:221 - Episode 311\n",
      "2021-09-07 17:18:00.665 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.666 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:18:00.666 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.5\n",
      "2021-09-07 17:18:00.667 | INFO     | src.policies:collect_trajectories:221 - Episode 312\n",
      "2021-09-07 17:18:00.670 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.671 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:18:00.671 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.285714285714285\n",
      "2021-09-07 17:18:00.672 | INFO     | src.policies:collect_trajectories:221 - Episode 313\n",
      "2021-09-07 17:18:00.676 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.677 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:00.677 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.125\n",
      "2021-09-07 17:18:00.677 | INFO     | src.policies:collect_trajectories:221 - Episode 314\n",
      "2021-09-07 17:18:00.685 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.686 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 34.0\n",
      "2021-09-07 17:18:00.686 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.555555555555557\n",
      "2021-09-07 17:18:00.687 | WARNING  | src.policies:train:144 - The actual batch size is 203, instead of 200\n",
      "2021-09-07 17:18:00.690 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:00.691 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3871898651123047, 'baseline_loss': 1.0836915969848633, 'total_loss': 0.15465593338012695}\n",
      "2021-09-07 17:18:00.692 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1324148178100586\n",
      "2021-09-07 17:18:00.693 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.48970991373062134\n",
      "2021-09-07 17:18:00.694 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1324148178100586\n",
      "2021-09-07 17:18:00.695 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.48970991373062134\n",
      "2021-09-07 17:18:00.696 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:00.698 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3512314558029175, 'baseline_loss': 1.0669728517532349, 'total_loss': 0.18225497007369995}\n",
      "2021-09-07 17:18:00.699 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.05779246240854263\n",
      "2021-09-07 17:18:00.700 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.47004419565200806\n",
      "2021-09-07 17:18:00.701 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.05779246240854263\n",
      "2021-09-07 17:18:00.703 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.47004419565200806\n",
      "2021-09-07 17:18:00.704 | INFO     | src.policies:train:123 - Epoch 37 / 800\n",
      "2021-09-07 17:18:00.705 | INFO     | src.policies:collect_trajectories:221 - Episode 315\n",
      "2021-09-07 17:18:00.709 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.710 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:18:00.710 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.0\n",
      "2021-09-07 17:18:00.710 | INFO     | src.policies:collect_trajectories:221 - Episode 316\n",
      "2021-09-07 17:18:00.713 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.714 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:18:00.714 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 17.0\n",
      "2021-09-07 17:18:00.715 | INFO     | src.policies:collect_trajectories:221 - Episode 317\n",
      "2021-09-07 17:18:00.718 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.719 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:18:00.719 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 15.0\n",
      "2021-09-07 17:18:00.720 | INFO     | src.policies:collect_trajectories:221 - Episode 318\n",
      "2021-09-07 17:18:00.725 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.726 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:00.726 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 16.25\n",
      "2021-09-07 17:18:00.726 | INFO     | src.policies:collect_trajectories:221 - Episode 319\n",
      "2021-09-07 17:18:00.732 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.733 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 30.0\n",
      "2021-09-07 17:18:00.733 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.0\n",
      "2021-09-07 17:18:00.733 | INFO     | src.policies:collect_trajectories:221 - Episode 320\n",
      "2021-09-07 17:18:00.738 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.739 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:00.740 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.166666666666668\n",
      "2021-09-07 17:18:00.740 | INFO     | src.policies:collect_trajectories:221 - Episode 321\n",
      "2021-09-07 17:18:00.749 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.750 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 46.0\n",
      "2021-09-07 17:18:00.750 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.0\n",
      "2021-09-07 17:18:00.751 | INFO     | src.policies:collect_trajectories:221 - Episode 322\n",
      "2021-09-07 17:18:00.756 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.756 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:18:00.757 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.5\n",
      "2021-09-07 17:18:00.757 | INFO     | src.policies:collect_trajectories:221 - Episode 323\n",
      "2021-09-07 17:18:00.766 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.767 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 49.0\n",
      "2021-09-07 17:18:00.767 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.444444444444443\n",
      "2021-09-07 17:18:00.768 | WARNING  | src.policies:train:144 - The actual batch size is 229, instead of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:00.771 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:00.772 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43674349784851074, 'baseline_loss': 1.1087031364440918, 'total_loss': 0.11760807037353516}\n",
      "2021-09-07 17:18:00.773 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3013475239276886\n",
      "2021-09-07 17:18:00.774 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4778384268283844\n",
      "2021-09-07 17:18:00.775 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3013475239276886\n",
      "2021-09-07 17:18:00.776 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4778384268283844\n",
      "2021-09-07 17:18:00.777 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:00.778 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4433099329471588, 'baseline_loss': 1.1775896549224854, 'total_loss': 0.14548489451408386}\n",
      "2021-09-07 17:18:00.779 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2674574851989746\n",
      "2021-09-07 17:18:00.780 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4103987514972687\n",
      "2021-09-07 17:18:00.782 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2674574851989746\n",
      "2021-09-07 17:18:00.783 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4103987514972687\n",
      "2021-09-07 17:18:00.785 | INFO     | src.policies:train:123 - Epoch 38 / 800\n",
      "2021-09-07 17:18:00.785 | INFO     | src.policies:collect_trajectories:221 - Episode 324\n",
      "2021-09-07 17:18:00.791 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.791 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 27.0\n",
      "2021-09-07 17:18:00.792 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 27.0\n",
      "2021-09-07 17:18:00.792 | INFO     | src.policies:collect_trajectories:221 - Episode 325\n",
      "2021-09-07 17:18:00.798 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.799 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:18:00.800 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.5\n",
      "2021-09-07 17:18:00.800 | INFO     | src.policies:collect_trajectories:221 - Episode 326\n",
      "2021-09-07 17:18:00.807 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.808 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:18:00.808 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.333333333333332\n",
      "2021-09-07 17:18:00.809 | INFO     | src.policies:collect_trajectories:221 - Episode 327\n",
      "2021-09-07 17:18:00.815 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.815 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 27.0\n",
      "2021-09-07 17:18:00.815 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.75\n",
      "2021-09-07 17:18:00.816 | INFO     | src.policies:collect_trajectories:221 - Episode 328\n",
      "2021-09-07 17:18:00.822 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.823 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:00.824 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.6\n",
      "2021-09-07 17:18:00.824 | INFO     | src.policies:collect_trajectories:221 - Episode 329\n",
      "2021-09-07 17:18:00.832 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.833 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 39.0\n",
      "2021-09-07 17:18:00.833 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 27.0\n",
      "2021-09-07 17:18:00.834 | INFO     | src.policies:collect_trajectories:221 - Episode 330\n",
      "2021-09-07 17:18:00.842 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.843 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 33.0\n",
      "2021-09-07 17:18:00.843 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 27.857142857142858\n",
      "2021-09-07 17:18:00.844 | INFO     | src.policies:collect_trajectories:221 - Episode 331\n",
      "2021-09-07 17:18:00.855 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.855 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 62.0\n",
      "2021-09-07 17:18:00.856 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 32.125\n",
      "2021-09-07 17:18:00.856 | WARNING  | src.policies:train:144 - The actual batch size is 257, instead of 200\n",
      "2021-09-07 17:18:00.860 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:00.861 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4184885025024414, 'baseline_loss': 1.2640016078948975, 'total_loss': 0.21351230144500732}\n",
      "2021-09-07 17:18:00.862 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07121734321117401\n",
      "2021-09-07 17:18:00.864 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.31789731979370117\n",
      "2021-09-07 17:18:00.865 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07121734321117401\n",
      "2021-09-07 17:18:00.866 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.31789731979370117\n",
      "2021-09-07 17:18:00.868 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:00.869 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41410353779792786, 'baseline_loss': 1.264737606048584, 'total_loss': 0.21826526522636414}\n",
      "2021-09-07 17:18:00.870 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11863212287425995\n",
      "2021-09-07 17:18:00.871 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.32825055718421936\n",
      "2021-09-07 17:18:00.872 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11863212287425995\n",
      "2021-09-07 17:18:00.873 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.32825055718421936\n",
      "2021-09-07 17:18:00.874 | INFO     | src.policies:train:123 - Epoch 39 / 800\n",
      "2021-09-07 17:18:00.875 | INFO     | src.policies:collect_trajectories:221 - Episode 332\n",
      "2021-09-07 17:18:00.885 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.886 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 63.0\n",
      "2021-09-07 17:18:00.887 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 63.0\n",
      "2021-09-07 17:18:00.887 | INFO     | src.policies:collect_trajectories:221 - Episode 333\n",
      "2021-09-07 17:18:00.891 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.892 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:18:00.892 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 38.5\n",
      "2021-09-07 17:18:00.892 | INFO     | src.policies:collect_trajectories:221 - Episode 334\n",
      "2021-09-07 17:18:00.897 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.898 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:18:00.898 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 31.333333333333332\n",
      "2021-09-07 17:18:00.899 | INFO     | src.policies:collect_trajectories:221 - Episode 335\n",
      "2021-09-07 17:18:00.911 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:00.911 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 59.0\n",
      "2021-09-07 17:18:00.912 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 38.25\n",
      "2021-09-07 17:18:00.912 | INFO     | src.policies:collect_trajectories:221 - Episode 336\n",
      "2021-09-07 17:18:00.918 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.919 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 27.0\n",
      "2021-09-07 17:18:00.919 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 36.0\n",
      "2021-09-07 17:18:00.920 | INFO     | src.policies:collect_trajectories:221 - Episode 337\n",
      "2021-09-07 17:18:00.930 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.931 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 51.0\n",
      "2021-09-07 17:18:00.932 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 38.5\n",
      "2021-09-07 17:18:00.932 | WARNING  | src.policies:train:144 - The actual batch size is 231, instead of 200\n",
      "2021-09-07 17:18:00.936 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:00.937 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.475411057472229, 'baseline_loss': 1.3158007860183716, 'total_loss': 0.1824893355369568}\n",
      "2021-09-07 17:18:00.938 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.146169513463974\n",
      "2021-09-07 17:18:00.939 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2977946102619171\n",
      "2021-09-07 17:18:00.941 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.146169513463974\n",
      "2021-09-07 17:18:00.942 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2977946102619171\n",
      "2021-09-07 17:18:00.943 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:00.945 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.44954022765159607, 'baseline_loss': 1.281324028968811, 'total_loss': 0.19112178683280945}\n",
      "2021-09-07 17:18:00.946 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.0916144996881485\n",
      "2021-09-07 17:18:00.947 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3872532844543457\n",
      "2021-09-07 17:18:00.948 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.0916144996881485\n",
      "2021-09-07 17:18:00.949 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3872532844543457\n",
      "2021-09-07 17:18:00.950 | INFO     | src.policies:train:123 - Epoch 40 / 800\n",
      "2021-09-07 17:18:00.951 | INFO     | src.policies:collect_trajectories:221 - Episode 338\n",
      "2021-09-07 17:18:00.956 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.957 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 29.0\n",
      "2021-09-07 17:18:00.957 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 29.0\n",
      "2021-09-07 17:18:00.958 | INFO     | src.policies:collect_trajectories:221 - Episode 339\n",
      "2021-09-07 17:18:00.963 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.964 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:00.965 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.5\n",
      "2021-09-07 17:18:00.965 | INFO     | src.policies:collect_trajectories:221 - Episode 340\n",
      "2021-09-07 17:18:00.970 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.970 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:18:00.971 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.0\n",
      "2021-09-07 17:18:00.971 | INFO     | src.policies:collect_trajectories:221 - Episode 341\n",
      "2021-09-07 17:18:00.976 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.976 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:18:00.977 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.25\n",
      "2021-09-07 17:18:00.977 | INFO     | src.policies:collect_trajectories:221 - Episode 342\n",
      "2021-09-07 17:18:00.984 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.985 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 26.0\n",
      "2021-09-07 17:18:00.985 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.0\n",
      "2021-09-07 17:18:00.987 | INFO     | src.policies:collect_trajectories:221 - Episode 343\n",
      "2021-09-07 17:18:00.992 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:00.992 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:18:00.993 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.166666666666668\n",
      "2021-09-07 17:18:00.993 | INFO     | src.policies:collect_trajectories:221 - Episode 344\n",
      "2021-09-07 17:18:00.999 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.000 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:18:01.000 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.285714285714285\n",
      "2021-09-07 17:18:01.001 | INFO     | src.policies:collect_trajectories:221 - Episode 345\n",
      "2021-09-07 17:18:01.007 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.008 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:18:01.009 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.375\n",
      "2021-09-07 17:18:01.009 | INFO     | src.policies:collect_trajectories:221 - Episode 346\n",
      "2021-09-07 17:18:01.013 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.014 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:18:01.014 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.333333333333332\n",
      "2021-09-07 17:18:01.015 | WARNING  | src.policies:train:144 - The actual batch size is 201, instead of 200\n",
      "2021-09-07 17:18:01.018 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:01.019 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3812985122203827, 'baseline_loss': 1.081005573272705, 'total_loss': 0.15920427441596985}\n",
      "2021-09-07 17:18:01.021 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.06889935582876205\n",
      "2021-09-07 17:18:01.022 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4431396424770355\n",
      "2021-09-07 17:18:01.023 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.06889935582876205\n",
      "2021-09-07 17:18:01.024 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4431396424770355\n",
      "2021-09-07 17:18:01.025 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:01.221 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4025951623916626, 'baseline_loss': 1.1032100915908813, 'total_loss': 0.14900988340377808}\n",
      "2021-09-07 17:18:01.222 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10667968541383743\n",
      "2021-09-07 17:18:01.223 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.43931829929351807\n",
      "2021-09-07 17:18:01.226 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10667968541383743\n",
      "2021-09-07 17:18:01.227 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.43931829929351807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:01.228 | INFO     | src.policies:train:123 - Epoch 41 / 800\n",
      "2021-09-07 17:18:01.229 | INFO     | src.policies:collect_trajectories:221 - Episode 347\n",
      "2021-09-07 17:18:01.233 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.233 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:01.234 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.0\n",
      "2021-09-07 17:18:01.234 | INFO     | src.policies:collect_trajectories:221 - Episode 348\n",
      "2021-09-07 17:18:01.237 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.238 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:18:01.238 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 16.5\n",
      "2021-09-07 17:18:01.239 | INFO     | src.policies:collect_trajectories:221 - Episode 349\n",
      "2021-09-07 17:18:01.243 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.244 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:18:01.244 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 16.333333333333332\n",
      "2021-09-07 17:18:01.245 | INFO     | src.policies:collect_trajectories:221 - Episode 350\n",
      "2021-09-07 17:18:01.260 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.261 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 79.0\n",
      "2021-09-07 17:18:01.261 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 32.0\n",
      "2021-09-07 17:18:01.262 | INFO     | src.policies:collect_trajectories:221 - Episode 351\n",
      "2021-09-07 17:18:01.268 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.268 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:18:01.269 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 28.8\n",
      "2021-09-07 17:18:01.269 | INFO     | src.policies:collect_trajectories:221 - Episode 352\n",
      "2021-09-07 17:18:01.272 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.273 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 9.0\n",
      "2021-09-07 17:18:01.273 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.5\n",
      "2021-09-07 17:18:01.274 | INFO     | src.policies:collect_trajectories:221 - Episode 353\n",
      "2021-09-07 17:18:01.278 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.278 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:18:01.279 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.285714285714285\n",
      "2021-09-07 17:18:01.279 | INFO     | src.policies:collect_trajectories:221 - Episode 354\n",
      "2021-09-07 17:18:01.284 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.285 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:01.285 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.75\n",
      "2021-09-07 17:18:01.285 | INFO     | src.policies:collect_trajectories:221 - Episode 355\n",
      "2021-09-07 17:18:01.290 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.291 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:18:01.291 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.11111111111111\n",
      "2021-09-07 17:18:01.292 | WARNING  | src.policies:train:144 - The actual batch size is 208, instead of 200\n",
      "2021-09-07 17:18:01.295 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:01.296 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.411681205034256, 'baseline_loss': 1.2000515460968018, 'total_loss': 0.1883445680141449}\n",
      "2021-09-07 17:18:01.297 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.0808309018611908\n",
      "2021-09-07 17:18:01.299 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4623170495033264\n",
      "2021-09-07 17:18:01.300 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.0808309018611908\n",
      "2021-09-07 17:18:01.301 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4623170495033264\n",
      "2021-09-07 17:18:01.302 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:01.304 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4856542646884918, 'baseline_loss': 1.1625999212265015, 'total_loss': 0.09564569592475891}\n",
      "2021-09-07 17:18:01.305 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31144317984580994\n",
      "2021-09-07 17:18:01.306 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3469561040401459\n",
      "2021-09-07 17:18:01.307 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31144317984580994\n",
      "2021-09-07 17:18:01.308 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3469561040401459\n",
      "2021-09-07 17:18:01.310 | INFO     | src.policies:train:123 - Epoch 42 / 800\n",
      "2021-09-07 17:18:01.310 | INFO     | src.policies:collect_trajectories:221 - Episode 356\n",
      "2021-09-07 17:18:01.324 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.325 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 81.0\n",
      "2021-09-07 17:18:01.325 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 81.0\n",
      "2021-09-07 17:18:01.326 | INFO     | src.policies:collect_trajectories:221 - Episode 357\n",
      "2021-09-07 17:18:01.332 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.333 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:18:01.333 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 53.0\n",
      "2021-09-07 17:18:01.333 | INFO     | src.policies:collect_trajectories:221 - Episode 358\n",
      "2021-09-07 17:18:01.341 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.342 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 37.0\n",
      "2021-09-07 17:18:01.342 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 47.666666666666664\n",
      "2021-09-07 17:18:01.343 | INFO     | src.policies:collect_trajectories:221 - Episode 359\n",
      "2021-09-07 17:18:01.347 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.348 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 8.0\n",
      "2021-09-07 17:18:01.348 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 37.75\n",
      "2021-09-07 17:18:01.348 | INFO     | src.policies:collect_trajectories:221 - Episode 360\n",
      "2021-09-07 17:18:01.353 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.353 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:18:01.354 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 33.6\n",
      "2021-09-07 17:18:01.354 | INFO     | src.policies:collect_trajectories:221 - Episode 361\n",
      "2021-09-07 17:18:01.365 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.365 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 60.0\n",
      "2021-09-07 17:18:01.366 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 38.0\n",
      "2021-09-07 17:18:01.366 | WARNING  | src.policies:train:144 - The actual batch size is 228, instead of 200\n",
      "2021-09-07 17:18:01.369 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:01.371 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4456835091114044, 'baseline_loss': 1.200881838798523, 'total_loss': 0.15475741028785706}\n",
      "2021-09-07 17:18:01.371 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07501809298992157\n",
      "2021-09-07 17:18:01.372 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.23358958959579468\n",
      "2021-09-07 17:18:01.373 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07501809298992157\n",
      "2021-09-07 17:18:01.374 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.23358958959579468\n",
      "2021-09-07 17:18:01.375 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:01.376 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4127517342567444, 'baseline_loss': 1.1794102191925049, 'total_loss': 0.17695337533950806}\n",
      "2021-09-07 17:18:01.377 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12739142775535583\n",
      "2021-09-07 17:18:01.378 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.25775498151779175\n",
      "2021-09-07 17:18:01.380 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12739142775535583\n",
      "2021-09-07 17:18:01.381 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.25775498151779175\n",
      "2021-09-07 17:18:01.382 | INFO     | src.policies:train:123 - Epoch 43 / 800\n",
      "2021-09-07 17:18:01.383 | INFO     | src.policies:collect_trajectories:221 - Episode 362\n",
      "2021-09-07 17:18:01.394 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.395 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 61.0\n",
      "2021-09-07 17:18:01.395 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 61.0\n",
      "2021-09-07 17:18:01.396 | INFO     | src.policies:collect_trajectories:221 - Episode 363\n",
      "2021-09-07 17:18:01.400 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.401 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 10.0\n",
      "2021-09-07 17:18:01.402 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 35.5\n",
      "2021-09-07 17:18:01.402 | INFO     | src.policies:collect_trajectories:221 - Episode 364\n",
      "2021-09-07 17:18:01.413 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.414 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 56.0\n",
      "2021-09-07 17:18:01.414 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 42.333333333333336\n",
      "2021-09-07 17:18:01.414 | INFO     | src.policies:collect_trajectories:221 - Episode 365\n",
      "2021-09-07 17:18:01.420 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.420 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:18:01.421 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 36.0\n",
      "2021-09-07 17:18:01.421 | INFO     | src.policies:collect_trajectories:221 - Episode 366\n",
      "2021-09-07 17:18:01.429 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.430 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 36.0\n",
      "2021-09-07 17:18:01.430 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 36.0\n",
      "2021-09-07 17:18:01.431 | INFO     | src.policies:collect_trajectories:221 - Episode 367\n",
      "2021-09-07 17:18:01.435 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.435 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:18:01.436 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 32.666666666666664\n",
      "2021-09-07 17:18:01.436 | INFO     | src.policies:collect_trajectories:221 - Episode 368\n",
      "2021-09-07 17:18:01.441 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.441 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 15.0\n",
      "2021-09-07 17:18:01.442 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 30.142857142857142\n",
      "2021-09-07 17:18:01.443 | WARNING  | src.policies:train:144 - The actual batch size is 211, instead of 200\n",
      "2021-09-07 17:18:01.446 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:01.448 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41827037930488586, 'baseline_loss': 1.1092642545700073, 'total_loss': 0.1363617479801178}\n",
      "2021-09-07 17:18:01.449 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09655418246984482\n",
      "2021-09-07 17:18:01.449 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3791006803512573\n",
      "2021-09-07 17:18:01.451 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09655418246984482\n",
      "2021-09-07 17:18:01.452 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3791006803512573\n",
      "2021-09-07 17:18:01.453 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:01.454 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40613582730293274, 'baseline_loss': 1.2381495237350464, 'total_loss': 0.21293893456459045}\n",
      "2021-09-07 17:18:01.455 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08664485067129135\n",
      "2021-09-07 17:18:01.455 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.23317168653011322\n",
      "2021-09-07 17:18:01.456 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08664485067129135\n",
      "2021-09-07 17:18:01.457 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.23317168653011322\n",
      "2021-09-07 17:18:01.459 | INFO     | src.policies:train:123 - Epoch 44 / 800\n",
      "2021-09-07 17:18:01.460 | INFO     | src.policies:collect_trajectories:221 - Episode 369\n",
      "2021-09-07 17:18:01.463 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.464 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:18:01.464 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 14.0\n",
      "2021-09-07 17:18:01.465 | INFO     | src.policies:collect_trajectories:221 - Episode 370\n",
      "2021-09-07 17:18:01.476 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.476 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 62.0\n",
      "2021-09-07 17:18:01.477 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 38.0\n",
      "2021-09-07 17:18:01.477 | INFO     | src.policies:collect_trajectories:221 - Episode 371\n",
      "2021-09-07 17:18:01.485 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.486 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 29.0\n",
      "2021-09-07 17:18:01.486 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 35.0\n",
      "2021-09-07 17:18:01.487 | INFO     | src.policies:collect_trajectories:221 - Episode 372\n",
      "2021-09-07 17:18:01.496 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.496 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 50.0\n",
      "2021-09-07 17:18:01.497 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 38.75\n",
      "2021-09-07 17:18:01.497 | INFO     | src.policies:collect_trajectories:221 - Episode 373\n",
      "2021-09-07 17:18:01.502 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.503 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:01.503 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 34.6\n",
      "2021-09-07 17:18:01.504 | INFO     | src.policies:collect_trajectories:221 - Episode 374\n",
      "2021-09-07 17:18:01.516 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.517 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 64.0\n",
      "2021-09-07 17:18:01.517 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 39.5\n",
      "2021-09-07 17:18:01.518 | WARNING  | src.policies:train:144 - The actual batch size is 237, instead of 200\n",
      "2021-09-07 17:18:01.522 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:01.524 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3692206144332886, 'baseline_loss': 1.0083894729614258, 'total_loss': 0.13497412204742432}\n",
      "2021-09-07 17:18:01.525 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11954299360513687\n",
      "2021-09-07 17:18:01.527 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4357776343822479\n",
      "2021-09-07 17:18:01.528 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11954299360513687\n",
      "2021-09-07 17:18:01.529 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4357776343822479\n",
      "2021-09-07 17:18:01.531 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:01.532 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40181270241737366, 'baseline_loss': 1.0155730247497559, 'total_loss': 0.10597380995750427}\n",
      "2021-09-07 17:18:01.532 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1668320745229721\n",
      "2021-09-07 17:18:01.533 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6351873278617859\n",
      "2021-09-07 17:18:01.534 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1668320745229721\n",
      "2021-09-07 17:18:01.535 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:01.537 | INFO     | src.policies:train:123 - Epoch 45 / 800\n",
      "2021-09-07 17:18:01.537 | INFO     | src.policies:collect_trajectories:221 - Episode 375\n",
      "2021-09-07 17:18:01.543 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.543 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:18:01.544 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.0\n",
      "2021-09-07 17:18:01.544 | INFO     | src.policies:collect_trajectories:221 - Episode 376\n",
      "2021-09-07 17:18:01.550 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.550 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:18:01.551 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.0\n",
      "2021-09-07 17:18:01.551 | INFO     | src.policies:collect_trajectories:221 - Episode 377\n",
      "2021-09-07 17:18:01.560 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.560 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 44.0\n",
      "2021-09-07 17:18:01.561 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 28.666666666666668\n",
      "2021-09-07 17:18:01.561 | INFO     | src.policies:collect_trajectories:221 - Episode 378\n",
      "2021-09-07 17:18:01.571 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.571 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 45.0\n",
      "2021-09-07 17:18:01.572 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 32.75\n",
      "2021-09-07 17:18:01.572 | INFO     | src.policies:collect_trajectories:221 - Episode 379\n",
      "2021-09-07 17:18:01.580 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.581 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 34.0\n",
      "2021-09-07 17:18:01.581 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 33.0\n",
      "2021-09-07 17:18:01.581 | INFO     | src.policies:collect_trajectories:221 - Episode 380\n",
      "2021-09-07 17:18:01.594 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.594 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 65.0\n",
      "2021-09-07 17:18:01.595 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 38.333333333333336\n",
      "2021-09-07 17:18:01.595 | WARNING  | src.policies:train:144 - The actual batch size is 230, instead of 200\n",
      "2021-09-07 17:18:01.598 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:01.601 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.46051478385925293, 'baseline_loss': 1.2945730686187744, 'total_loss': 0.18677175045013428}\n",
      "2021-09-07 17:18:01.602 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1193423941731453\n",
      "2021-09-07 17:18:01.603 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.42113202810287476\n",
      "2021-09-07 17:18:01.605 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1193423941731453\n",
      "2021-09-07 17:18:01.606 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.42113202810287476\n",
      "2021-09-07 17:18:01.608 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:01.609 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.48085227608680725, 'baseline_loss': 1.2863245010375977, 'total_loss': 0.16230997443199158}\n",
      "2021-09-07 17:18:01.610 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09614131599664688\n",
      "2021-09-07 17:18:01.611 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.28570279479026794\n",
      "2021-09-07 17:18:01.612 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09614131599664688\n",
      "2021-09-07 17:18:01.613 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.28570279479026794\n",
      "2021-09-07 17:18:01.615 | INFO     | src.policies:train:123 - Epoch 46 / 800\n",
      "2021-09-07 17:18:01.615 | INFO     | src.policies:collect_trajectories:221 - Episode 381\n",
      "2021-09-07 17:18:01.625 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.625 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 51.0\n",
      "2021-09-07 17:18:01.626 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 51.0\n",
      "2021-09-07 17:18:01.626 | INFO     | src.policies:collect_trajectories:221 - Episode 382\n",
      "2021-09-07 17:18:01.633 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.633 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 36.0\n",
      "2021-09-07 17:18:01.634 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 43.5\n",
      "2021-09-07 17:18:01.634 | INFO     | src.policies:collect_trajectories:221 - Episode 383\n",
      "2021-09-07 17:18:01.639 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.640 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:18:01.640 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 36.666666666666664\n",
      "2021-09-07 17:18:01.640 | INFO     | src.policies:collect_trajectories:221 - Episode 384\n",
      "2021-09-07 17:18:01.645 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.646 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:18:01.647 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 31.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:01.647 | INFO     | src.policies:collect_trajectories:221 - Episode 385\n",
      "2021-09-07 17:18:01.651 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.652 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 15.0\n",
      "2021-09-07 17:18:01.652 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 28.4\n",
      "2021-09-07 17:18:01.652 | INFO     | src.policies:collect_trajectories:221 - Episode 386\n",
      "2021-09-07 17:18:01.656 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.656 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:18:01.657 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.666666666666668\n",
      "2021-09-07 17:18:01.657 | INFO     | src.policies:collect_trajectories:221 - Episode 387\n",
      "2021-09-07 17:18:01.663 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.664 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 26.0\n",
      "2021-09-07 17:18:01.665 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.714285714285715\n",
      "2021-09-07 17:18:01.665 | INFO     | src.policies:collect_trajectories:221 - Episode 388\n",
      "2021-09-07 17:18:01.669 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.670 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:18:01.670 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.125\n",
      "2021-09-07 17:18:01.670 | INFO     | src.policies:collect_trajectories:221 - Episode 389\n",
      "2021-09-07 17:18:01.677 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.677 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 37.0\n",
      "2021-09-07 17:18:01.678 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.555555555555557\n",
      "2021-09-07 17:18:01.679 | WARNING  | src.policies:train:144 - The actual batch size is 230, instead of 200\n",
      "2021-09-07 17:18:01.683 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:01.685 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45737311244010925, 'baseline_loss': 1.1325922012329102, 'total_loss': 0.10892298817634583}\n",
      "2021-09-07 17:18:01.686 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07707679271697998\n",
      "2021-09-07 17:18:01.687 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.35810691118240356\n",
      "2021-09-07 17:18:01.688 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07707679271697998\n",
      "2021-09-07 17:18:01.689 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.35810691118240356\n",
      "2021-09-07 17:18:01.691 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:01.692 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3816693425178528, 'baseline_loss': 0.9593151211738586, 'total_loss': 0.09798821806907654}\n",
      "2021-09-07 17:18:01.693 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12290945649147034\n",
      "2021-09-07 17:18:01.694 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4660164713859558\n",
      "2021-09-07 17:18:01.695 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12290945649147034\n",
      "2021-09-07 17:18:01.696 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4660164713859558\n",
      "2021-09-07 17:18:01.697 | INFO     | src.policies:train:123 - Epoch 47 / 800\n",
      "2021-09-07 17:18:01.698 | INFO     | src.policies:collect_trajectories:221 - Episode 390\n",
      "2021-09-07 17:18:01.705 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.706 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 39.0\n",
      "2021-09-07 17:18:01.707 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 39.0\n",
      "2021-09-07 17:18:01.707 | INFO     | src.policies:collect_trajectories:221 - Episode 391\n",
      "2021-09-07 17:18:01.715 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.716 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 46.0\n",
      "2021-09-07 17:18:01.716 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 42.5\n",
      "2021-09-07 17:18:01.716 | INFO     | src.policies:collect_trajectories:221 - Episode 392\n",
      "2021-09-07 17:18:01.806 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.806 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 36.0\n",
      "2021-09-07 17:18:01.807 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 40.333333333333336\n",
      "2021-09-07 17:18:01.807 | INFO     | src.policies:collect_trajectories:221 - Episode 393\n",
      "2021-09-07 17:18:01.812 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.812 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:18:01.813 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 33.0\n",
      "2021-09-07 17:18:01.814 | INFO     | src.policies:collect_trajectories:221 - Episode 394\n",
      "2021-09-07 17:18:01.818 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.819 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:18:01.819 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 28.6\n",
      "2021-09-07 17:18:01.820 | INFO     | src.policies:collect_trajectories:221 - Episode 395\n",
      "2021-09-07 17:18:01.827 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.828 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 38.0\n",
      "2021-09-07 17:18:01.828 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 30.166666666666668\n",
      "2021-09-07 17:18:01.829 | INFO     | src.policies:collect_trajectories:221 - Episode 396\n",
      "2021-09-07 17:18:01.835 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.836 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 31.0\n",
      "2021-09-07 17:18:01.836 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 30.285714285714285\n",
      "2021-09-07 17:18:01.837 | WARNING  | src.policies:train:144 - The actual batch size is 212, instead of 200\n",
      "2021-09-07 17:18:01.840 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:01.842 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.44731560349464417, 'baseline_loss': 1.050657868385315, 'total_loss': 0.0780133306980133}\n",
      "2021-09-07 17:18:01.843 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11553046107292175\n",
      "2021-09-07 17:18:01.843 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.36312246322631836\n",
      "2021-09-07 17:18:01.845 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11553046107292175\n",
      "2021-09-07 17:18:01.846 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.36312246322631836\n",
      "2021-09-07 17:18:01.848 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:01.849 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5048734545707703, 'baseline_loss': 1.2138293981552124, 'total_loss': 0.10204124450683594}\n",
      "2021-09-07 17:18:01.850 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10557867586612701\n",
      "2021-09-07 17:18:01.851 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.20648349821567535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:01.852 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10557867586612701\n",
      "2021-09-07 17:18:01.853 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.20648349821567535\n",
      "2021-09-07 17:18:01.855 | INFO     | src.policies:train:123 - Epoch 48 / 800\n",
      "2021-09-07 17:18:01.856 | INFO     | src.policies:collect_trajectories:221 - Episode 397\n",
      "2021-09-07 17:18:01.860 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.860 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:18:01.861 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 14.0\n",
      "2021-09-07 17:18:01.861 | INFO     | src.policies:collect_trajectories:221 - Episode 398\n",
      "2021-09-07 17:18:01.868 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.868 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:18:01.869 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.5\n",
      "2021-09-07 17:18:01.869 | INFO     | src.policies:collect_trajectories:221 - Episode 399\n",
      "2021-09-07 17:18:01.873 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.874 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:18:01.874 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.666666666666668\n",
      "2021-09-07 17:18:01.875 | INFO     | src.policies:collect_trajectories:221 - Episode 400\n",
      "2021-09-07 17:18:01.883 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.883 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 38.0\n",
      "2021-09-07 17:18:01.884 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.5\n",
      "2021-09-07 17:18:01.884 | INFO     | src.policies:collect_trajectories:221 - Episode 401\n",
      "2021-09-07 17:18:01.888 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.889 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:18:01.890 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.4\n",
      "2021-09-07 17:18:01.890 | INFO     | src.policies:collect_trajectories:221 - Episode 402\n",
      "2021-09-07 17:18:01.893 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.894 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:18:01.894 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.666666666666668\n",
      "2021-09-07 17:18:01.895 | INFO     | src.policies:collect_trajectories:221 - Episode 403\n",
      "2021-09-07 17:18:01.903 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.903 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 39.0\n",
      "2021-09-07 17:18:01.904 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.428571428571427\n",
      "2021-09-07 17:18:01.904 | INFO     | src.policies:collect_trajectories:221 - Episode 404\n",
      "2021-09-07 17:18:01.913 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.914 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 48.0\n",
      "2021-09-07 17:18:01.914 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.625\n",
      "2021-09-07 17:18:01.915 | WARNING  | src.policies:train:144 - The actual batch size is 205, instead of 200\n",
      "2021-09-07 17:18:01.917 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:01.920 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4994124472141266, 'baseline_loss': 1.1778018474578857, 'total_loss': 0.08948847651481628}\n",
      "2021-09-07 17:18:01.921 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11704863607883453\n",
      "2021-09-07 17:18:01.922 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.27175143361091614\n",
      "2021-09-07 17:18:01.924 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11704863607883453\n",
      "2021-09-07 17:18:01.925 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.27175143361091614\n",
      "2021-09-07 17:18:01.927 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:01.928 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.47130435705184937, 'baseline_loss': 1.0901265144348145, 'total_loss': 0.07375890016555786}\n",
      "2021-09-07 17:18:01.929 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13719609379768372\n",
      "2021-09-07 17:18:01.930 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.34283244609832764\n",
      "2021-09-07 17:18:01.931 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13719609379768372\n",
      "2021-09-07 17:18:01.932 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.34283244609832764\n",
      "2021-09-07 17:18:01.934 | INFO     | src.policies:train:123 - Epoch 49 / 800\n",
      "2021-09-07 17:18:01.934 | INFO     | src.policies:collect_trajectories:221 - Episode 405\n",
      "2021-09-07 17:18:01.945 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.946 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 64.0\n",
      "2021-09-07 17:18:01.947 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 64.0\n",
      "2021-09-07 17:18:01.947 | INFO     | src.policies:collect_trajectories:221 - Episode 406\n",
      "2021-09-07 17:18:01.950 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.951 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:18:01.951 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 37.5\n",
      "2021-09-07 17:18:01.952 | INFO     | src.policies:collect_trajectories:221 - Episode 407\n",
      "2021-09-07 17:18:01.955 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.956 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 10.0\n",
      "2021-09-07 17:18:01.956 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 28.333333333333332\n",
      "2021-09-07 17:18:01.956 | INFO     | src.policies:collect_trajectories:221 - Episode 408\n",
      "2021-09-07 17:18:01.967 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.968 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 57.0\n",
      "2021-09-07 17:18:01.968 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 35.5\n",
      "2021-09-07 17:18:01.969 | INFO     | src.policies:collect_trajectories:221 - Episode 409\n",
      "2021-09-07 17:18:01.975 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.975 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 31.0\n",
      "2021-09-07 17:18:01.976 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 34.6\n",
      "2021-09-07 17:18:01.976 | INFO     | src.policies:collect_trajectories:221 - Episode 410\n",
      "2021-09-07 17:18:01.982 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.982 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:18:01.983 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 31.833333333333332\n",
      "2021-09-07 17:18:01.983 | INFO     | src.policies:collect_trajectories:221 - Episode 411\n",
      "2021-09-07 17:18:01.990 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:01.990 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:01.991 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 30.285714285714285\n",
      "2021-09-07 17:18:01.991 | WARNING  | src.policies:train:144 - The actual batch size is 212, instead of 200\n",
      "2021-09-07 17:18:01.994 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:01.995 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5116304159164429, 'baseline_loss': 1.2619110345840454, 'total_loss': 0.11932510137557983}\n",
      "2021-09-07 17:18:01.996 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15278927981853485\n",
      "2021-09-07 17:18:01.997 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.34671786427497864\n",
      "2021-09-07 17:18:01.998 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15278927981853485\n",
      "2021-09-07 17:18:02.000 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.34671786427497864\n",
      "2021-09-07 17:18:02.002 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:02.003 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.47166576981544495, 'baseline_loss': 1.1126999855041504, 'total_loss': 0.08468422293663025}\n",
      "2021-09-07 17:18:02.004 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11892762035131454\n",
      "2021-09-07 17:18:02.005 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.27075687050819397\n",
      "2021-09-07 17:18:02.007 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11892762035131454\n",
      "2021-09-07 17:18:02.008 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.27075687050819397\n",
      "2021-09-07 17:18:02.010 | INFO     | src.policies:train:123 - Epoch 50 / 800\n",
      "2021-09-07 17:18:02.010 | INFO     | src.policies:collect_trajectories:221 - Episode 412\n",
      "2021-09-07 17:18:02.015 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.016 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:18:02.016 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.0\n",
      "2021-09-07 17:18:02.016 | INFO     | src.policies:collect_trajectories:221 - Episode 413\n",
      "2021-09-07 17:18:02.024 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.025 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 29.0\n",
      "2021-09-07 17:18:02.025 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 26.5\n",
      "2021-09-07 17:18:02.026 | INFO     | src.policies:collect_trajectories:221 - Episode 414\n",
      "2021-09-07 17:18:02.031 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.031 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:18:02.032 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.666666666666668\n",
      "2021-09-07 17:18:02.032 | INFO     | src.policies:collect_trajectories:221 - Episode 415\n",
      "2021-09-07 17:18:02.037 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.037 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:18:02.038 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.25\n",
      "2021-09-07 17:18:02.038 | INFO     | src.policies:collect_trajectories:221 - Episode 416\n",
      "2021-09-07 17:18:02.046 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.047 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 30.0\n",
      "2021-09-07 17:18:02.048 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.8\n",
      "2021-09-07 17:18:02.048 | INFO     | src.policies:collect_trajectories:221 - Episode 417\n",
      "2021-09-07 17:18:02.052 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.052 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:18:02.052 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.0\n",
      "2021-09-07 17:18:02.053 | INFO     | src.policies:collect_trajectories:221 - Episode 418\n",
      "2021-09-07 17:18:02.060 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.060 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 34.0\n",
      "2021-09-07 17:18:02.061 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.714285714285715\n",
      "2021-09-07 17:18:02.061 | INFO     | src.policies:collect_trajectories:221 - Episode 419\n",
      "2021-09-07 17:18:02.069 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.070 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 28.0\n",
      "2021-09-07 17:18:02.070 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.25\n",
      "2021-09-07 17:18:02.071 | INFO     | src.policies:collect_trajectories:221 - Episode 420\n",
      "2021-09-07 17:18:02.083 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.084 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 67.0\n",
      "2021-09-07 17:18:02.084 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 29.0\n",
      "2021-09-07 17:18:02.085 | WARNING  | src.policies:train:144 - The actual batch size is 261, instead of 200\n",
      "2021-09-07 17:18:02.089 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:02.090 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.38264286518096924, 'baseline_loss': 1.0705355405807495, 'total_loss': 0.15262490510940552}\n",
      "2021-09-07 17:18:02.091 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1518910676240921\n",
      "2021-09-07 17:18:02.092 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5651631355285645\n",
      "2021-09-07 17:18:02.093 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1518910676240921\n",
      "2021-09-07 17:18:02.094 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:02.095 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:02.097 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41006726026535034, 'baseline_loss': 1.0382616519927979, 'total_loss': 0.10906356573104858}\n",
      "2021-09-07 17:18:02.098 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07495937496423721\n",
      "2021-09-07 17:18:02.099 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.471455842256546\n",
      "2021-09-07 17:18:02.100 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07495937496423721\n",
      "2021-09-07 17:18:02.101 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.471455842256546\n",
      "2021-09-07 17:18:02.103 | INFO     | src.policies:train:123 - Epoch 51 / 800\n",
      "2021-09-07 17:18:02.104 | INFO     | src.policies:collect_trajectories:221 - Episode 421\n",
      "2021-09-07 17:18:02.109 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.109 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:18:02.110 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.0\n",
      "2021-09-07 17:18:02.110 | INFO     | src.policies:collect_trajectories:221 - Episode 422\n",
      "2021-09-07 17:18:02.115 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.115 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:18:02.116 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:02.116 | INFO     | src.policies:collect_trajectories:221 - Episode 423\n",
      "2021-09-07 17:18:02.121 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.122 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:18:02.122 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.333333333333332\n",
      "2021-09-07 17:18:02.123 | INFO     | src.policies:collect_trajectories:221 - Episode 424\n",
      "2021-09-07 17:18:02.130 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.130 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 22.0\n",
      "2021-09-07 17:18:02.131 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.25\n",
      "2021-09-07 17:18:02.131 | INFO     | src.policies:collect_trajectories:221 - Episode 425\n",
      "2021-09-07 17:18:02.148 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.149 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 94.0\n",
      "2021-09-07 17:18:02.149 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 34.2\n",
      "2021-09-07 17:18:02.150 | INFO     | src.policies:collect_trajectories:221 - Episode 426\n",
      "2021-09-07 17:18:02.155 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.156 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 22.0\n",
      "2021-09-07 17:18:02.156 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 32.166666666666664\n",
      "2021-09-07 17:18:02.157 | INFO     | src.policies:collect_trajectories:221 - Episode 427\n",
      "2021-09-07 17:18:02.162 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.163 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:02.163 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 30.428571428571427\n",
      "2021-09-07 17:18:02.164 | WARNING  | src.policies:train:144 - The actual batch size is 213, instead of 200\n",
      "2021-09-07 17:18:02.166 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:02.168 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43658292293548584, 'baseline_loss': 1.0109882354736328, 'total_loss': 0.06891119480133057}\n",
      "2021-09-07 17:18:02.169 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10009495168924332\n",
      "2021-09-07 17:18:02.170 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3709057867527008\n",
      "2021-09-07 17:18:02.171 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10009495168924332\n",
      "2021-09-07 17:18:02.172 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3709057867527008\n",
      "2021-09-07 17:18:02.173 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:02.174 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.44944971799850464, 'baseline_loss': 1.147636890411377, 'total_loss': 0.12436872720718384}\n",
      "2021-09-07 17:18:02.175 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07925593107938766\n",
      "2021-09-07 17:18:02.176 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.31977975368499756\n",
      "2021-09-07 17:18:02.177 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07925593107938766\n",
      "2021-09-07 17:18:02.178 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.31977975368499756\n",
      "2021-09-07 17:18:02.179 | INFO     | src.policies:train:123 - Epoch 52 / 800\n",
      "2021-09-07 17:18:02.180 | INFO     | src.policies:collect_trajectories:221 - Episode 428\n",
      "2021-09-07 17:18:02.184 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.184 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:18:02.185 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 14.0\n",
      "2021-09-07 17:18:02.185 | INFO     | src.policies:collect_trajectories:221 - Episode 429\n",
      "2021-09-07 17:18:02.190 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.190 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:18:02.190 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 13.0\n",
      "2021-09-07 17:18:02.191 | INFO     | src.policies:collect_trajectories:221 - Episode 430\n",
      "2021-09-07 17:18:02.196 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.196 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:18:02.197 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 16.333333333333332\n",
      "2021-09-07 17:18:02.197 | INFO     | src.policies:collect_trajectories:221 - Episode 431\n",
      "2021-09-07 17:18:02.202 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.202 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:18:02.203 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 16.75\n",
      "2021-09-07 17:18:02.203 | INFO     | src.policies:collect_trajectories:221 - Episode 432\n",
      "2021-09-07 17:18:02.209 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.210 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 22.0\n",
      "2021-09-07 17:18:02.210 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 17.8\n",
      "2021-09-07 17:18:02.211 | INFO     | src.policies:collect_trajectories:221 - Episode 433\n",
      "2021-09-07 17:18:02.215 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.215 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:18:02.215 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 17.166666666666668\n",
      "2021-09-07 17:18:02.216 | INFO     | src.policies:collect_trajectories:221 - Episode 434\n",
      "2021-09-07 17:18:02.221 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.222 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 27.0\n",
      "2021-09-07 17:18:02.223 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.571428571428573\n",
      "2021-09-07 17:18:02.223 | INFO     | src.policies:collect_trajectories:221 - Episode 435\n",
      "2021-09-07 17:18:02.228 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.229 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:02.229 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.75\n",
      "2021-09-07 17:18:02.229 | INFO     | src.policies:collect_trajectories:221 - Episode 436\n",
      "2021-09-07 17:18:02.237 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.238 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 41.0\n",
      "2021-09-07 17:18:02.238 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.22222222222222\n",
      "2021-09-07 17:18:02.239 | INFO     | src.policies:collect_trajectories:221 - Episode 437\n",
      "2021-09-07 17:18:02.253 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.253 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 73.0\n",
      "2021-09-07 17:18:02.254 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 26.4\n",
      "2021-09-07 17:18:02.255 | WARNING  | src.policies:train:144 - The actual batch size is 264, instead of 200\n",
      "2021-09-07 17:18:02.258 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:02.260 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5169611573219299, 'baseline_loss': 1.3333277702331543, 'total_loss': 0.14970272779464722}\n",
      "2021-09-07 17:18:02.261 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07441367954015732\n",
      "2021-09-07 17:18:02.262 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.23442316055297852\n",
      "2021-09-07 17:18:02.263 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07441367954015732\n",
      "2021-09-07 17:18:02.265 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.23442316055297852\n",
      "2021-09-07 17:18:02.267 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:02.268 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4976745545864105, 'baseline_loss': 1.2325730323791504, 'total_loss': 0.11861196160316467}\n",
      "2021-09-07 17:18:02.269 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11284688115119934\n",
      "2021-09-07 17:18:02.270 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3340406119823456\n",
      "2021-09-07 17:18:02.271 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11284688115119934\n",
      "2021-09-07 17:18:02.272 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3340406119823456\n",
      "2021-09-07 17:18:02.274 | INFO     | src.policies:train:123 - Epoch 53 / 800\n",
      "2021-09-07 17:18:02.274 | INFO     | src.policies:collect_trajectories:221 - Episode 438\n",
      "2021-09-07 17:18:02.282 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.283 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 44.0\n",
      "2021-09-07 17:18:02.283 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 44.0\n",
      "2021-09-07 17:18:02.284 | INFO     | src.policies:collect_trajectories:221 - Episode 439\n",
      "2021-09-07 17:18:02.292 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.292 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 30.0\n",
      "2021-09-07 17:18:02.293 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 37.0\n",
      "2021-09-07 17:18:02.293 | INFO     | src.policies:collect_trajectories:221 - Episode 440\n",
      "2021-09-07 17:18:02.300 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.301 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 30.0\n",
      "2021-09-07 17:18:02.301 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 34.666666666666664\n",
      "2021-09-07 17:18:02.301 | INFO     | src.policies:collect_trajectories:221 - Episode 441\n",
      "2021-09-07 17:18:02.486 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.486 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 31.0\n",
      "2021-09-07 17:18:02.487 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 33.75\n",
      "2021-09-07 17:18:02.487 | INFO     | src.policies:collect_trajectories:221 - Episode 442\n",
      "2021-09-07 17:18:02.494 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.495 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 32.0\n",
      "2021-09-07 17:18:02.496 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 33.4\n",
      "2021-09-07 17:18:02.497 | INFO     | src.policies:collect_trajectories:221 - Episode 443\n",
      "2021-09-07 17:18:02.500 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.501 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:18:02.502 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 30.5\n",
      "2021-09-07 17:18:02.502 | INFO     | src.policies:collect_trajectories:221 - Episode 444\n",
      "2021-09-07 17:18:02.509 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.509 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 26.0\n",
      "2021-09-07 17:18:02.510 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 29.857142857142858\n",
      "2021-09-07 17:18:02.510 | WARNING  | src.policies:train:144 - The actual batch size is 209, instead of 200\n",
      "2021-09-07 17:18:02.513 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:02.515 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5641950368881226, 'baseline_loss': 1.4274888038635254, 'total_loss': 0.14954936504364014}\n",
      "2021-09-07 17:18:02.516 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14915193617343903\n",
      "2021-09-07 17:18:02.516 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5036745667457581\n",
      "2021-09-07 17:18:02.518 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14915193617343903\n",
      "2021-09-07 17:18:02.519 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999989867210388\n",
      "2021-09-07 17:18:02.521 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:02.523 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5494977235794067, 'baseline_loss': 1.381242036819458, 'total_loss': 0.14112329483032227}\n",
      "2021-09-07 17:18:02.524 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1300361156463623\n",
      "2021-09-07 17:18:02.525 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.466457724571228\n",
      "2021-09-07 17:18:02.526 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1300361156463623\n",
      "2021-09-07 17:18:02.527 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.466457724571228\n",
      "2021-09-07 17:18:02.529 | INFO     | src.policies:train:123 - Epoch 54 / 800\n",
      "2021-09-07 17:18:02.529 | INFO     | src.policies:collect_trajectories:221 - Episode 445\n",
      "2021-09-07 17:18:02.536 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.536 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 34.0\n",
      "2021-09-07 17:18:02.537 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 34.0\n",
      "2021-09-07 17:18:02.537 | INFO     | src.policies:collect_trajectories:221 - Episode 446\n",
      "2021-09-07 17:18:02.543 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.544 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:18:02.545 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 27.5\n",
      "2021-09-07 17:18:02.545 | INFO     | src.policies:collect_trajectories:221 - Episode 447\n",
      "2021-09-07 17:18:02.549 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.549 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:18:02.550 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.333333333333332\n",
      "2021-09-07 17:18:02.550 | INFO     | src.policies:collect_trajectories:221 - Episode 448\n",
      "2021-09-07 17:18:02.554 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.554 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:18:02.555 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.5\n",
      "2021-09-07 17:18:02.555 | INFO     | src.policies:collect_trajectories:221 - Episode 449\n",
      "2021-09-07 17:18:02.559 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.559 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:02.560 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.0\n",
      "2021-09-07 17:18:02.560 | INFO     | src.policies:collect_trajectories:221 - Episode 450\n",
      "2021-09-07 17:18:02.569 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.570 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 40.0\n",
      "2021-09-07 17:18:02.570 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.666666666666668\n",
      "2021-09-07 17:18:02.571 | INFO     | src.policies:collect_trajectories:221 - Episode 451\n",
      "2021-09-07 17:18:02.577 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.577 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 27.0\n",
      "2021-09-07 17:18:02.578 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.428571428571427\n",
      "2021-09-07 17:18:02.578 | INFO     | src.policies:collect_trajectories:221 - Episode 452\n",
      "2021-09-07 17:18:02.585 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.586 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:18:02.586 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.0\n",
      "2021-09-07 17:18:02.587 | INFO     | src.policies:collect_trajectories:221 - Episode 453\n",
      "2021-09-07 17:18:02.593 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.593 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:18:02.594 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.333333333333332\n",
      "2021-09-07 17:18:02.594 | WARNING  | src.policies:train:144 - The actual batch size is 201, instead of 200\n",
      "2021-09-07 17:18:02.598 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:02.599 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4696422815322876, 'baseline_loss': 0.9521868824958801, 'total_loss': 0.006451159715652466}\n",
      "2021-09-07 17:18:02.601 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13714174926280975\n",
      "2021-09-07 17:18:02.603 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3065803349018097\n",
      "2021-09-07 17:18:02.604 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13714174926280975\n",
      "2021-09-07 17:18:02.605 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3065803349018097\n",
      "2021-09-07 17:18:02.606 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:02.608 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.47511759400367737, 'baseline_loss': 1.0182790756225586, 'total_loss': 0.03402194380760193}\n",
      "2021-09-07 17:18:02.609 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14136925339698792\n",
      "2021-09-07 17:18:02.610 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3631880581378937\n",
      "2021-09-07 17:18:02.611 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14136925339698792\n",
      "2021-09-07 17:18:02.612 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3631880581378937\n",
      "2021-09-07 17:18:02.613 | INFO     | src.policies:train:123 - Epoch 55 / 800\n",
      "2021-09-07 17:18:02.614 | INFO     | src.policies:collect_trajectories:221 - Episode 454\n",
      "2021-09-07 17:18:02.621 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.622 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 35.0\n",
      "2021-09-07 17:18:02.622 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 35.0\n",
      "2021-09-07 17:18:02.623 | INFO     | src.policies:collect_trajectories:221 - Episode 455\n",
      "2021-09-07 17:18:02.634 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.635 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 57.0\n",
      "2021-09-07 17:18:02.635 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 46.0\n",
      "2021-09-07 17:18:02.636 | INFO     | src.policies:collect_trajectories:221 - Episode 456\n",
      "2021-09-07 17:18:02.640 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.640 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:18:02.641 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 35.0\n",
      "2021-09-07 17:18:02.642 | INFO     | src.policies:collect_trajectories:221 - Episode 457\n",
      "2021-09-07 17:18:02.647 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.647 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:18:02.648 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 29.25\n",
      "2021-09-07 17:18:02.648 | INFO     | src.policies:collect_trajectories:221 - Episode 458\n",
      "2021-09-07 17:18:02.655 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.655 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 30.0\n",
      "2021-09-07 17:18:02.656 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 29.4\n",
      "2021-09-07 17:18:02.656 | INFO     | src.policies:collect_trajectories:221 - Episode 459\n",
      "2021-09-07 17:18:02.661 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.662 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:18:02.663 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 27.5\n",
      "2021-09-07 17:18:02.663 | INFO     | src.policies:collect_trajectories:221 - Episode 460\n",
      "2021-09-07 17:18:02.668 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.669 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:02.669 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 26.428571428571427\n",
      "2021-09-07 17:18:02.670 | INFO     | src.policies:collect_trajectories:221 - Episode 461\n",
      "2021-09-07 17:18:02.674 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.674 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:18:02.675 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.75\n",
      "2021-09-07 17:18:02.675 | INFO     | src.policies:collect_trajectories:221 - Episode 462\n",
      "2021-09-07 17:18:02.680 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.681 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:18:02.681 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.0\n",
      "2021-09-07 17:18:02.682 | WARNING  | src.policies:train:144 - The actual batch size is 216, instead of 200\n",
      "2021-09-07 17:18:02.686 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:02.688 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.561782717704773, 'baseline_loss': 1.3651833534240723, 'total_loss': 0.12080895900726318}\n",
      "2021-09-07 17:18:02.689 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19054552912712097\n",
      "2021-09-07 17:18:02.690 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.32866260409355164\n",
      "2021-09-07 17:18:02.691 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19054552912712097\n",
      "2021-09-07 17:18:02.692 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.32866260409355164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:02.693 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:02.694 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.46958351135253906, 'baseline_loss': 1.2992199659347534, 'total_loss': 0.18002647161483765}\n",
      "2021-09-07 17:18:02.695 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09623665362596512\n",
      "2021-09-07 17:18:02.696 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.35026031732559204\n",
      "2021-09-07 17:18:02.697 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09623665362596512\n",
      "2021-09-07 17:18:02.698 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.35026031732559204\n",
      "2021-09-07 17:18:02.700 | INFO     | src.policies:train:123 - Epoch 56 / 800\n",
      "2021-09-07 17:18:02.701 | INFO     | src.policies:collect_trajectories:221 - Episode 463\n",
      "2021-09-07 17:18:02.710 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.711 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 47.0\n",
      "2021-09-07 17:18:02.712 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 47.0\n",
      "2021-09-07 17:18:02.712 | INFO     | src.policies:collect_trajectories:221 - Episode 464\n",
      "2021-09-07 17:18:02.726 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.727 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 73.0\n",
      "2021-09-07 17:18:02.727 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 60.0\n",
      "2021-09-07 17:18:02.728 | INFO     | src.policies:collect_trajectories:221 - Episode 465\n",
      "2021-09-07 17:18:02.733 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.733 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:18:02.734 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 46.0\n",
      "2021-09-07 17:18:02.734 | INFO     | src.policies:collect_trajectories:221 - Episode 466\n",
      "2021-09-07 17:18:02.741 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.742 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 28.0\n",
      "2021-09-07 17:18:02.742 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 41.5\n",
      "2021-09-07 17:18:02.743 | INFO     | src.policies:collect_trajectories:221 - Episode 467\n",
      "2021-09-07 17:18:02.747 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.748 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:18:02.748 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 35.4\n",
      "2021-09-07 17:18:02.748 | INFO     | src.policies:collect_trajectories:221 - Episode 468\n",
      "2021-09-07 17:18:02.756 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.756 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 34.0\n",
      "2021-09-07 17:18:02.757 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 35.166666666666664\n",
      "2021-09-07 17:18:02.757 | WARNING  | src.policies:train:144 - The actual batch size is 211, instead of 200\n",
      "2021-09-07 17:18:02.761 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:02.763 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45900958776474, 'baseline_loss': 1.190976858139038, 'total_loss': 0.13647884130477905}\n",
      "2021-09-07 17:18:02.764 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12106078118085861\n",
      "2021-09-07 17:18:02.765 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4380645453929901\n",
      "2021-09-07 17:18:02.766 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12106078118085861\n",
      "2021-09-07 17:18:02.768 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4380645453929901\n",
      "2021-09-07 17:18:02.769 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:02.770 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.426960289478302, 'baseline_loss': 1.0073384046554565, 'total_loss': 0.07670891284942627}\n",
      "2021-09-07 17:18:02.771 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07291325181722641\n",
      "2021-09-07 17:18:02.772 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3949396312236786\n",
      "2021-09-07 17:18:02.773 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07291325181722641\n",
      "2021-09-07 17:18:02.774 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3949396312236786\n",
      "2021-09-07 17:18:02.775 | INFO     | src.policies:train:123 - Epoch 57 / 800\n",
      "2021-09-07 17:18:02.776 | INFO     | src.policies:collect_trajectories:221 - Episode 469\n",
      "2021-09-07 17:18:02.782 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.783 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 28.0\n",
      "2021-09-07 17:18:02.783 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 28.0\n",
      "2021-09-07 17:18:02.784 | INFO     | src.policies:collect_trajectories:221 - Episode 470\n",
      "2021-09-07 17:18:02.809 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.809 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 132.0\n",
      "2021-09-07 17:18:02.810 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 80.0\n",
      "2021-09-07 17:18:02.810 | INFO     | src.policies:collect_trajectories:221 - Episode 471\n",
      "2021-09-07 17:18:02.814 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.815 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 15.0\n",
      "2021-09-07 17:18:02.815 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 58.333333333333336\n",
      "2021-09-07 17:18:02.816 | INFO     | src.policies:collect_trajectories:221 - Episode 472\n",
      "2021-09-07 17:18:02.830 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.830 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 75.0\n",
      "2021-09-07 17:18:02.831 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 62.5\n",
      "2021-09-07 17:18:02.831 | WARNING  | src.policies:train:144 - The actual batch size is 250, instead of 200\n",
      "2021-09-07 17:18:02.834 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:02.836 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4131743311882019, 'baseline_loss': 0.8013562560081482, 'total_loss': -0.012496203184127808}\n",
      "2021-09-07 17:18:02.837 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.05965963006019592\n",
      "2021-09-07 17:18:02.837 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5244225859642029\n",
      "2021-09-07 17:18:02.839 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.05965963006019592\n",
      "2021-09-07 17:18:02.840 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999989867210388\n",
      "2021-09-07 17:18:02.842 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:02.843 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4100719094276428, 'baseline_loss': 0.9057943224906921, 'total_loss': 0.04282525181770325}\n",
      "2021-09-07 17:18:02.845 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11830920726060867\n",
      "2021-09-07 17:18:02.846 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.39152973890304565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:02.847 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11830920726060867\n",
      "2021-09-07 17:18:02.849 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.39152973890304565\n",
      "2021-09-07 17:18:02.850 | INFO     | src.policies:train:123 - Epoch 58 / 800\n",
      "2021-09-07 17:18:02.851 | INFO     | src.policies:collect_trajectories:221 - Episode 473\n",
      "2021-09-07 17:18:02.862 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.863 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 58.0\n",
      "2021-09-07 17:18:02.863 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 58.0\n",
      "2021-09-07 17:18:02.864 | INFO     | src.policies:collect_trajectories:221 - Episode 474\n",
      "2021-09-07 17:18:02.871 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.871 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 30.0\n",
      "2021-09-07 17:18:02.872 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 44.0\n",
      "2021-09-07 17:18:02.872 | INFO     | src.policies:collect_trajectories:221 - Episode 475\n",
      "2021-09-07 17:18:02.878 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.879 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 26.0\n",
      "2021-09-07 17:18:02.879 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 38.0\n",
      "2021-09-07 17:18:02.880 | INFO     | src.policies:collect_trajectories:221 - Episode 476\n",
      "2021-09-07 17:18:02.888 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.889 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 37.0\n",
      "2021-09-07 17:18:02.890 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 37.75\n",
      "2021-09-07 17:18:02.890 | INFO     | src.policies:collect_trajectories:221 - Episode 477\n",
      "2021-09-07 17:18:02.894 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.895 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:18:02.895 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 33.6\n",
      "2021-09-07 17:18:02.895 | INFO     | src.policies:collect_trajectories:221 - Episode 478\n",
      "2021-09-07 17:18:02.903 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.904 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 31.0\n",
      "2021-09-07 17:18:02.905 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 33.166666666666664\n",
      "2021-09-07 17:18:02.905 | INFO     | src.policies:collect_trajectories:221 - Episode 479\n",
      "2021-09-07 17:18:02.910 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.910 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 15.0\n",
      "2021-09-07 17:18:02.911 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 30.571428571428573\n",
      "2021-09-07 17:18:02.911 | WARNING  | src.policies:train:144 - The actual batch size is 214, instead of 200\n",
      "2021-09-07 17:18:02.914 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:02.915 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5963724851608276, 'baseline_loss': 1.3718184232711792, 'total_loss': 0.08953672647476196}\n",
      "2021-09-07 17:18:02.916 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16950812935829163\n",
      "2021-09-07 17:18:02.917 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5253270268440247\n",
      "2021-09-07 17:18:02.918 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16950812935829163\n",
      "2021-09-07 17:18:02.920 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999895691871643\n",
      "2021-09-07 17:18:02.922 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:02.923 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5899327993392944, 'baseline_loss': 1.4408107995986938, 'total_loss': 0.1304726004600525}\n",
      "2021-09-07 17:18:02.925 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1585305631160736\n",
      "2021-09-07 17:18:02.926 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5018425583839417\n",
      "2021-09-07 17:18:02.927 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1585305631160736\n",
      "2021-09-07 17:18:02.928 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999892711639404\n",
      "2021-09-07 17:18:02.930 | INFO     | src.policies:train:123 - Epoch 59 / 800\n",
      "2021-09-07 17:18:02.930 | INFO     | src.policies:collect_trajectories:221 - Episode 480\n",
      "2021-09-07 17:18:02.934 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.935 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:18:02.935 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 16.0\n",
      "2021-09-07 17:18:02.936 | INFO     | src.policies:collect_trajectories:221 - Episode 481\n",
      "2021-09-07 17:18:02.943 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.944 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 32.0\n",
      "2021-09-07 17:18:02.944 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.0\n",
      "2021-09-07 17:18:02.945 | INFO     | src.policies:collect_trajectories:221 - Episode 482\n",
      "2021-09-07 17:18:02.955 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.956 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 51.0\n",
      "2021-09-07 17:18:02.956 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 33.0\n",
      "2021-09-07 17:18:02.957 | INFO     | src.policies:collect_trajectories:221 - Episode 483\n",
      "2021-09-07 17:18:02.963 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.963 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:18:02.964 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 30.5\n",
      "2021-09-07 17:18:02.964 | INFO     | src.policies:collect_trajectories:221 - Episode 484\n",
      "2021-09-07 17:18:02.968 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.969 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:18:02.969 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 27.0\n",
      "2021-09-07 17:18:02.970 | INFO     | src.policies:collect_trajectories:221 - Episode 485\n",
      "2021-09-07 17:18:02.976 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.977 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 27.0\n",
      "2021-09-07 17:18:02.977 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 27.0\n",
      "2021-09-07 17:18:02.978 | INFO     | src.policies:collect_trajectories:221 - Episode 486\n",
      "2021-09-07 17:18:02.983 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:02.984 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:18:03.036 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.0\n",
      "2021-09-07 17:18:03.070 | INFO     | src.policies:collect_trajectories:221 - Episode 487\n",
      "2021-09-07 17:18:03.074 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.074 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:03.075 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.0\n",
      "2021-09-07 17:18:03.075 | INFO     | src.policies:collect_trajectories:221 - Episode 488\n",
      "2021-09-07 17:18:03.079 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.080 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:18:03.080 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.0\n",
      "2021-09-07 17:18:03.081 | INFO     | src.policies:collect_trajectories:221 - Episode 489\n",
      "2021-09-07 17:18:03.087 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.088 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:18:03.088 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.3\n",
      "2021-09-07 17:18:03.089 | WARNING  | src.policies:train:144 - The actual batch size is 223, instead of 200\n",
      "2021-09-07 17:18:03.092 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:03.093 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5126917362213135, 'baseline_loss': 1.1747946739196777, 'total_loss': 0.07470560073852539}\n",
      "2021-09-07 17:18:03.094 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17765945196151733\n",
      "2021-09-07 17:18:03.095 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.23882345855236053\n",
      "2021-09-07 17:18:03.096 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17765945196151733\n",
      "2021-09-07 17:18:03.097 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.23882345855236053\n",
      "2021-09-07 17:18:03.098 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:03.100 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4955366551876068, 'baseline_loss': 1.1742444038391113, 'total_loss': 0.09158554673194885}\n",
      "2021-09-07 17:18:03.102 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09740832448005676\n",
      "2021-09-07 17:18:03.103 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.1503005176782608\n",
      "2021-09-07 17:18:03.104 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09740832448005676\n",
      "2021-09-07 17:18:03.106 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.1503005176782608\n",
      "2021-09-07 17:18:03.107 | INFO     | src.policies:train:123 - Epoch 60 / 800\n",
      "2021-09-07 17:18:03.107 | INFO     | src.policies:collect_trajectories:221 - Episode 490\n",
      "2021-09-07 17:18:03.125 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.126 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 100.0\n",
      "2021-09-07 17:18:03.126 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 100.0\n",
      "2021-09-07 17:18:03.127 | INFO     | src.policies:collect_trajectories:221 - Episode 491\n",
      "2021-09-07 17:18:03.134 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.135 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:18:03.135 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 61.5\n",
      "2021-09-07 17:18:03.136 | INFO     | src.policies:collect_trajectories:221 - Episode 492\n",
      "2021-09-07 17:18:03.141 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.142 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:18:03.142 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 45.0\n",
      "2021-09-07 17:18:03.143 | INFO     | src.policies:collect_trajectories:221 - Episode 493\n",
      "2021-09-07 17:18:03.150 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.150 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:18:03.151 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 40.0\n",
      "2021-09-07 17:18:03.313 | INFO     | src.policies:collect_trajectories:221 - Episode 494\n",
      "2021-09-07 17:18:03.320 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.321 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 28.0\n",
      "2021-09-07 17:18:03.322 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 37.6\n",
      "2021-09-07 17:18:03.322 | INFO     | src.policies:collect_trajectories:221 - Episode 495\n",
      "2021-09-07 17:18:03.330 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.331 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 40.0\n",
      "2021-09-07 17:18:03.331 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 38.0\n",
      "2021-09-07 17:18:03.332 | WARNING  | src.policies:train:144 - The actual batch size is 228, instead of 200\n",
      "2021-09-07 17:18:03.334 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:03.336 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5282222628593445, 'baseline_loss': 1.5302479267120361, 'total_loss': 0.23690170049667358}\n",
      "2021-09-07 17:18:03.337 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.0836176946759224\n",
      "2021-09-07 17:18:03.337 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.43599995970726013\n",
      "2021-09-07 17:18:03.339 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.0836176946759224\n",
      "2021-09-07 17:18:03.341 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.43599995970726013\n",
      "2021-09-07 17:18:03.343 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:03.344 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5156598091125488, 'baseline_loss': 1.4226129055023193, 'total_loss': 0.19564664363861084}\n",
      "2021-09-07 17:18:03.345 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12260007858276367\n",
      "2021-09-07 17:18:03.346 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.39854559302330017\n",
      "2021-09-07 17:18:03.348 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12260007858276367\n",
      "2021-09-07 17:18:03.349 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.39854559302330017\n",
      "2021-09-07 17:18:03.350 | INFO     | src.policies:train:123 - Epoch 61 / 800\n",
      "2021-09-07 17:18:03.351 | INFO     | src.policies:collect_trajectories:221 - Episode 496\n",
      "2021-09-07 17:18:03.354 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.355 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:18:03.355 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 13.0\n",
      "2021-09-07 17:18:03.355 | INFO     | src.policies:collect_trajectories:221 - Episode 497\n",
      "2021-09-07 17:18:03.362 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.363 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 28.0\n",
      "2021-09-07 17:18:03.363 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.5\n",
      "2021-09-07 17:18:03.364 | INFO     | src.policies:collect_trajectories:221 - Episode 498\n",
      "2021-09-07 17:18:03.369 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.370 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:18:03.370 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:03.371 | INFO     | src.policies:collect_trajectories:221 - Episode 499\n",
      "2021-09-07 17:18:03.381 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.382 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 51.0\n",
      "2021-09-07 17:18:03.383 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 29.25\n",
      "2021-09-07 17:18:03.383 | INFO     | src.policies:collect_trajectories:221 - Episode 500\n",
      "2021-09-07 17:18:03.387 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.388 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:18:03.388 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 26.2\n",
      "2021-09-07 17:18:03.389 | INFO     | src.policies:collect_trajectories:221 - Episode 501\n",
      "2021-09-07 17:18:03.394 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.395 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 27.0\n",
      "2021-09-07 17:18:03.395 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 26.333333333333332\n",
      "2021-09-07 17:18:03.396 | INFO     | src.policies:collect_trajectories:221 - Episode 502\n",
      "2021-09-07 17:18:03.401 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.402 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:18:03.403 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.0\n",
      "2021-09-07 17:18:03.403 | INFO     | src.policies:collect_trajectories:221 - Episode 503\n",
      "2021-09-07 17:18:03.407 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.408 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:18:03.408 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.375\n",
      "2021-09-07 17:18:03.408 | INFO     | src.policies:collect_trajectories:221 - Episode 504\n",
      "2021-09-07 17:18:03.412 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.413 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:18:03.413 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.22222222222222\n",
      "2021-09-07 17:18:03.416 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:03.417 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4988446831703186, 'baseline_loss': 1.244372844696045, 'total_loss': 0.12334173917770386}\n",
      "2021-09-07 17:18:03.419 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08000524342060089\n",
      "2021-09-07 17:18:03.420 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3372805416584015\n",
      "2021-09-07 17:18:03.422 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08000524342060089\n",
      "2021-09-07 17:18:03.423 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3372805416584015\n",
      "2021-09-07 17:18:03.424 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:03.425 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.465104877948761, 'baseline_loss': 1.1865524053573608, 'total_loss': 0.12817132472991943}\n",
      "2021-09-07 17:18:03.426 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.06649157404899597\n",
      "2021-09-07 17:18:03.427 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4357907176017761\n",
      "2021-09-07 17:18:03.428 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.06649157404899597\n",
      "2021-09-07 17:18:03.429 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4357907176017761\n",
      "2021-09-07 17:18:03.430 | INFO     | src.policies:train:123 - Epoch 62 / 800\n",
      "2021-09-07 17:18:03.431 | INFO     | src.policies:collect_trajectories:221 - Episode 505\n",
      "2021-09-07 17:18:03.436 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.437 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 28.0\n",
      "2021-09-07 17:18:03.437 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 28.0\n",
      "2021-09-07 17:18:03.438 | INFO     | src.policies:collect_trajectories:221 - Episode 506\n",
      "2021-09-07 17:18:03.450 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.451 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 63.0\n",
      "2021-09-07 17:18:03.451 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 45.5\n",
      "2021-09-07 17:18:03.452 | INFO     | src.policies:collect_trajectories:221 - Episode 507\n",
      "2021-09-07 17:18:03.457 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.457 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 9.0\n",
      "2021-09-07 17:18:03.458 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 33.333333333333336\n",
      "2021-09-07 17:18:03.458 | INFO     | src.policies:collect_trajectories:221 - Episode 508\n",
      "2021-09-07 17:18:03.473 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.473 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 80.0\n",
      "2021-09-07 17:18:03.474 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 45.0\n",
      "2021-09-07 17:18:03.475 | INFO     | src.policies:collect_trajectories:221 - Episode 509\n",
      "2021-09-07 17:18:03.484 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.485 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 41.0\n",
      "2021-09-07 17:18:03.486 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 44.2\n",
      "2021-09-07 17:18:03.487 | WARNING  | src.policies:train:144 - The actual batch size is 221, instead of 200\n",
      "2021-09-07 17:18:03.489 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:03.490 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5197206735610962, 'baseline_loss': 1.3054919242858887, 'total_loss': 0.13302528858184814}\n",
      "2021-09-07 17:18:03.491 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11018918454647064\n",
      "2021-09-07 17:18:03.492 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2791316509246826\n",
      "2021-09-07 17:18:03.493 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11018918454647064\n",
      "2021-09-07 17:18:03.494 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2791316509246826\n",
      "2021-09-07 17:18:03.495 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:03.496 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5427782535552979, 'baseline_loss': 1.4023184776306152, 'total_loss': 0.15838098526000977}\n",
      "2021-09-07 17:18:03.498 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1134837195277214\n",
      "2021-09-07 17:18:03.499 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6105712652206421\n",
      "2021-09-07 17:18:03.501 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1134837195277214\n",
      "2021-09-07 17:18:03.502 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:03.504 | INFO     | src.policies:train:123 - Epoch 63 / 800\n",
      "2021-09-07 17:18:03.505 | INFO     | src.policies:collect_trajectories:221 - Episode 510\n",
      "2021-09-07 17:18:03.511 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:03.512 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 32.0\n",
      "2021-09-07 17:18:03.512 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 32.0\n",
      "2021-09-07 17:18:03.513 | INFO     | src.policies:collect_trajectories:221 - Episode 511\n",
      "2021-09-07 17:18:03.528 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.529 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 81.0\n",
      "2021-09-07 17:18:03.529 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 56.5\n",
      "2021-09-07 17:18:03.530 | INFO     | src.policies:collect_trajectories:221 - Episode 512\n",
      "2021-09-07 17:18:03.549 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.550 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 104.0\n",
      "2021-09-07 17:18:03.550 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 72.33333333333333\n",
      "2021-09-07 17:18:03.551 | WARNING  | src.policies:train:144 - The actual batch size is 217, instead of 200\n",
      "2021-09-07 17:18:03.553 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:03.555 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43337953090667725, 'baseline_loss': 1.3141753673553467, 'total_loss': 0.2237081527709961}\n",
      "2021-09-07 17:18:03.556 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11015626788139343\n",
      "2021-09-07 17:18:03.557 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.020941138267517\n",
      "2021-09-07 17:18:03.558 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11015626788139343\n",
      "2021-09-07 17:18:03.560 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:03.561 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:03.563 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.44599980115890503, 'baseline_loss': 1.0990983247756958, 'total_loss': 0.10354936122894287}\n",
      "2021-09-07 17:18:03.564 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17691554129123688\n",
      "2021-09-07 17:18:03.565 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6161080598831177\n",
      "2021-09-07 17:18:03.566 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17691554129123688\n",
      "2021-09-07 17:18:03.567 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:03.568 | INFO     | src.policies:train:123 - Epoch 64 / 800\n",
      "2021-09-07 17:18:03.569 | INFO     | src.policies:collect_trajectories:221 - Episode 513\n",
      "2021-09-07 17:18:03.634 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.635 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 57.0\n",
      "2021-09-07 17:18:03.635 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 57.0\n",
      "2021-09-07 17:18:03.636 | INFO     | src.policies:collect_trajectories:221 - Episode 514\n",
      "2021-09-07 17:18:03.645 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.645 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 42.0\n",
      "2021-09-07 17:18:03.646 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 49.5\n",
      "2021-09-07 17:18:03.646 | INFO     | src.policies:collect_trajectories:221 - Episode 515\n",
      "2021-09-07 17:18:03.653 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.654 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 32.0\n",
      "2021-09-07 17:18:03.654 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 43.666666666666664\n",
      "2021-09-07 17:18:03.655 | INFO     | src.policies:collect_trajectories:221 - Episode 516\n",
      "2021-09-07 17:18:03.669 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.669 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 82.0\n",
      "2021-09-07 17:18:03.670 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 53.25\n",
      "2021-09-07 17:18:03.670 | WARNING  | src.policies:train:144 - The actual batch size is 213, instead of 200\n",
      "2021-09-07 17:18:03.673 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:03.674 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40261155366897583, 'baseline_loss': 1.082102656364441, 'total_loss': 0.13843977451324463}\n",
      "2021-09-07 17:18:03.675 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10400081425905228\n",
      "2021-09-07 17:18:03.676 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.31635746359825134\n",
      "2021-09-07 17:18:03.678 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10400081425905228\n",
      "2021-09-07 17:18:03.679 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.31635746359825134\n",
      "2021-09-07 17:18:03.680 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:03.682 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42969265580177307, 'baseline_loss': 1.256309151649475, 'total_loss': 0.19846192002296448}\n",
      "2021-09-07 17:18:03.683 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10963049530982971\n",
      "2021-09-07 17:18:03.684 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.513841986656189\n",
      "2021-09-07 17:18:03.685 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10963049530982971\n",
      "2021-09-07 17:18:03.686 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:03.687 | INFO     | src.policies:train:123 - Epoch 65 / 800\n",
      "2021-09-07 17:18:03.688 | INFO     | src.policies:collect_trajectories:221 - Episode 517\n",
      "2021-09-07 17:18:03.692 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.692 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:18:03.693 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 12.0\n",
      "2021-09-07 17:18:03.693 | INFO     | src.policies:collect_trajectories:221 - Episode 518\n",
      "2021-09-07 17:18:03.699 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.700 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 28.0\n",
      "2021-09-07 17:18:03.700 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.0\n",
      "2021-09-07 17:18:03.700 | INFO     | src.policies:collect_trajectories:221 - Episode 519\n",
      "2021-09-07 17:18:03.705 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.705 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:18:03.705 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.333333333333332\n",
      "2021-09-07 17:18:03.706 | INFO     | src.policies:collect_trajectories:221 - Episode 520\n",
      "2021-09-07 17:18:03.710 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.711 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:18:03.711 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.75\n",
      "2021-09-07 17:18:03.711 | INFO     | src.policies:collect_trajectories:221 - Episode 521\n",
      "2021-09-07 17:18:03.716 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.717 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:03.717 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.4\n",
      "2021-09-07 17:18:03.717 | INFO     | src.policies:collect_trajectories:221 - Episode 522\n",
      "2021-09-07 17:18:03.721 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.722 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:18:03.722 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.0\n",
      "2021-09-07 17:18:03.722 | INFO     | src.policies:collect_trajectories:221 - Episode 523\n",
      "2021-09-07 17:18:03.728 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.729 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 32.0\n",
      "2021-09-07 17:18:03.729 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.0\n",
      "2021-09-07 17:18:03.730 | INFO     | src.policies:collect_trajectories:221 - Episode 524\n",
      "2021-09-07 17:18:03.734 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.734 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 14.0\n",
      "2021-09-07 17:18:03.735 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.25\n",
      "2021-09-07 17:18:03.735 | INFO     | src.policies:collect_trajectories:221 - Episode 525\n",
      "2021-09-07 17:18:03.740 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.740 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:18:03.741 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.88888888888889\n",
      "2021-09-07 17:18:03.741 | INFO     | src.policies:collect_trajectories:221 - Episode 526\n",
      "2021-09-07 17:18:03.747 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.748 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 30.0\n",
      "2021-09-07 17:18:03.748 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.9\n",
      "2021-09-07 17:18:03.749 | WARNING  | src.policies:train:144 - The actual batch size is 209, instead of 200\n",
      "2021-09-07 17:18:03.752 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:03.753 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43687525391578674, 'baseline_loss': 1.0320587158203125, 'total_loss': 0.0791541039943695}\n",
      "2021-09-07 17:18:03.754 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10747818648815155\n",
      "2021-09-07 17:18:03.755 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5077748894691467\n",
      "2021-09-07 17:18:03.756 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10747818648815155\n",
      "2021-09-07 17:18:03.757 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999895691871643\n",
      "2021-09-07 17:18:03.759 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:03.760 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41573405265808105, 'baseline_loss': 1.0932801961898804, 'total_loss': 0.13090604543685913}\n",
      "2021-09-07 17:18:03.761 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.053786735981702805\n",
      "2021-09-07 17:18:03.761 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.38099193572998047\n",
      "2021-09-07 17:18:03.762 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.053786735981702805\n",
      "2021-09-07 17:18:03.763 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.38099193572998047\n",
      "2021-09-07 17:18:03.765 | INFO     | src.policies:train:123 - Epoch 66 / 800\n",
      "2021-09-07 17:18:03.765 | INFO     | src.policies:collect_trajectories:221 - Episode 527\n",
      "2021-09-07 17:18:03.770 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.771 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 26.0\n",
      "2021-09-07 17:18:03.771 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 26.0\n",
      "2021-09-07 17:18:03.771 | INFO     | src.policies:collect_trajectories:221 - Episode 528\n",
      "2021-09-07 17:18:03.775 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.776 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:18:03.776 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.0\n",
      "2021-09-07 17:18:03.777 | INFO     | src.policies:collect_trajectories:221 - Episode 529\n",
      "2021-09-07 17:18:03.783 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.784 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:18:03.784 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.666666666666668\n",
      "2021-09-07 17:18:03.784 | INFO     | src.policies:collect_trajectories:221 - Episode 530\n",
      "2021-09-07 17:18:03.789 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.790 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:18:03.790 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.25\n",
      "2021-09-07 17:18:03.790 | INFO     | src.policies:collect_trajectories:221 - Episode 531\n",
      "2021-09-07 17:18:03.795 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.796 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:18:03.796 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.8\n",
      "2021-09-07 17:18:03.796 | INFO     | src.policies:collect_trajectories:221 - Episode 532\n",
      "2021-09-07 17:18:03.806 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.807 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 49.0\n",
      "2021-09-07 17:18:03.807 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.5\n",
      "2021-09-07 17:18:03.808 | INFO     | src.policies:collect_trajectories:221 - Episode 533\n",
      "2021-09-07 17:18:03.816 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.817 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 37.0\n",
      "2021-09-07 17:18:03.817 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 27.142857142857142\n",
      "2021-09-07 17:18:03.817 | INFO     | src.policies:collect_trajectories:221 - Episode 534\n",
      "2021-09-07 17:18:03.824 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.825 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 35.0\n",
      "2021-09-07 17:18:03.825 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 28.125\n",
      "2021-09-07 17:18:03.826 | WARNING  | src.policies:train:144 - The actual batch size is 225, instead of 200\n",
      "2021-09-07 17:18:03.829 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:03.830 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40922287106513977, 'baseline_loss': 0.9325723052024841, 'total_loss': 0.057063281536102295}\n",
      "2021-09-07 17:18:03.831 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1840156465768814\n",
      "2021-09-07 17:18:03.832 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.47811442613601685\n",
      "2021-09-07 17:18:03.833 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1840156465768814\n",
      "2021-09-07 17:18:03.834 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.47811442613601685\n",
      "2021-09-07 17:18:03.835 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:03.836 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32200539112091064, 'baseline_loss': 0.8831784725189209, 'total_loss': 0.1195838451385498}\n",
      "2021-09-07 17:18:03.837 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15828406810760498\n",
      "2021-09-07 17:18:03.838 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7376359701156616\n",
      "2021-09-07 17:18:03.839 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15828406810760498\n",
      "2021-09-07 17:18:03.840 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:03.842 | INFO     | src.policies:train:123 - Epoch 67 / 800\n",
      "2021-09-07 17:18:03.842 | INFO     | src.policies:collect_trajectories:221 - Episode 535\n",
      "2021-09-07 17:18:03.848 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.848 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 32.0\n",
      "2021-09-07 17:18:03.849 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 32.0\n",
      "2021-09-07 17:18:03.849 | INFO     | src.policies:collect_trajectories:221 - Episode 536\n",
      "2021-09-07 17:18:03.854 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.854 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:18:03.855 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 27.5\n",
      "2021-09-07 17:18:03.855 | INFO     | src.policies:collect_trajectories:221 - Episode 537\n",
      "2021-09-07 17:18:03.859 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.859 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:18:03.860 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.0\n",
      "2021-09-07 17:18:03.860 | INFO     | src.policies:collect_trajectories:221 - Episode 538\n",
      "2021-09-07 17:18:03.863 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.864 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:18:03.864 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.75\n",
      "2021-09-07 17:18:03.864 | INFO     | src.policies:collect_trajectories:221 - Episode 539\n",
      "2021-09-07 17:18:03.872 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.872 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 41.0\n",
      "2021-09-07 17:18:03.872 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.8\n",
      "2021-09-07 17:18:03.873 | INFO     | src.policies:collect_trajectories:221 - Episode 540\n",
      "2021-09-07 17:18:03.878 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.878 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:03.879 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 24.0\n",
      "2021-09-07 17:18:03.879 | INFO     | src.policies:collect_trajectories:221 - Episode 541\n",
      "2021-09-07 17:18:03.884 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.884 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:03.884 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.428571428571427\n",
      "2021-09-07 17:18:03.885 | INFO     | src.policies:collect_trajectories:221 - Episode 542\n",
      "2021-09-07 17:18:03.895 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.896 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 58.0\n",
      "2021-09-07 17:18:03.896 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 27.75\n",
      "2021-09-07 17:18:03.897 | WARNING  | src.policies:train:144 - The actual batch size is 222, instead of 200\n",
      "2021-09-07 17:18:03.902 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:03.903 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5051730871200562, 'baseline_loss': 1.1336050033569336, 'total_loss': 0.061629414558410645}\n",
      "2021-09-07 17:18:03.904 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18865007162094116\n",
      "2021-09-07 17:18:03.905 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.33281809091567993\n",
      "2021-09-07 17:18:03.906 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18865007162094116\n",
      "2021-09-07 17:18:03.907 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.33281809091567993\n",
      "2021-09-07 17:18:03.908 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:03.909 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45276781916618347, 'baseline_loss': 1.116324782371521, 'total_loss': 0.10539457201957703}\n",
      "2021-09-07 17:18:03.910 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08011682331562042\n",
      "2021-09-07 17:18:03.911 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.26567721366882324\n",
      "2021-09-07 17:18:03.912 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08011682331562042\n",
      "2021-09-07 17:18:03.913 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.26567721366882324\n",
      "2021-09-07 17:18:03.915 | INFO     | src.policies:train:123 - Epoch 68 / 800\n",
      "2021-09-07 17:18:03.915 | INFO     | src.policies:collect_trajectories:221 - Episode 543\n",
      "2021-09-07 17:18:03.928 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.929 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 82.0\n",
      "2021-09-07 17:18:03.929 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 82.0\n",
      "2021-09-07 17:18:03.930 | INFO     | src.policies:collect_trajectories:221 - Episode 544\n",
      "2021-09-07 17:18:03.935 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.935 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:18:03.936 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 49.0\n",
      "2021-09-07 17:18:03.936 | INFO     | src.policies:collect_trajectories:221 - Episode 545\n",
      "2021-09-07 17:18:03.943 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.943 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 31.0\n",
      "2021-09-07 17:18:03.944 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 43.0\n",
      "2021-09-07 17:18:03.944 | INFO     | src.policies:collect_trajectories:221 - Episode 546\n",
      "2021-09-07 17:18:03.949 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.950 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:18:03.950 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 36.75\n",
      "2021-09-07 17:18:03.950 | INFO     | src.policies:collect_trajectories:221 - Episode 547\n",
      "2021-09-07 17:18:03.966 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.967 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 99.0\n",
      "2021-09-07 17:18:03.968 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 49.2\n",
      "2021-09-07 17:18:03.968 | WARNING  | src.policies:train:144 - The actual batch size is 246, instead of 200\n",
      "2021-09-07 17:18:03.972 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:03.974 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5481802821159363, 'baseline_loss': 1.650636911392212, 'total_loss': 0.2771381735801697}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:03.975 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3110700249671936\n",
      "2021-09-07 17:18:03.976 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6891926527023315\n",
      "2021-09-07 17:18:03.978 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3110700249671936\n",
      "2021-09-07 17:18:03.979 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:03.981 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:03.982 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5169339179992676, 'baseline_loss': 1.3587348461151123, 'total_loss': 0.16243350505828857}\n",
      "2021-09-07 17:18:03.983 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.27006807923316956\n",
      "2021-09-07 17:18:03.984 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.537684440612793\n",
      "2021-09-07 17:18:03.986 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.27006807923316956\n",
      "2021-09-07 17:18:03.987 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:03.989 | INFO     | src.policies:train:123 - Epoch 69 / 800\n",
      "2021-09-07 17:18:03.990 | INFO     | src.policies:collect_trajectories:221 - Episode 548\n",
      "2021-09-07 17:18:03.995 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:03.996 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:18:03.996 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.0\n",
      "2021-09-07 17:18:03.996 | INFO     | src.policies:collect_trajectories:221 - Episode 549\n",
      "2021-09-07 17:18:04.011 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.011 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 77.0\n",
      "2021-09-07 17:18:04.012 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 51.0\n",
      "2021-09-07 17:18:04.012 | INFO     | src.policies:collect_trajectories:221 - Episode 550\n",
      "2021-09-07 17:18:04.020 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.021 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:18:04.021 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 42.0\n",
      "2021-09-07 17:18:04.022 | INFO     | src.policies:collect_trajectories:221 - Episode 551\n",
      "2021-09-07 17:18:04.026 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.026 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:18:04.027 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 34.75\n",
      "2021-09-07 17:18:04.027 | INFO     | src.policies:collect_trajectories:221 - Episode 552\n",
      "2021-09-07 17:18:04.034 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.035 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 27.0\n",
      "2021-09-07 17:18:04.035 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 33.2\n",
      "2021-09-07 17:18:04.036 | INFO     | src.policies:collect_trajectories:221 - Episode 553\n",
      "2021-09-07 17:18:04.048 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.049 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 64.0\n",
      "2021-09-07 17:18:04.049 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 38.333333333333336\n",
      "2021-09-07 17:18:04.050 | WARNING  | src.policies:train:144 - The actual batch size is 230, instead of 200\n",
      "2021-09-07 17:18:04.053 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:04.054 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5657116174697876, 'baseline_loss': 1.4047160148620605, 'total_loss': 0.13664638996124268}\n",
      "2021-09-07 17:18:04.055 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.124344103038311\n",
      "2021-09-07 17:18:04.056 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3345210552215576\n",
      "2021-09-07 17:18:04.058 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.124344103038311\n",
      "2021-09-07 17:18:04.059 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3345210552215576\n",
      "2021-09-07 17:18:04.061 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:04.062 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.581260621547699, 'baseline_loss': 1.5527868270874023, 'total_loss': 0.1951327919960022}\n",
      "2021-09-07 17:18:04.064 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21401828527450562\n",
      "2021-09-07 17:18:04.065 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4130763113498688\n",
      "2021-09-07 17:18:04.066 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21401828527450562\n",
      "2021-09-07 17:18:04.067 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4130763113498688\n",
      "2021-09-07 17:18:04.068 | INFO     | src.policies:train:123 - Epoch 70 / 800\n",
      "2021-09-07 17:18:04.069 | INFO     | src.policies:collect_trajectories:221 - Episode 554\n",
      "2021-09-07 17:18:04.072 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.073 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:18:04.073 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 17.0\n",
      "2021-09-07 17:18:04.074 | INFO     | src.policies:collect_trajectories:221 - Episode 555\n",
      "2021-09-07 17:18:04.078 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.078 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:04.079 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.5\n",
      "2021-09-07 17:18:04.079 | INFO     | src.policies:collect_trajectories:221 - Episode 556\n",
      "2021-09-07 17:18:04.086 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.087 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 26.0\n",
      "2021-09-07 17:18:04.087 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.0\n",
      "2021-09-07 17:18:04.088 | INFO     | src.policies:collect_trajectories:221 - Episode 557\n",
      "2021-09-07 17:18:04.091 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.092 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 10.0\n",
      "2021-09-07 17:18:04.093 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.25\n",
      "2021-09-07 17:18:04.093 | INFO     | src.policies:collect_trajectories:221 - Episode 558\n",
      "2021-09-07 17:18:04.100 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.101 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 39.0\n",
      "2021-09-07 17:18:04.102 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.4\n",
      "2021-09-07 17:18:04.102 | INFO     | src.policies:collect_trajectories:221 - Episode 559\n",
      "2021-09-07 17:18:04.107 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.107 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:18:04.107 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.333333333333332\n",
      "2021-09-07 17:18:04.108 | INFO     | src.policies:collect_trajectories:221 - Episode 560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:04.114 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.114 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 29.0\n",
      "2021-09-07 17:18:04.114 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.428571428571427\n",
      "2021-09-07 17:18:04.115 | INFO     | src.policies:collect_trajectories:221 - Episode 561\n",
      "2021-09-07 17:18:04.121 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.122 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 26.0\n",
      "2021-09-07 17:18:04.122 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 22.875\n",
      "2021-09-07 17:18:04.123 | INFO     | src.policies:collect_trajectories:221 - Episode 562\n",
      "2021-09-07 17:18:04.303 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.304 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 47.0\n",
      "2021-09-07 17:18:04.305 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.555555555555557\n",
      "2021-09-07 17:18:04.305 | WARNING  | src.policies:train:144 - The actual batch size is 230, instead of 200\n",
      "2021-09-07 17:18:04.309 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:04.310 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6230560541152954, 'baseline_loss': 1.3671441078186035, 'total_loss': 0.06051599979400635}\n",
      "2021-09-07 17:18:04.311 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31309834122657776\n",
      "2021-09-07 17:18:04.312 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.22186191380023956\n",
      "2021-09-07 17:18:04.314 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31309834122657776\n",
      "2021-09-07 17:18:04.315 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.22186191380023956\n",
      "2021-09-07 17:18:04.316 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:04.317 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5642072558403015, 'baseline_loss': 1.3234906196594238, 'total_loss': 0.0975380539894104}\n",
      "2021-09-07 17:18:04.318 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15686637163162231\n",
      "2021-09-07 17:18:04.319 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3525688350200653\n",
      "2021-09-07 17:18:04.320 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15686637163162231\n",
      "2021-09-07 17:18:04.321 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3525688350200653\n",
      "2021-09-07 17:18:04.323 | INFO     | src.policies:train:123 - Epoch 71 / 800\n",
      "2021-09-07 17:18:04.324 | INFO     | src.policies:collect_trajectories:221 - Episode 563\n",
      "2021-09-07 17:18:04.333 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.334 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 57.0\n",
      "2021-09-07 17:18:04.334 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 57.0\n",
      "2021-09-07 17:18:04.334 | INFO     | src.policies:collect_trajectories:221 - Episode 564\n",
      "2021-09-07 17:18:04.339 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.339 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:18:04.340 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 34.0\n",
      "2021-09-07 17:18:04.340 | INFO     | src.policies:collect_trajectories:221 - Episode 565\n",
      "2021-09-07 17:18:04.350 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.350 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 57.0\n",
      "2021-09-07 17:18:04.351 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 41.666666666666664\n",
      "2021-09-07 17:18:04.351 | INFO     | src.policies:collect_trajectories:221 - Episode 566\n",
      "2021-09-07 17:18:04.362 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.363 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 61.0\n",
      "2021-09-07 17:18:04.363 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 46.5\n",
      "2021-09-07 17:18:04.363 | INFO     | src.policies:collect_trajectories:221 - Episode 567\n",
      "2021-09-07 17:18:04.369 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.370 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 29.0\n",
      "2021-09-07 17:18:04.370 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 43.0\n",
      "2021-09-07 17:18:04.371 | WARNING  | src.policies:train:144 - The actual batch size is 215, instead of 200\n",
      "2021-09-07 17:18:04.373 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:04.375 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5318590402603149, 'baseline_loss': 1.2401713132858276, 'total_loss': 0.08822661638259888}\n",
      "2021-09-07 17:18:04.376 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3326275646686554\n",
      "2021-09-07 17:18:04.376 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.49072450399398804\n",
      "2021-09-07 17:18:04.377 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3326275646686554\n",
      "2021-09-07 17:18:04.379 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49072450399398804\n",
      "2021-09-07 17:18:04.380 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:04.381 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4619988203048706, 'baseline_loss': 0.9090424180030823, 'total_loss': -0.007477611303329468}\n",
      "2021-09-07 17:18:04.382 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1452680230140686\n",
      "2021-09-07 17:18:04.383 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.41373148560523987\n",
      "2021-09-07 17:18:04.384 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1452680230140686\n",
      "2021-09-07 17:18:04.385 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.41373148560523987\n",
      "2021-09-07 17:18:04.386 | INFO     | src.policies:train:123 - Epoch 72 / 800\n",
      "2021-09-07 17:18:04.386 | INFO     | src.policies:collect_trajectories:221 - Episode 568\n",
      "2021-09-07 17:18:04.393 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.394 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 39.0\n",
      "2021-09-07 17:18:04.394 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 39.0\n",
      "2021-09-07 17:18:04.395 | INFO     | src.policies:collect_trajectories:221 - Episode 569\n",
      "2021-09-07 17:18:04.400 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.400 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:18:04.401 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.0\n",
      "2021-09-07 17:18:04.401 | INFO     | src.policies:collect_trajectories:221 - Episode 570\n",
      "2021-09-07 17:18:04.411 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.412 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 50.0\n",
      "2021-09-07 17:18:04.412 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 33.333333333333336\n",
      "2021-09-07 17:18:04.413 | INFO     | src.policies:collect_trajectories:221 - Episode 571\n",
      "2021-09-07 17:18:04.431 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:04.432 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 101.0\n",
      "2021-09-07 17:18:04.432 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 50.25\n",
      "2021-09-07 17:18:04.432 | WARNING  | src.policies:train:144 - The actual batch size is 201, instead of 200\n",
      "2021-09-07 17:18:04.435 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:04.437 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.48016202449798584, 'baseline_loss': 1.272067904472351, 'total_loss': 0.1558719277381897}\n",
      "2021-09-07 17:18:04.438 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32943859696388245\n",
      "2021-09-07 17:18:04.439 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5054675340652466\n",
      "2021-09-07 17:18:04.441 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32943859696388245\n",
      "2021-09-07 17:18:04.442 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999989867210388\n",
      "2021-09-07 17:18:04.443 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:04.445 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4482027292251587, 'baseline_loss': 1.1559563875198364, 'total_loss': 0.12977546453475952}\n",
      "2021-09-07 17:18:04.446 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.276787132024765\n",
      "2021-09-07 17:18:04.447 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.48137956857681274\n",
      "2021-09-07 17:18:04.448 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.276787132024765\n",
      "2021-09-07 17:18:04.449 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.48137956857681274\n",
      "2021-09-07 17:18:04.450 | INFO     | src.policies:train:123 - Epoch 73 / 800\n",
      "2021-09-07 17:18:04.451 | INFO     | src.policies:collect_trajectories:221 - Episode 572\n",
      "2021-09-07 17:18:04.460 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.461 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 52.0\n",
      "2021-09-07 17:18:04.461 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 52.0\n",
      "2021-09-07 17:18:04.462 | INFO     | src.policies:collect_trajectories:221 - Episode 573\n",
      "2021-09-07 17:18:04.470 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.471 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 43.0\n",
      "2021-09-07 17:18:04.471 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 47.5\n",
      "2021-09-07 17:18:04.472 | INFO     | src.policies:collect_trajectories:221 - Episode 574\n",
      "2021-09-07 17:18:04.480 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.481 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 29.0\n",
      "2021-09-07 17:18:04.482 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 41.333333333333336\n",
      "2021-09-07 17:18:04.483 | INFO     | src.policies:collect_trajectories:221 - Episode 575\n",
      "2021-09-07 17:18:04.497 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.498 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 49.0\n",
      "2021-09-07 17:18:04.498 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 43.25\n",
      "2021-09-07 17:18:04.499 | INFO     | src.policies:collect_trajectories:221 - Episode 576\n",
      "2021-09-07 17:18:04.514 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.514 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 69.0\n",
      "2021-09-07 17:18:04.515 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 48.4\n",
      "2021-09-07 17:18:04.515 | WARNING  | src.policies:train:144 - The actual batch size is 242, instead of 200\n",
      "2021-09-07 17:18:04.519 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:04.521 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6158353686332703, 'baseline_loss': 1.6585910320281982, 'total_loss': 0.21346014738082886}\n",
      "2021-09-07 17:18:04.522 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5296656489372253\n",
      "2021-09-07 17:18:04.524 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0149325132369995\n",
      "2021-09-07 17:18:04.525 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:04.527 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:04.528 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:04.530 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6159777641296387, 'baseline_loss': 1.645462155342102, 'total_loss': 0.20675331354141235}\n",
      "2021-09-07 17:18:04.531 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3713166117668152\n",
      "2021-09-07 17:18:04.532 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0170409679412842\n",
      "2021-09-07 17:18:04.533 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3713166117668152\n",
      "2021-09-07 17:18:04.535 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:04.537 | INFO     | src.policies:train:123 - Epoch 74 / 800\n",
      "2021-09-07 17:18:04.538 | INFO     | src.policies:collect_trajectories:221 - Episode 577\n",
      "2021-09-07 17:18:04.550 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.551 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 59.0\n",
      "2021-09-07 17:18:04.552 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 59.0\n",
      "2021-09-07 17:18:04.552 | INFO     | src.policies:collect_trajectories:221 - Episode 578\n",
      "2021-09-07 17:18:04.562 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.563 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 44.0\n",
      "2021-09-07 17:18:04.563 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 51.5\n",
      "2021-09-07 17:18:04.564 | INFO     | src.policies:collect_trajectories:221 - Episode 579\n",
      "2021-09-07 17:18:04.569 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.569 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 18.0\n",
      "2021-09-07 17:18:04.570 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 40.333333333333336\n",
      "2021-09-07 17:18:04.570 | INFO     | src.policies:collect_trajectories:221 - Episode 580\n",
      "2021-09-07 17:18:04.579 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.580 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 41.0\n",
      "2021-09-07 17:18:04.580 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 40.5\n",
      "2021-09-07 17:18:04.581 | INFO     | src.policies:collect_trajectories:221 - Episode 581\n",
      "2021-09-07 17:18:04.585 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.586 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 12.0\n",
      "2021-09-07 17:18:04.586 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 34.8\n",
      "2021-09-07 17:18:04.586 | INFO     | src.policies:collect_trajectories:221 - Episode 582\n",
      "2021-09-07 17:18:04.591 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.591 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:04.591 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 31.166666666666668\n",
      "2021-09-07 17:18:04.592 | INFO     | src.policies:collect_trajectories:221 - Episode 583\n",
      "2021-09-07 17:18:04.597 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.597 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:18:04.598 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 29.714285714285715\n",
      "2021-09-07 17:18:04.598 | WARNING  | src.policies:train:144 - The actual batch size is 208, instead of 200\n",
      "2021-09-07 17:18:04.603 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:04.604 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5609764456748962, 'baseline_loss': 1.3868539333343506, 'total_loss': 0.13245052099227905}\n",
      "2021-09-07 17:18:04.605 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31398704648017883\n",
      "2021-09-07 17:18:04.606 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2953867018222809\n",
      "2021-09-07 17:18:04.608 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31398704648017883\n",
      "2021-09-07 17:18:04.609 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2953867018222809\n",
      "2021-09-07 17:18:04.610 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:04.611 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5411590337753296, 'baseline_loss': 1.1938093900680542, 'total_loss': 0.05574566125869751}\n",
      "2021-09-07 17:18:04.613 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25868090987205505\n",
      "2021-09-07 17:18:04.614 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.20314204692840576\n",
      "2021-09-07 17:18:04.615 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25868090987205505\n",
      "2021-09-07 17:18:04.616 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.20314204692840576\n",
      "2021-09-07 17:18:04.618 | INFO     | src.policies:train:123 - Epoch 75 / 800\n",
      "2021-09-07 17:18:04.618 | INFO     | src.policies:collect_trajectories:221 - Episode 584\n",
      "2021-09-07 17:18:04.629 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.629 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 54.0\n",
      "2021-09-07 17:18:04.630 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 54.0\n",
      "2021-09-07 17:18:04.630 | INFO     | src.policies:collect_trajectories:221 - Episode 585\n",
      "2021-09-07 17:18:04.641 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.642 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 49.0\n",
      "2021-09-07 17:18:04.642 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 51.5\n",
      "2021-09-07 17:18:04.643 | INFO     | src.policies:collect_trajectories:221 - Episode 586\n",
      "2021-09-07 17:18:04.648 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.648 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:04.649 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 41.0\n",
      "2021-09-07 17:18:04.649 | INFO     | src.policies:collect_trajectories:221 - Episode 587\n",
      "2021-09-07 17:18:04.664 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.664 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 83.0\n",
      "2021-09-07 17:18:04.665 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 51.5\n",
      "2021-09-07 17:18:04.665 | WARNING  | src.policies:train:144 - The actual batch size is 206, instead of 200\n",
      "2021-09-07 17:18:04.667 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:04.670 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4866321086883545, 'baseline_loss': 1.4361659288406372, 'total_loss': 0.2314508557319641}\n",
      "2021-09-07 17:18:04.670 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12116300314664841\n",
      "2021-09-07 17:18:04.671 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.47899314761161804\n",
      "2021-09-07 17:18:04.672 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12116300314664841\n",
      "2021-09-07 17:18:04.674 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.47899314761161804\n",
      "2021-09-07 17:18:04.675 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:04.676 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.47825759649276733, 'baseline_loss': 1.4339369535446167, 'total_loss': 0.23871088027954102}\n",
      "2021-09-07 17:18:04.677 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2824046015739441\n",
      "2021-09-07 17:18:04.678 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7730730175971985\n",
      "2021-09-07 17:18:04.680 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2824046015739441\n",
      "2021-09-07 17:18:04.681 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:04.683 | INFO     | src.policies:train:123 - Epoch 76 / 800\n",
      "2021-09-07 17:18:04.684 | INFO     | src.policies:collect_trajectories:221 - Episode 588\n",
      "2021-09-07 17:18:04.692 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.692 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 42.0\n",
      "2021-09-07 17:18:04.693 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 42.0\n",
      "2021-09-07 17:18:04.693 | INFO     | src.policies:collect_trajectories:221 - Episode 589\n",
      "2021-09-07 17:18:04.701 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.702 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 39.0\n",
      "2021-09-07 17:18:04.702 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 40.5\n",
      "2021-09-07 17:18:04.703 | INFO     | src.policies:collect_trajectories:221 - Episode 590\n",
      "2021-09-07 17:18:04.713 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.713 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 61.0\n",
      "2021-09-07 17:18:04.714 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 47.333333333333336\n",
      "2021-09-07 17:18:04.714 | INFO     | src.policies:collect_trajectories:221 - Episode 591\n",
      "2021-09-07 17:18:04.720 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.721 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 26.0\n",
      "2021-09-07 17:18:04.721 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 42.0\n",
      "2021-09-07 17:18:04.722 | INFO     | src.policies:collect_trajectories:221 - Episode 592\n",
      "2021-09-07 17:18:04.729 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.730 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 37.0\n",
      "2021-09-07 17:18:04.730 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 41.0\n",
      "2021-09-07 17:18:04.731 | WARNING  | src.policies:train:144 - The actual batch size is 205, instead of 200\n",
      "2021-09-07 17:18:04.733 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:04.735 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5149175524711609, 'baseline_loss': 1.279231071472168, 'total_loss': 0.1246979832649231}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:04.736 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12812195718288422\n",
      "2021-09-07 17:18:04.737 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5674140453338623\n",
      "2021-09-07 17:18:04.738 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12812195718288422\n",
      "2021-09-07 17:18:04.739 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:04.741 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:04.742 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5222604274749756, 'baseline_loss': 1.2480316162109375, 'total_loss': 0.10175538063049316}\n",
      "2021-09-07 17:18:04.743 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1463005095720291\n",
      "2021-09-07 17:18:04.744 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5024613738059998\n",
      "2021-09-07 17:18:04.745 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1463005095720291\n",
      "2021-09-07 17:18:04.747 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:04.748 | INFO     | src.policies:train:123 - Epoch 77 / 800\n",
      "2021-09-07 17:18:04.749 | INFO     | src.policies:collect_trajectories:221 - Episode 593\n",
      "2021-09-07 17:18:04.757 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.757 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 44.0\n",
      "2021-09-07 17:18:04.758 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 44.0\n",
      "2021-09-07 17:18:04.758 | INFO     | src.policies:collect_trajectories:221 - Episode 594\n",
      "2021-09-07 17:18:04.766 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.767 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 27.0\n",
      "2021-09-07 17:18:04.767 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 35.5\n",
      "2021-09-07 17:18:04.768 | INFO     | src.policies:collect_trajectories:221 - Episode 595\n",
      "2021-09-07 17:18:04.783 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.784 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 89.0\n",
      "2021-09-07 17:18:04.784 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 53.333333333333336\n",
      "2021-09-07 17:18:04.784 | INFO     | src.policies:collect_trajectories:221 - Episode 596\n",
      "2021-09-07 17:18:04.793 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.793 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 40.0\n",
      "2021-09-07 17:18:04.794 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 50.0\n",
      "2021-09-07 17:18:04.796 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:04.798 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.49433663487434387, 'baseline_loss': 1.273428201675415, 'total_loss': 0.14237746596336365}\n",
      "2021-09-07 17:18:04.800 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1851995885372162\n",
      "2021-09-07 17:18:04.869 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.37823978066444397\n",
      "2021-09-07 17:18:04.880 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1851995885372162\n",
      "2021-09-07 17:18:04.881 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.37823978066444397\n",
      "2021-09-07 17:18:04.883 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:04.884 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5007918477058411, 'baseline_loss': 1.2713435888290405, 'total_loss': 0.1348799467086792}\n",
      "2021-09-07 17:18:04.885 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12511403858661652\n",
      "2021-09-07 17:18:04.886 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3498978018760681\n",
      "2021-09-07 17:18:04.887 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12511403858661652\n",
      "2021-09-07 17:18:04.889 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3498978018760681\n",
      "2021-09-07 17:18:04.890 | INFO     | src.policies:train:123 - Epoch 78 / 800\n",
      "2021-09-07 17:18:04.891 | INFO     | src.policies:collect_trajectories:221 - Episode 597\n",
      "2021-09-07 17:18:04.895 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.895 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 15.0\n",
      "2021-09-07 17:18:04.896 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 15.0\n",
      "2021-09-07 17:18:04.896 | INFO     | src.policies:collect_trajectories:221 - Episode 598\n",
      "2021-09-07 17:18:04.921 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.921 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 145.0\n",
      "2021-09-07 17:18:04.922 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 80.0\n",
      "2021-09-07 17:18:04.922 | INFO     | src.policies:collect_trajectories:221 - Episode 599\n",
      "2021-09-07 17:18:04.935 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.936 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 74.0\n",
      "2021-09-07 17:18:04.937 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 78.0\n",
      "2021-09-07 17:18:04.938 | WARNING  | src.policies:train:144 - The actual batch size is 234, instead of 200\n",
      "2021-09-07 17:18:04.941 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:04.943 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3662295639514923, 'baseline_loss': 0.9450282454490662, 'total_loss': 0.10628455877304077}\n",
      "2021-09-07 17:18:04.944 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3113984763622284\n",
      "2021-09-07 17:18:04.945 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0812406539916992\n",
      "2021-09-07 17:18:04.946 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3113984763622284\n",
      "2021-09-07 17:18:04.947 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:04.948 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:04.950 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3482900559902191, 'baseline_loss': 0.8941456079483032, 'total_loss': 0.0987827479839325}\n",
      "2021-09-07 17:18:04.951 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2534104883670807\n",
      "2021-09-07 17:18:04.952 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0217646360397339\n",
      "2021-09-07 17:18:04.953 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2534104883670807\n",
      "2021-09-07 17:18:04.954 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:04.955 | INFO     | src.policies:train:123 - Epoch 79 / 800\n",
      "2021-09-07 17:18:04.956 | INFO     | src.policies:collect_trajectories:221 - Episode 600\n",
      "2021-09-07 17:18:04.965 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.966 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 48.0\n",
      "2021-09-07 17:18:04.966 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 48.0\n",
      "2021-09-07 17:18:04.967 | INFO     | src.policies:collect_trajectories:221 - Episode 601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:04.976 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.976 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 37.0\n",
      "2021-09-07 17:18:04.977 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 42.5\n",
      "2021-09-07 17:18:04.977 | INFO     | src.policies:collect_trajectories:221 - Episode 602\n",
      "2021-09-07 17:18:04.987 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.987 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 40.0\n",
      "2021-09-07 17:18:04.988 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 41.666666666666664\n",
      "2021-09-07 17:18:04.988 | INFO     | src.policies:collect_trajectories:221 - Episode 603\n",
      "2021-09-07 17:18:04.997 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:04.998 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 47.0\n",
      "2021-09-07 17:18:04.999 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 43.0\n",
      "2021-09-07 17:18:05.000 | INFO     | src.policies:collect_trajectories:221 - Episode 604\n",
      "2021-09-07 17:18:05.010 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.011 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 37.0\n",
      "2021-09-07 17:18:05.012 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 41.8\n",
      "2021-09-07 17:18:05.013 | WARNING  | src.policies:train:144 - The actual batch size is 209, instead of 200\n",
      "2021-09-07 17:18:05.016 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:05.018 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.496680349111557, 'baseline_loss': 1.0442028045654297, 'total_loss': 0.025421053171157837}\n",
      "2021-09-07 17:18:05.019 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3810761868953705\n",
      "2021-09-07 17:18:05.020 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.30795156955718994\n",
      "2021-09-07 17:18:05.022 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3810761868953705\n",
      "2021-09-07 17:18:05.023 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.30795156955718994\n",
      "2021-09-07 17:18:05.024 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:05.026 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4756534695625305, 'baseline_loss': 1.1133267879486084, 'total_loss': 0.08100992441177368}\n",
      "2021-09-07 17:18:05.027 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18050961196422577\n",
      "2021-09-07 17:18:05.028 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.38050103187561035\n",
      "2021-09-07 17:18:05.029 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18050961196422577\n",
      "2021-09-07 17:18:05.030 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.38050103187561035\n",
      "2021-09-07 17:18:05.032 | INFO     | src.policies:train:123 - Epoch 80 / 800\n",
      "2021-09-07 17:18:05.032 | INFO     | src.policies:collect_trajectories:221 - Episode 605\n",
      "2021-09-07 17:18:05.040 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.041 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 36.0\n",
      "2021-09-07 17:18:05.041 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 36.0\n",
      "2021-09-07 17:18:05.041 | INFO     | src.policies:collect_trajectories:221 - Episode 606\n",
      "2021-09-07 17:18:05.045 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.046 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:18:05.046 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.5\n",
      "2021-09-07 17:18:05.047 | INFO     | src.policies:collect_trajectories:221 - Episode 607\n",
      "2021-09-07 17:18:05.060 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.061 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 67.0\n",
      "2021-09-07 17:18:05.061 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 38.0\n",
      "2021-09-07 17:18:05.062 | INFO     | src.policies:collect_trajectories:221 - Episode 608\n",
      "2021-09-07 17:18:05.075 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.076 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 79.0\n",
      "2021-09-07 17:18:05.076 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 48.25\n",
      "2021-09-07 17:18:05.076 | INFO     | src.policies:collect_trajectories:221 - Episode 609\n",
      "2021-09-07 17:18:05.090 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.090 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 66.0\n",
      "2021-09-07 17:18:05.091 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 51.8\n",
      "2021-09-07 17:18:05.091 | WARNING  | src.policies:train:144 - The actual batch size is 259, instead of 200\n",
      "2021-09-07 17:18:05.094 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:05.095 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40484970808029175, 'baseline_loss': 0.9460307955741882, 'total_loss': 0.06816568970680237}\n",
      "2021-09-07 17:18:05.096 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2570316195487976\n",
      "2021-09-07 17:18:05.097 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.665867805480957\n",
      "2021-09-07 17:18:05.099 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2570316195487976\n",
      "2021-09-07 17:18:05.100 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:05.102 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:05.104 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4594067633152008, 'baseline_loss': 1.0714287757873535, 'total_loss': 0.07630762457847595}\n",
      "2021-09-07 17:18:05.105 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21301037073135376\n",
      "2021-09-07 17:18:05.106 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5231294631958008\n",
      "2021-09-07 17:18:05.107 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21301037073135376\n",
      "2021-09-07 17:18:05.108 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:05.109 | INFO     | src.policies:train:123 - Epoch 81 / 800\n",
      "2021-09-07 17:18:05.110 | INFO     | src.policies:collect_trajectories:221 - Episode 610\n",
      "2021-09-07 17:18:05.114 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.115 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:18:05.115 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.0\n",
      "2021-09-07 17:18:05.116 | INFO     | src.policies:collect_trajectories:221 - Episode 611\n",
      "2021-09-07 17:18:05.123 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.124 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 34.0\n",
      "2021-09-07 17:18:05.124 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 29.5\n",
      "2021-09-07 17:18:05.125 | INFO     | src.policies:collect_trajectories:221 - Episode 612\n",
      "2021-09-07 17:18:05.137 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:05.138 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 71.0\n",
      "2021-09-07 17:18:05.138 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 43.333333333333336\n",
      "2021-09-07 17:18:05.139 | INFO     | src.policies:collect_trajectories:221 - Episode 613\n",
      "2021-09-07 17:18:05.150 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.150 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 38.0\n",
      "2021-09-07 17:18:05.151 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 42.0\n",
      "2021-09-07 17:18:05.151 | INFO     | src.policies:collect_trajectories:221 - Episode 614\n",
      "2021-09-07 17:18:05.157 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.158 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 22.0\n",
      "2021-09-07 17:18:05.158 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 38.0\n",
      "2021-09-07 17:18:05.159 | INFO     | src.policies:collect_trajectories:221 - Episode 615\n",
      "2021-09-07 17:18:05.172 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.173 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 65.0\n",
      "2021-09-07 17:18:05.174 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 42.5\n",
      "2021-09-07 17:18:05.175 | WARNING  | src.policies:train:144 - The actual batch size is 255, instead of 200\n",
      "2021-09-07 17:18:05.182 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:05.184 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41758468747138977, 'baseline_loss': 1.0937678813934326, 'total_loss': 0.12929925322532654}\n",
      "2021-09-07 17:18:05.185 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23120208084583282\n",
      "2021-09-07 17:18:05.186 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4259968101978302\n",
      "2021-09-07 17:18:05.188 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23120208084583282\n",
      "2021-09-07 17:18:05.189 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4259968101978302\n",
      "2021-09-07 17:18:05.191 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:05.193 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3397027254104614, 'baseline_loss': 0.9576424956321716, 'total_loss': 0.1391185224056244}\n",
      "2021-09-07 17:18:05.194 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14602693915367126\n",
      "2021-09-07 17:18:05.195 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6585605144500732\n",
      "2021-09-07 17:18:05.197 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14602693915367126\n",
      "2021-09-07 17:18:05.199 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:05.202 | INFO     | src.policies:train:123 - Epoch 82 / 800\n",
      "2021-09-07 17:18:05.203 | INFO     | src.policies:collect_trajectories:221 - Episode 616\n",
      "2021-09-07 17:18:05.212 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.213 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 45.0\n",
      "2021-09-07 17:18:05.213 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 45.0\n",
      "2021-09-07 17:18:05.214 | INFO     | src.policies:collect_trajectories:221 - Episode 617\n",
      "2021-09-07 17:18:05.224 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.224 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 45.0\n",
      "2021-09-07 17:18:05.225 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 45.0\n",
      "2021-09-07 17:18:05.225 | INFO     | src.policies:collect_trajectories:221 - Episode 618\n",
      "2021-09-07 17:18:05.234 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.234 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 41.0\n",
      "2021-09-07 17:18:05.235 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 43.666666666666664\n",
      "2021-09-07 17:18:05.235 | INFO     | src.policies:collect_trajectories:221 - Episode 619\n",
      "2021-09-07 17:18:05.244 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.245 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 42.0\n",
      "2021-09-07 17:18:05.245 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 43.25\n",
      "2021-09-07 17:18:05.246 | INFO     | src.policies:collect_trajectories:221 - Episode 620\n",
      "2021-09-07 17:18:05.253 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.253 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 37.0\n",
      "2021-09-07 17:18:05.254 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 42.0\n",
      "2021-09-07 17:18:05.254 | WARNING  | src.policies:train:144 - The actual batch size is 210, instead of 200\n",
      "2021-09-07 17:18:05.257 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:05.259 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4453517496585846, 'baseline_loss': 1.0791751146316528, 'total_loss': 0.09423580765724182}\n",
      "2021-09-07 17:18:05.260 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1318749338388443\n",
      "2021-09-07 17:18:05.262 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.26220405101776123\n",
      "2021-09-07 17:18:05.263 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1318749338388443\n",
      "2021-09-07 17:18:05.264 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.26220405101776123\n",
      "2021-09-07 17:18:05.265 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:05.266 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5123486518859863, 'baseline_loss': 1.222521185874939, 'total_loss': 0.09891194105148315}\n",
      "2021-09-07 17:18:05.267 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1681697815656662\n",
      "2021-09-07 17:18:05.268 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.36793291568756104\n",
      "2021-09-07 17:18:05.269 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1681697815656662\n",
      "2021-09-07 17:18:05.270 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.36793291568756104\n",
      "2021-09-07 17:18:05.272 | INFO     | src.policies:train:123 - Epoch 83 / 800\n",
      "2021-09-07 17:18:05.272 | INFO     | src.policies:collect_trajectories:221 - Episode 621\n",
      "2021-09-07 17:18:05.282 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.282 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 52.0\n",
      "2021-09-07 17:18:05.283 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 52.0\n",
      "2021-09-07 17:18:05.283 | INFO     | src.policies:collect_trajectories:221 - Episode 622\n",
      "2021-09-07 17:18:05.291 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.292 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 42.0\n",
      "2021-09-07 17:18:05.293 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 47.0\n",
      "2021-09-07 17:18:05.293 | INFO     | src.policies:collect_trajectories:221 - Episode 623\n",
      "2021-09-07 17:18:05.305 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.306 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:05.307 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 51.333333333333336\n",
      "2021-09-07 17:18:05.307 | INFO     | src.policies:collect_trajectories:221 - Episode 624\n",
      "2021-09-07 17:18:05.313 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.314 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:18:05.314 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 44.75\n",
      "2021-09-07 17:18:05.314 | INFO     | src.policies:collect_trajectories:221 - Episode 625\n",
      "2021-09-07 17:18:05.509 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.510 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 26.0\n",
      "2021-09-07 17:18:05.510 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 41.0\n",
      "2021-09-07 17:18:05.511 | WARNING  | src.policies:train:144 - The actual batch size is 205, instead of 200\n",
      "2021-09-07 17:18:05.514 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:05.515 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4220067262649536, 'baseline_loss': 0.9185493588447571, 'total_loss': 0.03726795315742493}\n",
      "2021-09-07 17:18:05.516 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23004372417926788\n",
      "2021-09-07 17:18:05.517 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.62261962890625\n",
      "2021-09-07 17:18:05.518 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23004372417926788\n",
      "2021-09-07 17:18:05.520 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:05.521 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:05.523 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45119377970695496, 'baseline_loss': 0.9594660997390747, 'total_loss': 0.028539270162582397}\n",
      "2021-09-07 17:18:05.524 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22351466119289398\n",
      "2021-09-07 17:18:05.525 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5326004028320312\n",
      "2021-09-07 17:18:05.526 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22351466119289398\n",
      "2021-09-07 17:18:05.527 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:05.528 | INFO     | src.policies:train:123 - Epoch 84 / 800\n",
      "2021-09-07 17:18:05.529 | INFO     | src.policies:collect_trajectories:221 - Episode 626\n",
      "2021-09-07 17:18:05.535 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.536 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 36.0\n",
      "2021-09-07 17:18:05.536 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 36.0\n",
      "2021-09-07 17:18:05.537 | INFO     | src.policies:collect_trajectories:221 - Episode 627\n",
      "2021-09-07 17:18:05.552 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.553 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 77.0\n",
      "2021-09-07 17:18:05.553 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 56.5\n",
      "2021-09-07 17:18:05.553 | INFO     | src.policies:collect_trajectories:221 - Episode 628\n",
      "2021-09-07 17:18:05.566 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.566 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 59.0\n",
      "2021-09-07 17:18:05.567 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 57.333333333333336\n",
      "2021-09-07 17:18:05.567 | INFO     | src.policies:collect_trajectories:221 - Episode 629\n",
      "2021-09-07 17:18:05.572 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.573 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:18:05.573 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 47.25\n",
      "2021-09-07 17:18:05.573 | INFO     | src.policies:collect_trajectories:221 - Episode 630\n",
      "2021-09-07 17:18:05.586 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.587 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 63.0\n",
      "2021-09-07 17:18:05.588 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 50.4\n",
      "2021-09-07 17:18:05.589 | WARNING  | src.policies:train:144 - The actual batch size is 252, instead of 200\n",
      "2021-09-07 17:18:05.591 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:05.593 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5226420760154724, 'baseline_loss': 1.4671419858932495, 'total_loss': 0.21092891693115234}\n",
      "2021-09-07 17:18:05.594 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09997003525495529\n",
      "2021-09-07 17:18:05.595 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7433333396911621\n",
      "2021-09-07 17:18:05.596 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09997003525495529\n",
      "2021-09-07 17:18:05.597 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:05.599 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:05.601 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5332121849060059, 'baseline_loss': 1.3457123041152954, 'total_loss': 0.13964396715164185}\n",
      "2021-09-07 17:18:05.602 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.48838865756988525\n",
      "2021-09-07 17:18:05.604 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8281792402267456\n",
      "2021-09-07 17:18:05.605 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.48838865756988525\n",
      "2021-09-07 17:18:05.606 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:05.608 | INFO     | src.policies:train:123 - Epoch 85 / 800\n",
      "2021-09-07 17:18:05.608 | INFO     | src.policies:collect_trajectories:221 - Episode 631\n",
      "2021-09-07 17:18:05.629 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.630 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 113.0\n",
      "2021-09-07 17:18:05.631 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 113.0\n",
      "2021-09-07 17:18:05.631 | INFO     | src.policies:collect_trajectories:221 - Episode 632\n",
      "2021-09-07 17:18:05.638 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.639 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 31.0\n",
      "2021-09-07 17:18:05.639 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 72.0\n",
      "2021-09-07 17:18:05.640 | INFO     | src.policies:collect_trajectories:221 - Episode 633\n",
      "2021-09-07 17:18:05.652 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.653 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 61.0\n",
      "2021-09-07 17:18:05.653 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 68.33333333333333\n",
      "2021-09-07 17:18:05.654 | WARNING  | src.policies:train:144 - The actual batch size is 205, instead of 200\n",
      "2021-09-07 17:18:05.657 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:05.659 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.47037220001220703, 'baseline_loss': 1.503822922706604, 'total_loss': 0.28153926134109497}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:05.660 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1929067075252533\n",
      "2021-09-07 17:18:05.661 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8657205104827881\n",
      "2021-09-07 17:18:05.662 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1929067075252533\n",
      "2021-09-07 17:18:05.664 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:05.665 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:05.666 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5015142560005188, 'baseline_loss': 1.4489381313323975, 'total_loss': 0.22295480966567993}\n",
      "2021-09-07 17:18:05.667 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.29608049988746643\n",
      "2021-09-07 17:18:05.668 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7379629015922546\n",
      "2021-09-07 17:18:05.669 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.29608049988746643\n",
      "2021-09-07 17:18:05.670 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:05.672 | INFO     | src.policies:train:123 - Epoch 86 / 800\n",
      "2021-09-07 17:18:05.672 | INFO     | src.policies:collect_trajectories:221 - Episode 634\n",
      "2021-09-07 17:18:05.684 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.684 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 62.0\n",
      "2021-09-07 17:18:05.685 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 62.0\n",
      "2021-09-07 17:18:05.685 | INFO     | src.policies:collect_trajectories:221 - Episode 635\n",
      "2021-09-07 17:18:05.697 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.698 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 62.0\n",
      "2021-09-07 17:18:05.699 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 62.0\n",
      "2021-09-07 17:18:05.700 | INFO     | src.policies:collect_trajectories:221 - Episode 636\n",
      "2021-09-07 17:18:05.719 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.719 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 90.0\n",
      "2021-09-07 17:18:05.720 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 71.33333333333333\n",
      "2021-09-07 17:18:05.721 | WARNING  | src.policies:train:144 - The actual batch size is 214, instead of 200\n",
      "2021-09-07 17:18:05.724 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:05.726 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5758583545684814, 'baseline_loss': 1.3796021938323975, 'total_loss': 0.11394274234771729}\n",
      "2021-09-07 17:18:05.727 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19546964764595032\n",
      "2021-09-07 17:18:05.728 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6022310256958008\n",
      "2021-09-07 17:18:05.730 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19546964764595032\n",
      "2021-09-07 17:18:05.731 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:05.732 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:05.733 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.46895843744277954, 'baseline_loss': 1.4551504850387573, 'total_loss': 0.2586168050765991}\n",
      "2021-09-07 17:18:05.734 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2503291666507721\n",
      "2021-09-07 17:18:05.735 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8617745637893677\n",
      "2021-09-07 17:18:05.736 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2503291666507721\n",
      "2021-09-07 17:18:05.737 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:05.739 | INFO     | src.policies:train:123 - Epoch 87 / 800\n",
      "2021-09-07 17:18:05.739 | INFO     | src.policies:collect_trajectories:221 - Episode 637\n",
      "2021-09-07 17:18:05.744 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.744 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 13.0\n",
      "2021-09-07 17:18:05.745 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 13.0\n",
      "2021-09-07 17:18:05.745 | INFO     | src.policies:collect_trajectories:221 - Episode 638\n",
      "2021-09-07 17:18:05.751 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.751 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:18:05.752 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.5\n",
      "2021-09-07 17:18:05.752 | INFO     | src.policies:collect_trajectories:221 - Episode 639\n",
      "2021-09-07 17:18:05.757 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.757 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:18:05.758 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 18.666666666666668\n",
      "2021-09-07 17:18:05.759 | INFO     | src.policies:collect_trajectories:221 - Episode 640\n",
      "2021-09-07 17:18:05.764 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.765 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:18:05.766 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 19.25\n",
      "2021-09-07 17:18:05.766 | INFO     | src.policies:collect_trajectories:221 - Episode 641\n",
      "2021-09-07 17:18:05.770 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.770 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 11.0\n",
      "2021-09-07 17:18:05.771 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 17.6\n",
      "2021-09-07 17:18:05.771 | INFO     | src.policies:collect_trajectories:221 - Episode 642\n",
      "2021-09-07 17:18:05.786 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.786 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 68.0\n",
      "2021-09-07 17:18:05.787 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 26.0\n",
      "2021-09-07 17:18:05.787 | INFO     | src.policies:collect_trajectories:221 - Episode 643\n",
      "2021-09-07 17:18:05.805 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.806 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 91.0\n",
      "2021-09-07 17:18:05.807 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 35.285714285714285\n",
      "2021-09-07 17:18:05.808 | WARNING  | src.policies:train:144 - The actual batch size is 247, instead of 200\n",
      "2021-09-07 17:18:05.811 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:05.812 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5902048349380493, 'baseline_loss': 1.2759572267532349, 'total_loss': 0.047773778438568115}\n",
      "2021-09-07 17:18:05.813 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25060611963272095\n",
      "2021-09-07 17:18:05.814 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.48123401403427124\n",
      "2021-09-07 17:18:05.815 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25060611963272095\n",
      "2021-09-07 17:18:05.816 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.48123401403427124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:05.818 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:05.819 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.49479275941848755, 'baseline_loss': 1.2154130935668945, 'total_loss': 0.11291378736495972}\n",
      "2021-09-07 17:18:05.820 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19849590957164764\n",
      "2021-09-07 17:18:05.821 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.464422345161438\n",
      "2021-09-07 17:18:05.822 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19849590957164764\n",
      "2021-09-07 17:18:05.823 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.464422345161438\n",
      "2021-09-07 17:18:05.825 | INFO     | src.policies:train:123 - Epoch 88 / 800\n",
      "2021-09-07 17:18:05.826 | INFO     | src.policies:collect_trajectories:221 - Episode 644\n",
      "2021-09-07 17:18:05.829 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.830 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 15.0\n",
      "2021-09-07 17:18:05.830 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 15.0\n",
      "2021-09-07 17:18:05.831 | INFO     | src.policies:collect_trajectories:221 - Episode 645\n",
      "2021-09-07 17:18:05.836 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.836 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 19.0\n",
      "2021-09-07 17:18:05.837 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 17.0\n",
      "2021-09-07 17:18:05.838 | INFO     | src.policies:collect_trajectories:221 - Episode 646\n",
      "2021-09-07 17:18:05.845 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.847 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 41.0\n",
      "2021-09-07 17:18:05.847 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 25.0\n",
      "2021-09-07 17:18:05.848 | INFO     | src.policies:collect_trajectories:221 - Episode 647\n",
      "2021-09-07 17:18:05.861 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.862 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 57.0\n",
      "2021-09-07 17:18:05.862 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 33.0\n",
      "2021-09-07 17:18:05.862 | INFO     | src.policies:collect_trajectories:221 - Episode 648\n",
      "2021-09-07 17:18:05.871 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.871 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 38.0\n",
      "2021-09-07 17:18:05.872 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 34.0\n",
      "2021-09-07 17:18:05.872 | INFO     | src.policies:collect_trajectories:221 - Episode 649\n",
      "2021-09-07 17:18:05.892 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.892 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 104.0\n",
      "2021-09-07 17:18:05.893 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 45.666666666666664\n",
      "2021-09-07 17:18:05.894 | WARNING  | src.policies:train:144 - The actual batch size is 274, instead of 200\n",
      "2021-09-07 17:18:05.897 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:05.898 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5210385322570801, 'baseline_loss': 1.2441099882125854, 'total_loss': 0.10101646184921265}\n",
      "2021-09-07 17:18:05.900 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4555369019508362\n",
      "2021-09-07 17:18:05.902 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.43287718296051025\n",
      "2021-09-07 17:18:05.903 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4555369019508362\n",
      "2021-09-07 17:18:05.905 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.43287718296051025\n",
      "2021-09-07 17:18:05.906 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:05.908 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42585432529449463, 'baseline_loss': 1.2604432106018066, 'total_loss': 0.2043672800064087}\n",
      "2021-09-07 17:18:05.909 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18507853150367737\n",
      "2021-09-07 17:18:05.910 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4399011433124542\n",
      "2021-09-07 17:18:05.911 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18507853150367737\n",
      "2021-09-07 17:18:05.912 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4399011433124542\n",
      "2021-09-07 17:18:05.914 | INFO     | src.policies:train:123 - Epoch 89 / 800\n",
      "2021-09-07 17:18:05.915 | INFO     | src.policies:collect_trajectories:221 - Episode 650\n",
      "2021-09-07 17:18:05.922 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.923 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 32.0\n",
      "2021-09-07 17:18:05.923 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 32.0\n",
      "2021-09-07 17:18:05.924 | INFO     | src.policies:collect_trajectories:221 - Episode 651\n",
      "2021-09-07 17:18:05.949 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.949 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 131.0\n",
      "2021-09-07 17:18:05.950 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 81.5\n",
      "2021-09-07 17:18:05.950 | INFO     | src.policies:collect_trajectories:221 - Episode 652\n",
      "2021-09-07 17:18:05.961 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.961 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 46.0\n",
      "2021-09-07 17:18:05.962 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 69.66666666666667\n",
      "2021-09-07 17:18:05.963 | WARNING  | src.policies:train:144 - The actual batch size is 209, instead of 200\n",
      "2021-09-07 17:18:05.965 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:05.967 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45603954792022705, 'baseline_loss': 1.2527862787246704, 'total_loss': 0.17035359144210815}\n",
      "2021-09-07 17:18:05.968 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12429401278495789\n",
      "2021-09-07 17:18:05.969 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7375257015228271\n",
      "2021-09-07 17:18:05.970 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12429401278495789\n",
      "2021-09-07 17:18:05.971 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:05.972 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:05.973 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.48771584033966064, 'baseline_loss': 1.3321019411087036, 'total_loss': 0.17833513021469116}\n",
      "2021-09-07 17:18:05.974 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1546601802110672\n",
      "2021-09-07 17:18:05.975 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7011495232582092\n",
      "2021-09-07 17:18:05.976 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1546601802110672\n",
      "2021-09-07 17:18:05.978 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:05.979 | INFO     | src.policies:train:123 - Epoch 90 / 800\n",
      "2021-09-07 17:18:05.980 | INFO     | src.policies:collect_trajectories:221 - Episode 653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:05.986 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:05.987 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 26.0\n",
      "2021-09-07 17:18:05.988 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 26.0\n",
      "2021-09-07 17:18:05.988 | INFO     | src.policies:collect_trajectories:221 - Episode 654\n",
      "2021-09-07 17:18:06.001 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.002 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 65.0\n",
      "2021-09-07 17:18:06.002 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 45.5\n",
      "2021-09-07 17:18:06.003 | INFO     | src.policies:collect_trajectories:221 - Episode 655\n",
      "2021-09-07 17:18:06.161 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.162 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 71.0\n",
      "2021-09-07 17:18:06.162 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 54.0\n",
      "2021-09-07 17:18:06.163 | INFO     | src.policies:collect_trajectories:221 - Episode 656\n",
      "2021-09-07 17:18:06.175 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.176 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 61.0\n",
      "2021-09-07 17:18:06.177 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 55.75\n",
      "2021-09-07 17:18:06.177 | WARNING  | src.policies:train:144 - The actual batch size is 223, instead of 200\n",
      "2021-09-07 17:18:06.179 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:06.182 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.489963561296463, 'baseline_loss': 1.3433352708816528, 'total_loss': 0.1817040741443634}\n",
      "2021-09-07 17:18:06.183 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2994231879711151\n",
      "2021-09-07 17:18:06.184 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.877628743648529\n",
      "2021-09-07 17:18:06.185 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2994231879711151\n",
      "2021-09-07 17:18:06.186 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:06.187 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:06.188 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5567254424095154, 'baseline_loss': 1.3378604650497437, 'total_loss': 0.11220479011535645}\n",
      "2021-09-07 17:18:06.189 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.26011282205581665\n",
      "2021-09-07 17:18:06.190 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6129992008209229\n",
      "2021-09-07 17:18:06.191 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.26011282205581665\n",
      "2021-09-07 17:18:06.192 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:06.194 | INFO     | src.policies:train:123 - Epoch 91 / 800\n",
      "2021-09-07 17:18:06.194 | INFO     | src.policies:collect_trajectories:221 - Episode 657\n",
      "2021-09-07 17:18:06.198 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.199 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:06.199 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.0\n",
      "2021-09-07 17:18:06.200 | INFO     | src.policies:collect_trajectories:221 - Episode 658\n",
      "2021-09-07 17:18:06.214 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.214 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 80.0\n",
      "2021-09-07 17:18:06.215 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 50.0\n",
      "2021-09-07 17:18:06.215 | INFO     | src.policies:collect_trajectories:221 - Episode 659\n",
      "2021-09-07 17:18:06.233 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.235 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 108.0\n",
      "2021-09-07 17:18:06.235 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 69.33333333333333\n",
      "2021-09-07 17:18:06.236 | WARNING  | src.policies:train:144 - The actual batch size is 208, instead of 200\n",
      "2021-09-07 17:18:06.238 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:06.240 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2868137061595917, 'baseline_loss': 0.6756975650787354, 'total_loss': 0.051035076379776}\n",
      "2021-09-07 17:18:06.241 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.35436004400253296\n",
      "2021-09-07 17:18:06.242 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.245549201965332\n",
      "2021-09-07 17:18:06.243 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.35436004400253296\n",
      "2021-09-07 17:18:06.244 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:06.246 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:06.247 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3097226619720459, 'baseline_loss': 0.6749284267425537, 'total_loss': 0.027741551399230957}\n",
      "2021-09-07 17:18:06.248 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17084984481334686\n",
      "2021-09-07 17:18:06.249 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7795877456665039\n",
      "2021-09-07 17:18:06.250 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17084984481334686\n",
      "2021-09-07 17:18:06.251 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:06.253 | INFO     | src.policies:train:123 - Epoch 92 / 800\n",
      "2021-09-07 17:18:06.253 | INFO     | src.policies:collect_trajectories:221 - Episode 660\n",
      "2021-09-07 17:18:06.259 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.260 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 35.0\n",
      "2021-09-07 17:18:06.260 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 35.0\n",
      "2021-09-07 17:18:06.261 | INFO     | src.policies:collect_trajectories:221 - Episode 661\n",
      "2021-09-07 17:18:06.275 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.276 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 81.0\n",
      "2021-09-07 17:18:06.276 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 58.0\n",
      "2021-09-07 17:18:06.276 | INFO     | src.policies:collect_trajectories:221 - Episode 662\n",
      "2021-09-07 17:18:06.282 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.283 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 16.0\n",
      "2021-09-07 17:18:06.283 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 44.0\n",
      "2021-09-07 17:18:06.284 | INFO     | src.policies:collect_trajectories:221 - Episode 663\n",
      "2021-09-07 17:18:06.291 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.292 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 40.0\n",
      "2021-09-07 17:18:06.292 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 43.0\n",
      "2021-09-07 17:18:06.292 | INFO     | src.policies:collect_trajectories:221 - Episode 664\n",
      "2021-09-07 17:18:06.303 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:06.304 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 56.0\n",
      "2021-09-07 17:18:06.304 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 45.6\n",
      "2021-09-07 17:18:06.305 | WARNING  | src.policies:train:144 - The actual batch size is 228, instead of 200\n",
      "2021-09-07 17:18:06.308 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:06.309 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3086192011833191, 'baseline_loss': 0.8722154498100281, 'total_loss': 0.12748852372169495}\n",
      "2021-09-07 17:18:06.310 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10154945403337479\n",
      "2021-09-07 17:18:06.311 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6041718125343323\n",
      "2021-09-07 17:18:06.312 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10154945403337479\n",
      "2021-09-07 17:18:06.313 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:06.314 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:06.315 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3813050091266632, 'baseline_loss': 0.9112550616264343, 'total_loss': 0.07432252168655396}\n",
      "2021-09-07 17:18:06.316 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2815355658531189\n",
      "2021-09-07 17:18:06.317 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.750035285949707\n",
      "2021-09-07 17:18:06.318 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2815355658531189\n",
      "2021-09-07 17:18:06.319 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:06.321 | INFO     | src.policies:train:123 - Epoch 93 / 800\n",
      "2021-09-07 17:18:06.322 | INFO     | src.policies:collect_trajectories:221 - Episode 665\n",
      "2021-09-07 17:18:06.327 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.327 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 26.0\n",
      "2021-09-07 17:18:06.328 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 26.0\n",
      "2021-09-07 17:18:06.328 | INFO     | src.policies:collect_trajectories:221 - Episode 666\n",
      "2021-09-07 17:18:06.349 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.349 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 135.0\n",
      "2021-09-07 17:18:06.350 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 80.5\n",
      "2021-09-07 17:18:06.350 | INFO     | src.policies:collect_trajectories:221 - Episode 667\n",
      "2021-09-07 17:18:06.358 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.358 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 40.0\n",
      "2021-09-07 17:18:06.359 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 67.0\n",
      "2021-09-07 17:18:06.361 | WARNING  | src.policies:train:144 - The actual batch size is 201, instead of 200\n",
      "2021-09-07 17:18:06.365 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:06.367 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27734771370887756, 'baseline_loss': 0.8374969363212585, 'total_loss': 0.1414007544517517}\n",
      "2021-09-07 17:18:06.368 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1553920954465866\n",
      "2021-09-07 17:18:06.369 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.614849328994751\n",
      "2021-09-07 17:18:06.370 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1553920954465866\n",
      "2021-09-07 17:18:06.371 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:06.372 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:06.373 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3546639680862427, 'baseline_loss': 0.9274487495422363, 'total_loss': 0.10906040668487549}\n",
      "2021-09-07 17:18:06.374 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1676768660545349\n",
      "2021-09-07 17:18:06.375 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5037639141082764\n",
      "2021-09-07 17:18:06.376 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1676768660545349\n",
      "2021-09-07 17:18:06.377 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999895691871643\n",
      "2021-09-07 17:18:06.379 | INFO     | src.policies:train:123 - Epoch 94 / 800\n",
      "2021-09-07 17:18:06.379 | INFO     | src.policies:collect_trajectories:221 - Episode 668\n",
      "2021-09-07 17:18:06.391 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.391 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 61.0\n",
      "2021-09-07 17:18:06.392 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 61.0\n",
      "2021-09-07 17:18:06.392 | INFO     | src.policies:collect_trajectories:221 - Episode 669\n",
      "2021-09-07 17:18:06.404 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.405 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 64.0\n",
      "2021-09-07 17:18:06.405 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 62.5\n",
      "2021-09-07 17:18:06.406 | INFO     | src.policies:collect_trajectories:221 - Episode 670\n",
      "2021-09-07 17:18:06.410 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.411 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:06.411 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 48.333333333333336\n",
      "2021-09-07 17:18:06.412 | INFO     | src.policies:collect_trajectories:221 - Episode 671\n",
      "2021-09-07 17:18:06.418 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.419 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 33.0\n",
      "2021-09-07 17:18:06.419 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 44.5\n",
      "2021-09-07 17:18:06.420 | INFO     | src.policies:collect_trajectories:221 - Episode 672\n",
      "2021-09-07 17:18:06.433 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.433 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 60.0\n",
      "2021-09-07 17:18:06.434 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 47.6\n",
      "2021-09-07 17:18:06.434 | WARNING  | src.policies:train:144 - The actual batch size is 238, instead of 200\n",
      "2021-09-07 17:18:06.437 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:06.438 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45986270904541016, 'baseline_loss': 1.0460455417633057, 'total_loss': 0.06316006183624268}\n",
      "2021-09-07 17:18:06.439 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22021569311618805\n",
      "2021-09-07 17:18:06.441 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4526185095310211\n",
      "2021-09-07 17:18:06.442 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22021569311618805\n",
      "2021-09-07 17:18:06.444 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4526185095310211\n",
      "2021-09-07 17:18:06.445 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:06.446 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4688836336135864, 'baseline_loss': 1.077886700630188, 'total_loss': 0.07005971670150757}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:06.448 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4185495674610138\n",
      "2021-09-07 17:18:06.449 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3027212917804718\n",
      "2021-09-07 17:18:06.450 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4185495674610138\n",
      "2021-09-07 17:18:06.451 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3027212917804718\n",
      "2021-09-07 17:18:06.452 | INFO     | src.policies:train:123 - Epoch 95 / 800\n",
      "2021-09-07 17:18:06.453 | INFO     | src.policies:collect_trajectories:221 - Episode 673\n",
      "2021-09-07 17:18:06.472 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.472 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 107.0\n",
      "2021-09-07 17:18:06.473 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 107.0\n",
      "2021-09-07 17:18:06.474 | INFO     | src.policies:collect_trajectories:221 - Episode 674\n",
      "2021-09-07 17:18:06.493 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.494 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 94.0\n",
      "2021-09-07 17:18:06.495 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 100.5\n",
      "2021-09-07 17:18:06.495 | WARNING  | src.policies:train:144 - The actual batch size is 201, instead of 200\n",
      "2021-09-07 17:18:06.498 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:06.499 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41343894600868225, 'baseline_loss': 1.0875910520553589, 'total_loss': 0.1303565800189972}\n",
      "2021-09-07 17:18:06.501 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17425958812236786\n",
      "2021-09-07 17:18:06.502 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8990334272384644\n",
      "2021-09-07 17:18:06.503 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17425958812236786\n",
      "2021-09-07 17:18:06.505 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:06.506 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:06.508 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5189002752304077, 'baseline_loss': 1.0493139028549194, 'total_loss': 0.005756676197052002}\n",
      "2021-09-07 17:18:06.509 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4756590723991394\n",
      "2021-09-07 17:18:06.510 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8130301833152771\n",
      "2021-09-07 17:18:06.511 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4756590723991394\n",
      "2021-09-07 17:18:06.512 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:06.513 | INFO     | src.policies:train:123 - Epoch 96 / 800\n",
      "2021-09-07 17:18:06.514 | INFO     | src.policies:collect_trajectories:221 - Episode 675\n",
      "2021-09-07 17:18:06.542 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.543 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 170.0\n",
      "2021-09-07 17:18:06.543 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 170.0\n",
      "2021-09-07 17:18:06.544 | INFO     | src.policies:collect_trajectories:221 - Episode 676\n",
      "2021-09-07 17:18:06.556 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.557 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 73.0\n",
      "2021-09-07 17:18:06.557 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 121.5\n",
      "2021-09-07 17:18:06.558 | WARNING  | src.policies:train:144 - The actual batch size is 243, instead of 200\n",
      "2021-09-07 17:18:06.562 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:06.564 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43650081753730774, 'baseline_loss': 1.2047052383422852, 'total_loss': 0.16585180163383484}\n",
      "2021-09-07 17:18:06.565 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09554725140333176\n",
      "2021-09-07 17:18:06.566 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.47881537675857544\n",
      "2021-09-07 17:18:06.567 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09554725140333176\n",
      "2021-09-07 17:18:06.568 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.47881537675857544\n",
      "2021-09-07 17:18:06.569 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:06.571 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4601958096027374, 'baseline_loss': 1.1052039861679077, 'total_loss': 0.09240618348121643}\n",
      "2021-09-07 17:18:06.571 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14626987278461456\n",
      "2021-09-07 17:18:06.572 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4506800174713135\n",
      "2021-09-07 17:18:06.573 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14626987278461456\n",
      "2021-09-07 17:18:06.574 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4506800174713135\n",
      "2021-09-07 17:18:06.576 | INFO     | src.policies:train:123 - Epoch 97 / 800\n",
      "2021-09-07 17:18:06.576 | INFO     | src.policies:collect_trajectories:221 - Episode 677\n",
      "2021-09-07 17:18:06.588 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.588 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 69.0\n",
      "2021-09-07 17:18:06.589 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 69.0\n",
      "2021-09-07 17:18:06.589 | INFO     | src.policies:collect_trajectories:221 - Episode 678\n",
      "2021-09-07 17:18:06.616 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.617 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 167.0\n",
      "2021-09-07 17:18:06.617 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 118.0\n",
      "2021-09-07 17:18:06.618 | WARNING  | src.policies:train:144 - The actual batch size is 236, instead of 200\n",
      "2021-09-07 17:18:06.622 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:06.625 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4652794301509857, 'baseline_loss': 0.8882551789283752, 'total_loss': -0.021151840686798096}\n",
      "2021-09-07 17:18:06.627 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22416479885578156\n",
      "2021-09-07 17:18:06.628 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5528930425643921\n",
      "2021-09-07 17:18:06.629 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22416479885578156\n",
      "2021-09-07 17:18:06.630 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:06.631 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:06.632 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3846864402294159, 'baseline_loss': 0.7527865767478943, 'total_loss': -0.00829315185546875}\n",
      "2021-09-07 17:18:06.633 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12756788730621338\n",
      "2021-09-07 17:18:06.634 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5957186222076416\n",
      "2021-09-07 17:18:06.635 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12756788730621338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:06.636 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:06.637 | INFO     | src.policies:train:123 - Epoch 98 / 800\n",
      "2021-09-07 17:18:06.638 | INFO     | src.policies:collect_trajectories:221 - Episode 679\n",
      "2021-09-07 17:18:06.651 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.652 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 73.0\n",
      "2021-09-07 17:18:06.658 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 73.0\n",
      "2021-09-07 17:18:06.721 | INFO     | src.policies:collect_trajectories:221 - Episode 680\n",
      "2021-09-07 17:18:06.743 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.743 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 115.0\n",
      "2021-09-07 17:18:06.744 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 94.0\n",
      "2021-09-07 17:18:06.744 | INFO     | src.policies:collect_trajectories:221 - Episode 681\n",
      "2021-09-07 17:18:06.759 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.760 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 92.0\n",
      "2021-09-07 17:18:06.761 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 93.33333333333333\n",
      "2021-09-07 17:18:06.762 | WARNING  | src.policies:train:144 - The actual batch size is 280, instead of 200\n",
      "2021-09-07 17:18:06.766 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:06.768 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.49566495418548584, 'baseline_loss': 0.8788342475891113, 'total_loss': -0.056247830390930176}\n",
      "2021-09-07 17:18:06.769 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4887694716453552\n",
      "2021-09-07 17:18:06.770 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5413343906402588\n",
      "2021-09-07 17:18:06.771 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4887694716453552\n",
      "2021-09-07 17:18:06.772 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:06.773 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:06.774 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.38357555866241455, 'baseline_loss': 0.8282642364501953, 'total_loss': 0.030556559562683105}\n",
      "2021-09-07 17:18:06.775 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22352394461631775\n",
      "2021-09-07 17:18:06.776 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4853370785713196\n",
      "2021-09-07 17:18:06.777 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22352394461631775\n",
      "2021-09-07 17:18:06.778 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4853370785713196\n",
      "2021-09-07 17:18:06.780 | INFO     | src.policies:train:123 - Epoch 99 / 800\n",
      "2021-09-07 17:18:06.781 | INFO     | src.policies:collect_trajectories:221 - Episode 682\n",
      "2021-09-07 17:18:06.794 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.795 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 81.0\n",
      "2021-09-07 17:18:06.795 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 81.0\n",
      "2021-09-07 17:18:06.795 | INFO     | src.policies:collect_trajectories:221 - Episode 683\n",
      "2021-09-07 17:18:06.813 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.814 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 91.0\n",
      "2021-09-07 17:18:06.814 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 86.0\n",
      "2021-09-07 17:18:06.814 | INFO     | src.policies:collect_trajectories:221 - Episode 684\n",
      "2021-09-07 17:18:06.834 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.834 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 106.0\n",
      "2021-09-07 17:18:06.835 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 92.66666666666667\n",
      "2021-09-07 17:18:06.836 | WARNING  | src.policies:train:144 - The actual batch size is 278, instead of 200\n",
      "2021-09-07 17:18:06.838 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:06.841 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.48676350712776184, 'baseline_loss': 1.0965182781219482, 'total_loss': 0.06149563193321228}\n",
      "2021-09-07 17:18:06.842 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2627928853034973\n",
      "2021-09-07 17:18:06.843 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4961385726928711\n",
      "2021-09-07 17:18:06.844 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2627928853034973\n",
      "2021-09-07 17:18:06.845 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4961385726928711\n",
      "2021-09-07 17:18:06.846 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:06.847 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3346717059612274, 'baseline_loss': 1.0968037843704224, 'total_loss': 0.21373018622398376}\n",
      "2021-09-07 17:18:06.848 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1397058069705963\n",
      "2021-09-07 17:18:06.849 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8275833129882812\n",
      "2021-09-07 17:18:06.850 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1397058069705963\n",
      "2021-09-07 17:18:06.851 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:06.852 | INFO     | src.policies:train:123 - Epoch 100 / 800\n",
      "2021-09-07 17:18:06.853 | INFO     | src.policies:collect_trajectories:221 - Episode 685\n",
      "2021-09-07 17:18:06.880 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.881 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 179.0\n",
      "2021-09-07 17:18:06.881 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 179.0\n",
      "2021-09-07 17:18:06.882 | INFO     | src.policies:collect_trajectories:221 - Episode 686\n",
      "2021-09-07 17:18:06.888 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.889 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 34.0\n",
      "2021-09-07 17:18:06.889 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 106.5\n",
      "2021-09-07 17:18:06.891 | WARNING  | src.policies:train:144 - The actual batch size is 213, instead of 200\n",
      "2021-09-07 17:18:06.893 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:06.894 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.37876832485198975, 'baseline_loss': 0.844816267490387, 'total_loss': 0.043639808893203735}\n",
      "2021-09-07 17:18:06.895 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4401084780693054\n",
      "2021-09-07 17:18:06.896 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5968935489654541\n",
      "2021-09-07 17:18:06.897 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4401084780693054\n",
      "2021-09-07 17:18:06.899 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:06.900 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:06.901 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3717147707939148, 'baseline_loss': 0.941964864730835, 'total_loss': 0.09926766157150269}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:06.902 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3213431239128113\n",
      "2021-09-07 17:18:06.903 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4919082522392273\n",
      "2021-09-07 17:18:06.904 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3213431239128113\n",
      "2021-09-07 17:18:06.905 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4919082522392273\n",
      "2021-09-07 17:18:06.906 | INFO     | src.policies:train:123 - Epoch 101 / 800\n",
      "2021-09-07 17:18:06.907 | INFO     | src.policies:collect_trajectories:221 - Episode 687\n",
      "2021-09-07 17:18:06.934 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.935 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 177.0\n",
      "2021-09-07 17:18:06.935 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 177.0\n",
      "2021-09-07 17:18:06.935 | INFO     | src.policies:collect_trajectories:221 - Episode 688\n",
      "2021-09-07 17:18:06.953 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.953 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 90.0\n",
      "2021-09-07 17:18:06.954 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 133.5\n",
      "2021-09-07 17:18:06.955 | WARNING  | src.policies:train:144 - The actual batch size is 267, instead of 200\n",
      "2021-09-07 17:18:06.957 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:06.959 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4957677721977234, 'baseline_loss': 1.2147607803344727, 'total_loss': 0.11161261796951294}\n",
      "2021-09-07 17:18:06.960 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16353879868984222\n",
      "2021-09-07 17:18:06.961 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7433565855026245\n",
      "2021-09-07 17:18:06.962 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16353879868984222\n",
      "2021-09-07 17:18:06.963 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:06.964 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:06.965 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5700067281723022, 'baseline_loss': 1.254258394241333, 'total_loss': 0.05712246894836426}\n",
      "2021-09-07 17:18:06.966 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2804147005081177\n",
      "2021-09-07 17:18:06.967 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6139611005783081\n",
      "2021-09-07 17:18:06.968 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2804147005081177\n",
      "2021-09-07 17:18:06.969 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:06.970 | INFO     | src.policies:train:123 - Epoch 102 / 800\n",
      "2021-09-07 17:18:06.970 | INFO     | src.policies:collect_trajectories:221 - Episode 689\n",
      "2021-09-07 17:18:06.980 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.980 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 50.0\n",
      "2021-09-07 17:18:06.981 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 50.0\n",
      "2021-09-07 17:18:06.981 | INFO     | src.policies:collect_trajectories:221 - Episode 690\n",
      "2021-09-07 17:18:06.991 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:06.992 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 43.0\n",
      "2021-09-07 17:18:06.992 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 46.5\n",
      "2021-09-07 17:18:06.992 | INFO     | src.policies:collect_trajectories:221 - Episode 691\n",
      "2021-09-07 17:18:07.007 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.008 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 85.0\n",
      "2021-09-07 17:18:07.009 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 59.333333333333336\n",
      "2021-09-07 17:18:07.009 | INFO     | src.policies:collect_trajectories:221 - Episode 692\n",
      "2021-09-07 17:18:07.015 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.015 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 24.0\n",
      "2021-09-07 17:18:07.016 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 50.5\n",
      "2021-09-07 17:18:07.016 | WARNING  | src.policies:train:144 - The actual batch size is 202, instead of 200\n",
      "2021-09-07 17:18:07.020 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:07.021 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6718074679374695, 'baseline_loss': 1.7540020942687988, 'total_loss': 0.20519357919692993}\n",
      "2021-09-07 17:18:07.023 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.544791042804718\n",
      "2021-09-07 17:18:07.023 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.4893360137939453\n",
      "2021-09-07 17:18:07.024 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:07.025 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:07.027 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:07.028 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7404521703720093, 'baseline_loss': 1.9226889610290527, 'total_loss': 0.2208923101425171}\n",
      "2021-09-07 17:18:07.029 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6679162979125977\n",
      "2021-09-07 17:18:07.031 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.051353931427002\n",
      "2021-09-07 17:18:07.032 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:07.034 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:07.036 | INFO     | src.policies:train:123 - Epoch 103 / 800\n",
      "2021-09-07 17:18:07.037 | INFO     | src.policies:collect_trajectories:221 - Episode 693\n",
      "2021-09-07 17:18:07.052 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.053 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 55.0\n",
      "2021-09-07 17:18:07.053 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 55.0\n",
      "2021-09-07 17:18:07.054 | INFO     | src.policies:collect_trajectories:221 - Episode 694\n",
      "2021-09-07 17:18:07.082 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.083 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 150.0\n",
      "2021-09-07 17:18:07.084 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 102.5\n",
      "2021-09-07 17:18:07.084 | WARNING  | src.policies:train:144 - The actual batch size is 205, instead of 200\n",
      "2021-09-07 17:18:07.087 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:07.089 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5885673761367798, 'baseline_loss': 1.5574753284454346, 'total_loss': 0.1901702880859375}\n",
      "2021-09-07 17:18:07.090 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15517547726631165\n",
      "2021-09-07 17:18:07.091 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9104301929473877\n",
      "2021-09-07 17:18:07.092 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15517547726631165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:07.093 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:07.095 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:07.096 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6969920992851257, 'baseline_loss': 1.7827702760696411, 'total_loss': 0.19439303874969482}\n",
      "2021-09-07 17:18:07.097 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5955057740211487\n",
      "2021-09-07 17:18:07.098 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5756452083587646\n",
      "2021-09-07 17:18:07.099 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:07.101 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:07.103 | INFO     | src.policies:train:123 - Epoch 104 / 800\n",
      "2021-09-07 17:18:07.103 | INFO     | src.policies:collect_trajectories:221 - Episode 695\n",
      "2021-09-07 17:18:07.137 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.138 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:07.138 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:07.141 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:07.143 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5104436278343201, 'baseline_loss': 1.6411854028701782, 'total_loss': 0.31014907360076904}\n",
      "2021-09-07 17:18:07.144 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.34426701068878174\n",
      "2021-09-07 17:18:07.145 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.280450463294983\n",
      "2021-09-07 17:18:07.146 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.34426701068878174\n",
      "2021-09-07 17:18:07.148 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:07.149 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:07.151 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.46523627638816833, 'baseline_loss': 1.6745681762695312, 'total_loss': 0.3720478117465973}\n",
      "2021-09-07 17:18:07.152 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3399592339992523\n",
      "2021-09-07 17:18:07.153 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.554202675819397\n",
      "2021-09-07 17:18:07.154 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3399592339992523\n",
      "2021-09-07 17:18:07.155 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:07.157 | INFO     | src.policies:train:123 - Epoch 105 / 800\n",
      "2021-09-07 17:18:07.157 | INFO     | src.policies:collect_trajectories:221 - Episode 696\n",
      "2021-09-07 17:18:07.164 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.165 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:18:07.165 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 23.0\n",
      "2021-09-07 17:18:07.166 | INFO     | src.policies:collect_trajectories:221 - Episode 697\n",
      "2021-09-07 17:18:07.205 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.206 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:07.207 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 111.5\n",
      "2021-09-07 17:18:07.207 | WARNING  | src.policies:train:144 - The actual batch size is 223, instead of 200\n",
      "2021-09-07 17:18:07.210 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:07.212 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3115021586418152, 'baseline_loss': 0.872328519821167, 'total_loss': 0.12466210126876831}\n",
      "2021-09-07 17:18:07.213 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18751215934753418\n",
      "2021-09-07 17:18:07.213 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9933591485023499\n",
      "2021-09-07 17:18:07.215 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18751215934753418\n",
      "2021-09-07 17:18:07.216 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:07.217 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:07.219 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3818061947822571, 'baseline_loss': 0.8760349750518799, 'total_loss': 0.05621129274368286}\n",
      "2021-09-07 17:18:07.220 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5223947763442993\n",
      "2021-09-07 17:18:07.221 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1794466972351074\n",
      "2021-09-07 17:18:07.282 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:07.283 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:07.285 | INFO     | src.policies:train:123 - Epoch 106 / 800\n",
      "2021-09-07 17:18:07.285 | INFO     | src.policies:collect_trajectories:221 - Episode 698\n",
      "2021-09-07 17:18:07.290 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.291 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:18:07.291 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 21.0\n",
      "2021-09-07 17:18:07.292 | INFO     | src.policies:collect_trajectories:221 - Episode 699\n",
      "2021-09-07 17:18:07.308 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.309 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 81.0\n",
      "2021-09-07 17:18:07.310 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 51.0\n",
      "2021-09-07 17:18:07.310 | INFO     | src.policies:collect_trajectories:221 - Episode 700\n",
      "2021-09-07 17:18:07.318 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.320 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 40.0\n",
      "2021-09-07 17:18:07.321 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 47.333333333333336\n",
      "2021-09-07 17:18:07.322 | INFO     | src.policies:collect_trajectories:221 - Episode 701\n",
      "2021-09-07 17:18:07.341 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.341 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 99.0\n",
      "2021-09-07 17:18:07.342 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 60.25\n",
      "2021-09-07 17:18:07.343 | WARNING  | src.policies:train:144 - The actual batch size is 241, instead of 200\n",
      "2021-09-07 17:18:07.346 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:07.347 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.46336615085601807, 'baseline_loss': 1.067946434020996, 'total_loss': 0.07060706615447998}\n",
      "2021-09-07 17:18:07.349 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.43816739320755005\n",
      "2021-09-07 17:18:07.349 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.27227675914764404\n",
      "2021-09-07 17:18:07.351 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.43816739320755005\n",
      "2021-09-07 17:18:07.352 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.27227675914764404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:07.353 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:07.354 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3567737638950348, 'baseline_loss': 0.9221231341362, 'total_loss': 0.10428780317306519}\n",
      "2021-09-07 17:18:07.355 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09743321686983109\n",
      "2021-09-07 17:18:07.356 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.46000877022743225\n",
      "2021-09-07 17:18:07.357 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09743321686983109\n",
      "2021-09-07 17:18:07.358 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.46000877022743225\n",
      "2021-09-07 17:18:07.360 | INFO     | src.policies:train:123 - Epoch 107 / 800\n",
      "2021-09-07 17:18:07.361 | INFO     | src.policies:collect_trajectories:221 - Episode 702\n",
      "2021-09-07 17:18:07.385 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.386 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 138.0\n",
      "2021-09-07 17:18:07.387 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 138.0\n",
      "2021-09-07 17:18:07.387 | INFO     | src.policies:collect_trajectories:221 - Episode 703\n",
      "2021-09-07 17:18:07.404 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.405 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 105.0\n",
      "2021-09-07 17:18:07.406 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 121.5\n",
      "2021-09-07 17:18:07.406 | WARNING  | src.policies:train:144 - The actual batch size is 243, instead of 200\n",
      "2021-09-07 17:18:07.408 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:07.410 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3194173276424408, 'baseline_loss': 0.5380430221557617, 'total_loss': -0.050395816564559937}\n",
      "2021-09-07 17:18:07.411 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10051397979259491\n",
      "2021-09-07 17:18:07.412 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9183738827705383\n",
      "2021-09-07 17:18:07.413 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10051397979259491\n",
      "2021-09-07 17:18:07.414 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:07.416 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:07.417 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24333935976028442, 'baseline_loss': 0.5363666415214539, 'total_loss': 0.024843961000442505}\n",
      "2021-09-07 17:18:07.418 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3966881036758423\n",
      "2021-09-07 17:18:07.419 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0310171842575073\n",
      "2021-09-07 17:18:07.421 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3966881036758423\n",
      "2021-09-07 17:18:07.422 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:07.423 | INFO     | src.policies:train:123 - Epoch 108 / 800\n",
      "2021-09-07 17:18:07.424 | INFO     | src.policies:collect_trajectories:221 - Episode 704\n",
      "2021-09-07 17:18:07.448 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.449 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 144.0\n",
      "2021-09-07 17:18:07.449 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 144.0\n",
      "2021-09-07 17:18:07.450 | INFO     | src.policies:collect_trajectories:221 - Episode 705\n",
      "2021-09-07 17:18:07.593 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.594 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 55.0\n",
      "2021-09-07 17:18:07.594 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 99.5\n",
      "2021-09-07 17:18:07.595 | INFO     | src.policies:collect_trajectories:221 - Episode 706\n",
      "2021-09-07 17:18:07.614 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.615 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 112.0\n",
      "2021-09-07 17:18:07.615 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 103.66666666666667\n",
      "2021-09-07 17:18:07.616 | WARNING  | src.policies:train:144 - The actual batch size is 311, instead of 200\n",
      "2021-09-07 17:18:07.621 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:07.623 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2600645422935486, 'baseline_loss': 0.5721439123153687, 'total_loss': 0.026007413864135742}\n",
      "2021-09-07 17:18:07.624 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2054162621498108\n",
      "2021-09-07 17:18:07.625 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0721954107284546\n",
      "2021-09-07 17:18:07.626 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2054162621498108\n",
      "2021-09-07 17:18:07.627 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:07.628 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:07.630 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2543891966342926, 'baseline_loss': 0.5245698690414429, 'total_loss': 0.007895737886428833}\n",
      "2021-09-07 17:18:07.631 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20531708002090454\n",
      "2021-09-07 17:18:07.632 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.16846764087677\n",
      "2021-09-07 17:18:07.633 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20531708002090454\n",
      "2021-09-07 17:18:07.634 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:07.636 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:07.637 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27943554520606995, 'baseline_loss': 0.5907440781593323, 'total_loss': 0.01593649387359619}\n",
      "2021-09-07 17:18:07.638 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.29700055718421936\n",
      "2021-09-07 17:18:07.639 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9270879030227661\n",
      "2021-09-07 17:18:07.641 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.29700055718421936\n",
      "2021-09-07 17:18:07.642 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:07.644 | INFO     | src.policies:train:123 - Epoch 109 / 800\n",
      "2021-09-07 17:18:07.645 | INFO     | src.policies:collect_trajectories:221 - Episode 707\n",
      "2021-09-07 17:18:07.653 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.654 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 33.0\n",
      "2021-09-07 17:18:07.654 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 33.0\n",
      "2021-09-07 17:18:07.655 | INFO     | src.policies:collect_trajectories:221 - Episode 708\n",
      "2021-09-07 17:18:07.664 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.665 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 43.0\n",
      "2021-09-07 17:18:07.666 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 38.0\n",
      "2021-09-07 17:18:07.666 | INFO     | src.policies:collect_trajectories:221 - Episode 709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:07.678 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.678 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 62.0\n",
      "2021-09-07 17:18:07.679 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 46.0\n",
      "2021-09-07 17:18:07.679 | INFO     | src.policies:collect_trajectories:221 - Episode 710\n",
      "2021-09-07 17:18:07.692 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.692 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 64.0\n",
      "2021-09-07 17:18:07.693 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 50.5\n",
      "2021-09-07 17:18:07.693 | WARNING  | src.policies:train:144 - The actual batch size is 202, instead of 200\n",
      "2021-09-07 17:18:07.696 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:07.698 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5769041776657104, 'baseline_loss': 1.2368029356002808, 'total_loss': 0.04149729013442993}\n",
      "2021-09-07 17:18:07.699 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5646363496780396\n",
      "2021-09-07 17:18:07.701 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1305683851242065\n",
      "2021-09-07 17:18:07.702 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:07.703 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:07.705 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:07.706 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5008150339126587, 'baseline_loss': 1.2547508478164673, 'total_loss': 0.12656038999557495}\n",
      "2021-09-07 17:18:07.707 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3297598361968994\n",
      "2021-09-07 17:18:07.708 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.231196641921997\n",
      "2021-09-07 17:18:07.709 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3297598361968994\n",
      "2021-09-07 17:18:07.710 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:07.712 | INFO     | src.policies:train:123 - Epoch 110 / 800\n",
      "2021-09-07 17:18:07.712 | INFO     | src.policies:collect_trajectories:221 - Episode 711\n",
      "2021-09-07 17:18:07.720 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.721 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 40.0\n",
      "2021-09-07 17:18:07.721 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 40.0\n",
      "2021-09-07 17:18:07.722 | INFO     | src.policies:collect_trajectories:221 - Episode 712\n",
      "2021-09-07 17:18:07.742 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.743 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 123.0\n",
      "2021-09-07 17:18:07.743 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 81.5\n",
      "2021-09-07 17:18:07.744 | INFO     | src.policies:collect_trajectories:221 - Episode 713\n",
      "2021-09-07 17:18:07.764 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.765 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 123.0\n",
      "2021-09-07 17:18:07.765 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 95.33333333333333\n",
      "2021-09-07 17:18:07.766 | WARNING  | src.policies:train:144 - The actual batch size is 286, instead of 200\n",
      "2021-09-07 17:18:07.768 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:07.770 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5647128224372864, 'baseline_loss': 1.2806476354599, 'total_loss': 0.07561099529266357}\n",
      "2021-09-07 17:18:07.771 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3450176417827606\n",
      "2021-09-07 17:18:07.772 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9205157160758972\n",
      "2021-09-07 17:18:07.773 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3450176417827606\n",
      "2021-09-07 17:18:07.774 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:07.775 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:07.776 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.501251220703125, 'baseline_loss': 1.3104829788208008, 'total_loss': 0.1539902687072754}\n",
      "2021-09-07 17:18:07.777 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15737290680408478\n",
      "2021-09-07 17:18:07.778 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1173841953277588\n",
      "2021-09-07 17:18:07.779 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15737290680408478\n",
      "2021-09-07 17:18:07.780 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:07.782 | INFO     | src.policies:train:123 - Epoch 111 / 800\n",
      "2021-09-07 17:18:07.783 | INFO     | src.policies:collect_trajectories:221 - Episode 714\n",
      "2021-09-07 17:18:07.857 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.858 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 128.0\n",
      "2021-09-07 17:18:07.859 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 128.0\n",
      "2021-09-07 17:18:07.859 | INFO     | src.policies:collect_trajectories:221 - Episode 715\n",
      "2021-09-07 17:18:07.879 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.880 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 111.0\n",
      "2021-09-07 17:18:07.881 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 119.5\n",
      "2021-09-07 17:18:07.882 | WARNING  | src.policies:train:144 - The actual batch size is 239, instead of 200\n",
      "2021-09-07 17:18:07.885 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:07.886 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3901163935661316, 'baseline_loss': 1.0431804656982422, 'total_loss': 0.1314738392829895}\n",
      "2021-09-07 17:18:07.887 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19506196677684784\n",
      "2021-09-07 17:18:07.888 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3979523181915283\n",
      "2021-09-07 17:18:07.889 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19506196677684784\n",
      "2021-09-07 17:18:07.890 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3979523181915283\n",
      "2021-09-07 17:18:07.891 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:07.893 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5862972736358643, 'baseline_loss': 1.1407275199890137, 'total_loss': -0.015933513641357422}\n",
      "2021-09-07 17:18:07.894 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3960743546485901\n",
      "2021-09-07 17:18:07.894 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4419982433319092\n",
      "2021-09-07 17:18:07.895 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3960743546485901\n",
      "2021-09-07 17:18:07.897 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4419982433319092\n",
      "2021-09-07 17:18:07.898 | INFO     | src.policies:train:123 - Epoch 112 / 800\n",
      "2021-09-07 17:18:07.899 | INFO     | src.policies:collect_trajectories:221 - Episode 716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:07.904 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.904 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:07.905 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.0\n",
      "2021-09-07 17:18:07.905 | INFO     | src.policies:collect_trajectories:221 - Episode 717\n",
      "2021-09-07 17:18:07.923 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.923 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 99.0\n",
      "2021-09-07 17:18:07.924 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 59.5\n",
      "2021-09-07 17:18:07.924 | INFO     | src.policies:collect_trajectories:221 - Episode 718\n",
      "2021-09-07 17:18:07.943 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.944 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 108.0\n",
      "2021-09-07 17:18:07.944 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 75.66666666666667\n",
      "2021-09-07 17:18:07.945 | WARNING  | src.policies:train:144 - The actual batch size is 227, instead of 200\n",
      "2021-09-07 17:18:07.947 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:07.949 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5589632987976074, 'baseline_loss': 1.1708118915557861, 'total_loss': 0.026442646980285645}\n",
      "2021-09-07 17:18:07.950 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1563931405544281\n",
      "2021-09-07 17:18:07.951 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8465880155563354\n",
      "2021-09-07 17:18:07.952 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1563931405544281\n",
      "2021-09-07 17:18:07.954 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:07.955 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:07.956 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5465709567070007, 'baseline_loss': 1.3907486200332642, 'total_loss': 0.14880335330963135}\n",
      "2021-09-07 17:18:07.957 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14693817496299744\n",
      "2021-09-07 17:18:07.958 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0904964208602905\n",
      "2021-09-07 17:18:07.960 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14693817496299744\n",
      "2021-09-07 17:18:07.961 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:07.963 | INFO     | src.policies:train:123 - Epoch 113 / 800\n",
      "2021-09-07 17:18:07.964 | INFO     | src.policies:collect_trajectories:221 - Episode 719\n",
      "2021-09-07 17:18:07.974 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.975 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 63.0\n",
      "2021-09-07 17:18:07.976 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 63.0\n",
      "2021-09-07 17:18:07.976 | INFO     | src.policies:collect_trajectories:221 - Episode 720\n",
      "2021-09-07 17:18:07.986 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:07.987 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 44.0\n",
      "2021-09-07 17:18:07.987 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 53.5\n",
      "2021-09-07 17:18:07.988 | INFO     | src.policies:collect_trajectories:221 - Episode 721\n",
      "2021-09-07 17:18:08.003 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.004 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 84.0\n",
      "2021-09-07 17:18:08.004 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 63.666666666666664\n",
      "2021-09-07 17:18:08.005 | INFO     | src.policies:collect_trajectories:221 - Episode 722\n",
      "2021-09-07 17:18:08.037 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.038 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 191.0\n",
      "2021-09-07 17:18:08.039 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 95.5\n",
      "2021-09-07 17:18:08.039 | WARNING  | src.policies:train:144 - The actual batch size is 382, instead of 200\n",
      "2021-09-07 17:18:08.043 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:08.045 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45509952306747437, 'baseline_loss': 0.8946993350982666, 'total_loss': -0.0077498555183410645}\n",
      "2021-09-07 17:18:08.046 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3049534261226654\n",
      "2021-09-07 17:18:08.047 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.37369170784950256\n",
      "2021-09-07 17:18:08.048 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3049534261226654\n",
      "2021-09-07 17:18:08.049 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.37369170784950256\n",
      "2021-09-07 17:18:08.050 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:08.051 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3401261568069458, 'baseline_loss': 0.6374396085739136, 'total_loss': -0.021406352519989014}\n",
      "2021-09-07 17:18:08.052 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16321340203285217\n",
      "2021-09-07 17:18:08.053 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6112383604049683\n",
      "2021-09-07 17:18:08.054 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16321340203285217\n",
      "2021-09-07 17:18:08.055 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:08.056 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:08.057 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.39725205302238464, 'baseline_loss': 0.8376511931419373, 'total_loss': 0.021573543548583984}\n",
      "2021-09-07 17:18:08.058 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22621795535087585\n",
      "2021-09-07 17:18:08.060 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3204626142978668\n",
      "2021-09-07 17:18:08.062 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22621795535087585\n",
      "2021-09-07 17:18:08.063 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3204626142978668\n",
      "2021-09-07 17:18:08.064 | INFO     | src.policies:train:123 - Epoch 114 / 800\n",
      "2021-09-07 17:18:08.065 | INFO     | src.policies:collect_trajectories:221 - Episode 723\n",
      "2021-09-07 17:18:08.075 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.076 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 67.0\n",
      "2021-09-07 17:18:08.076 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 67.0\n",
      "2021-09-07 17:18:08.077 | INFO     | src.policies:collect_trajectories:221 - Episode 724\n",
      "2021-09-07 17:18:08.114 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.115 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:08.116 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 133.5\n",
      "2021-09-07 17:18:08.117 | WARNING  | src.policies:train:144 - The actual batch size is 267, instead of 200\n",
      "2021-09-07 17:18:08.119 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:08.123 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5424262285232544, 'baseline_loss': 1.2853182554244995, 'total_loss': 0.10023289918899536}\n",
      "2021-09-07 17:18:08.124 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.33427363634109497\n",
      "2021-09-07 17:18:08.126 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7276410460472107\n",
      "2021-09-07 17:18:08.127 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.33427363634109497\n",
      "2021-09-07 17:18:08.128 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:08.130 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:08.131 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.473753958940506, 'baseline_loss': 1.386712670326233, 'total_loss': 0.21960237622261047}\n",
      "2021-09-07 17:18:08.132 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3258618116378784\n",
      "2021-09-07 17:18:08.133 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9958272576332092\n",
      "2021-09-07 17:18:08.134 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3258618116378784\n",
      "2021-09-07 17:18:08.135 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:08.137 | INFO     | src.policies:train:123 - Epoch 115 / 800\n",
      "2021-09-07 17:18:08.138 | INFO     | src.policies:collect_trajectories:221 - Episode 725\n",
      "2021-09-07 17:18:08.149 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.150 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 61.0\n",
      "2021-09-07 17:18:08.150 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 61.0\n",
      "2021-09-07 17:18:08.151 | INFO     | src.policies:collect_trajectories:221 - Episode 726\n",
      "2021-09-07 17:18:08.171 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.171 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 111.0\n",
      "2021-09-07 17:18:08.172 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 86.0\n",
      "2021-09-07 17:18:08.172 | INFO     | src.policies:collect_trajectories:221 - Episode 727\n",
      "2021-09-07 17:18:08.180 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.181 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 40.0\n",
      "2021-09-07 17:18:08.181 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 70.66666666666667\n",
      "2021-09-07 17:18:08.182 | WARNING  | src.policies:train:144 - The actual batch size is 212, instead of 200\n",
      "2021-09-07 17:18:08.184 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:08.186 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.38039809465408325, 'baseline_loss': 0.9191316962242126, 'total_loss': 0.07916775345802307}\n",
      "2021-09-07 17:18:08.186 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2387392669916153\n",
      "2021-09-07 17:18:08.187 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3987564742565155\n",
      "2021-09-07 17:18:08.188 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2387392669916153\n",
      "2021-09-07 17:18:08.189 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3987564742565155\n",
      "2021-09-07 17:18:08.191 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:08.192 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.34526050090789795, 'baseline_loss': 0.7195467948913574, 'total_loss': 0.014512896537780762}\n",
      "2021-09-07 17:18:08.192 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23006515204906464\n",
      "2021-09-07 17:18:08.193 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8139607310295105\n",
      "2021-09-07 17:18:08.194 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23006515204906464\n",
      "2021-09-07 17:18:08.195 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:08.197 | INFO     | src.policies:train:123 - Epoch 116 / 800\n",
      "2021-09-07 17:18:08.197 | INFO     | src.policies:collect_trajectories:221 - Episode 728\n",
      "2021-09-07 17:18:08.209 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.209 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 70.0\n",
      "2021-09-07 17:18:08.210 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 70.0\n",
      "2021-09-07 17:18:08.210 | INFO     | src.policies:collect_trajectories:221 - Episode 729\n",
      "2021-09-07 17:18:08.227 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.228 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 103.0\n",
      "2021-09-07 17:18:08.228 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 86.5\n",
      "2021-09-07 17:18:08.229 | INFO     | src.policies:collect_trajectories:221 - Episode 730\n",
      "2021-09-07 17:18:08.256 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.257 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 134.0\n",
      "2021-09-07 17:18:08.257 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 102.33333333333333\n",
      "2021-09-07 17:18:08.258 | WARNING  | src.policies:train:144 - The actual batch size is 307, instead of 200\n",
      "2021-09-07 17:18:08.260 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:08.262 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4496692419052124, 'baseline_loss': 0.9461792707443237, 'total_loss': 0.023420393466949463}\n",
      "2021-09-07 17:18:08.263 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22269421815872192\n",
      "2021-09-07 17:18:08.264 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3411385715007782\n",
      "2021-09-07 17:18:08.265 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22269421815872192\n",
      "2021-09-07 17:18:08.267 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3411385715007782\n",
      "2021-09-07 17:18:08.268 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:08.269 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4003653824329376, 'baseline_loss': 0.8379284739494324, 'total_loss': 0.018598854541778564}\n",
      "2021-09-07 17:18:08.270 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.28235602378845215\n",
      "2021-09-07 17:18:08.271 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5333200097084045\n",
      "2021-09-07 17:18:08.272 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.28235602378845215\n",
      "2021-09-07 17:18:08.273 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:08.274 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:08.275 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42383354902267456, 'baseline_loss': 0.9584022760391235, 'total_loss': 0.05536758899688721}\n",
      "2021-09-07 17:18:08.276 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2150159478187561\n",
      "2021-09-07 17:18:08.277 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4783543348312378\n",
      "2021-09-07 17:18:08.278 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2150159478187561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:08.279 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4783543348312378\n",
      "2021-09-07 17:18:08.280 | INFO     | src.policies:train:123 - Epoch 117 / 800\n",
      "2021-09-07 17:18:08.281 | INFO     | src.policies:collect_trajectories:221 - Episode 731\n",
      "2021-09-07 17:18:08.296 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.298 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 108.0\n",
      "2021-09-07 17:18:08.299 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 108.0\n",
      "2021-09-07 17:18:08.300 | INFO     | src.policies:collect_trajectories:221 - Episode 732\n",
      "2021-09-07 17:18:08.315 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.316 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 93.0\n",
      "2021-09-07 17:18:08.316 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 100.5\n",
      "2021-09-07 17:18:08.317 | WARNING  | src.policies:train:144 - The actual batch size is 201, instead of 200\n",
      "2021-09-07 17:18:08.319 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:08.321 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.25892212986946106, 'baseline_loss': 0.6811479330062866, 'total_loss': 0.08165183663368225}\n",
      "2021-09-07 17:18:08.322 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12350556254386902\n",
      "2021-09-07 17:18:08.323 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5174704790115356\n",
      "2021-09-07 17:18:08.324 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12350556254386902\n",
      "2021-09-07 17:18:08.325 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:08.326 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:08.327 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.19124393165111542, 'baseline_loss': 0.6818588972091675, 'total_loss': 0.14968551695346832}\n",
      "2021-09-07 17:18:08.329 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09442789852619171\n",
      "2021-09-07 17:18:08.330 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3923091888427734\n",
      "2021-09-07 17:18:08.331 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09442789852619171\n",
      "2021-09-07 17:18:08.332 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:08.334 | INFO     | src.policies:train:123 - Epoch 118 / 800\n",
      "2021-09-07 17:18:08.334 | INFO     | src.policies:collect_trajectories:221 - Episode 733\n",
      "2021-09-07 17:18:08.486 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.486 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 98.0\n",
      "2021-09-07 17:18:08.487 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 98.0\n",
      "2021-09-07 17:18:08.487 | INFO     | src.policies:collect_trajectories:221 - Episode 734\n",
      "2021-09-07 17:18:08.505 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.506 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 92.0\n",
      "2021-09-07 17:18:08.506 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 95.0\n",
      "2021-09-07 17:18:08.507 | INFO     | src.policies:collect_trajectories:221 - Episode 735\n",
      "2021-09-07 17:18:08.532 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.532 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 131.0\n",
      "2021-09-07 17:18:08.533 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 107.0\n",
      "2021-09-07 17:18:08.533 | WARNING  | src.policies:train:144 - The actual batch size is 321, instead of 200\n",
      "2021-09-07 17:18:08.536 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:08.538 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3218750059604645, 'baseline_loss': 0.9095180630683899, 'total_loss': 0.13288402557373047}\n",
      "2021-09-07 17:18:08.538 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19504491984844208\n",
      "2021-09-07 17:18:08.539 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5569505095481873\n",
      "2021-09-07 17:18:08.540 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19504491984844208\n",
      "2021-09-07 17:18:08.541 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:08.543 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:08.544 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3208425045013428, 'baseline_loss': 0.7976546287536621, 'total_loss': 0.07798480987548828}\n",
      "2021-09-07 17:18:08.544 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18674959242343903\n",
      "2021-09-07 17:18:08.545 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7176163792610168\n",
      "2021-09-07 17:18:08.546 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18674959242343903\n",
      "2021-09-07 17:18:08.547 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:08.548 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:08.549 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.33084818720817566, 'baseline_loss': 1.023614525794983, 'total_loss': 0.1809590756893158}\n",
      "2021-09-07 17:18:08.550 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23847167193889618\n",
      "2021-09-07 17:18:08.551 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3804911673069\n",
      "2021-09-07 17:18:08.552 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23847167193889618\n",
      "2021-09-07 17:18:08.554 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3804911673069\n",
      "2021-09-07 17:18:08.555 | INFO     | src.policies:train:123 - Epoch 119 / 800\n",
      "2021-09-07 17:18:08.556 | INFO     | src.policies:collect_trajectories:221 - Episode 736\n",
      "2021-09-07 17:18:08.572 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.573 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 115.0\n",
      "2021-09-07 17:18:08.573 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 115.0\n",
      "2021-09-07 17:18:08.573 | INFO     | src.policies:collect_trajectories:221 - Episode 737\n",
      "2021-09-07 17:18:08.596 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.596 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 137.0\n",
      "2021-09-07 17:18:08.597 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 126.0\n",
      "2021-09-07 17:18:08.597 | WARNING  | src.policies:train:144 - The actual batch size is 252, instead of 200\n",
      "2021-09-07 17:18:08.600 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:08.603 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.16504131257534027, 'baseline_loss': 0.5496410131454468, 'total_loss': 0.10977919399738312}\n",
      "2021-09-07 17:18:08.605 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1258910894393921\n",
      "2021-09-07 17:18:08.606 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0197373628616333\n",
      "2021-09-07 17:18:08.607 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1258910894393921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:08.609 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:08.610 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:08.611 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2660945653915405, 'baseline_loss': 0.579770565032959, 'total_loss': 0.023790717124938965}\n",
      "2021-09-07 17:18:08.612 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1691228300333023\n",
      "2021-09-07 17:18:08.613 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.143545150756836\n",
      "2021-09-07 17:18:08.615 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1691228300333023\n",
      "2021-09-07 17:18:08.616 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:08.618 | INFO     | src.policies:train:123 - Epoch 120 / 800\n",
      "2021-09-07 17:18:08.618 | INFO     | src.policies:collect_trajectories:221 - Episode 738\n",
      "2021-09-07 17:18:08.638 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.639 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 141.0\n",
      "2021-09-07 17:18:08.639 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 141.0\n",
      "2021-09-07 17:18:08.640 | INFO     | src.policies:collect_trajectories:221 - Episode 739\n",
      "2021-09-07 17:18:08.662 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.663 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 149.0\n",
      "2021-09-07 17:18:08.663 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 145.0\n",
      "2021-09-07 17:18:08.664 | WARNING  | src.policies:train:144 - The actual batch size is 290, instead of 200\n",
      "2021-09-07 17:18:08.666 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:08.669 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5599415898323059, 'baseline_loss': 1.357627511024475, 'total_loss': 0.11887216567993164}\n",
      "2021-09-07 17:18:08.670 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21070969104766846\n",
      "2021-09-07 17:18:08.671 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7796643972396851\n",
      "2021-09-07 17:18:08.672 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21070969104766846\n",
      "2021-09-07 17:18:08.673 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:08.675 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:08.676 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5148307681083679, 'baseline_loss': 1.3724875450134277, 'total_loss': 0.17141300439834595}\n",
      "2021-09-07 17:18:08.677 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.33395498991012573\n",
      "2021-09-07 17:18:08.678 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.048503041267395\n",
      "2021-09-07 17:18:08.679 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.33395498991012573\n",
      "2021-09-07 17:18:08.680 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:08.682 | INFO     | src.policies:train:123 - Epoch 121 / 800\n",
      "2021-09-07 17:18:08.682 | INFO     | src.policies:collect_trajectories:221 - Episode 740\n",
      "2021-09-07 17:18:08.686 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.686 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:08.687 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.0\n",
      "2021-09-07 17:18:08.687 | INFO     | src.policies:collect_trajectories:221 - Episode 741\n",
      "2021-09-07 17:18:08.706 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.707 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 123.0\n",
      "2021-09-07 17:18:08.707 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 71.5\n",
      "2021-09-07 17:18:08.708 | INFO     | src.policies:collect_trajectories:221 - Episode 742\n",
      "2021-09-07 17:18:08.713 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.714 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 25.0\n",
      "2021-09-07 17:18:08.714 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 56.0\n",
      "2021-09-07 17:18:08.715 | INFO     | src.policies:collect_trajectories:221 - Episode 743\n",
      "2021-09-07 17:18:08.730 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.730 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 97.0\n",
      "2021-09-07 17:18:08.731 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 66.25\n",
      "2021-09-07 17:18:08.731 | WARNING  | src.policies:train:144 - The actual batch size is 265, instead of 200\n",
      "2021-09-07 17:18:08.734 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:08.736 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.518894612789154, 'baseline_loss': 1.0222450494766235, 'total_loss': -0.007772088050842285}\n",
      "2021-09-07 17:18:08.737 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18322822451591492\n",
      "2021-09-07 17:18:08.738 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.530992865562439\n",
      "2021-09-07 17:18:08.739 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18322822451591492\n",
      "2021-09-07 17:18:08.740 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:08.741 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:08.742 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5616639256477356, 'baseline_loss': 1.2025227546691895, 'total_loss': 0.03959745168685913}\n",
      "2021-09-07 17:18:08.743 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3453918695449829\n",
      "2021-09-07 17:18:08.744 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7617815136909485\n",
      "2021-09-07 17:18:08.745 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3453918695449829\n",
      "2021-09-07 17:18:08.746 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:08.747 | INFO     | src.policies:train:123 - Epoch 122 / 800\n",
      "2021-09-07 17:18:08.748 | INFO     | src.policies:collect_trajectories:221 - Episode 744\n",
      "2021-09-07 17:18:08.777 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.778 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:08.778 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:08.780 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:08.783 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.34708502888679504, 'baseline_loss': 0.5609834790229797, 'total_loss': -0.06659328937530518}\n",
      "2021-09-07 17:18:08.785 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1213567927479744\n",
      "2021-09-07 17:18:08.787 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6070742011070251\n",
      "2021-09-07 17:18:08.789 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1213567927479744\n",
      "2021-09-07 17:18:08.790 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:08.791 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:08.793 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.37853333353996277, 'baseline_loss': 0.6674444079399109, 'total_loss': -0.044811129570007324}\n",
      "2021-09-07 17:18:08.794 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12044239044189453\n",
      "2021-09-07 17:18:08.795 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4347134828567505\n",
      "2021-09-07 17:18:08.796 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12044239044189453\n",
      "2021-09-07 17:18:08.797 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4347134828567505\n",
      "2021-09-07 17:18:08.799 | INFO     | src.policies:train:123 - Epoch 123 / 800\n",
      "2021-09-07 17:18:08.800 | INFO     | src.policies:collect_trajectories:221 - Episode 745\n",
      "2021-09-07 17:18:08.807 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.808 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 33.0\n",
      "2021-09-07 17:18:08.808 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 33.0\n",
      "2021-09-07 17:18:08.809 | INFO     | src.policies:collect_trajectories:221 - Episode 746\n",
      "2021-09-07 17:18:08.831 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.832 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 125.0\n",
      "2021-09-07 17:18:08.832 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 79.0\n",
      "2021-09-07 17:18:08.833 | INFO     | src.policies:collect_trajectories:221 - Episode 747\n",
      "2021-09-07 17:18:08.857 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.858 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 147.0\n",
      "2021-09-07 17:18:08.858 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 101.66666666666667\n",
      "2021-09-07 17:18:08.859 | WARNING  | src.policies:train:144 - The actual batch size is 305, instead of 200\n",
      "2021-09-07 17:18:08.863 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:08.865 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3910579979419708, 'baseline_loss': 1.0574772357940674, 'total_loss': 0.13768061995506287}\n",
      "2021-09-07 17:18:08.866 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19205822050571442\n",
      "2021-09-07 17:18:08.867 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4764745235443115\n",
      "2021-09-07 17:18:08.869 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19205822050571442\n",
      "2021-09-07 17:18:08.870 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4764745235443115\n",
      "2021-09-07 17:18:08.871 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:08.872 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4440203607082367, 'baseline_loss': 1.2003463506698608, 'total_loss': 0.15615281462669373}\n",
      "2021-09-07 17:18:08.873 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3298150300979614\n",
      "2021-09-07 17:18:08.874 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6902483105659485\n",
      "2021-09-07 17:18:08.875 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3298150300979614\n",
      "2021-09-07 17:18:08.876 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:08.878 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:08.879 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3436249792575836, 'baseline_loss': 0.9903253316879272, 'total_loss': 0.15153768658638}\n",
      "2021-09-07 17:18:08.880 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1433122307062149\n",
      "2021-09-07 17:18:08.881 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.44250842928886414\n",
      "2021-09-07 17:18:08.882 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1433122307062149\n",
      "2021-09-07 17:18:08.883 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.44250842928886414\n",
      "2021-09-07 17:18:08.885 | INFO     | src.policies:train:123 - Epoch 124 / 800\n",
      "2021-09-07 17:18:08.885 | INFO     | src.policies:collect_trajectories:221 - Episode 748\n",
      "2021-09-07 17:18:08.916 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.917 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 193.0\n",
      "2021-09-07 17:18:08.918 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 193.0\n",
      "2021-09-07 17:18:08.919 | INFO     | src.policies:collect_trajectories:221 - Episode 749\n",
      "2021-09-07 17:18:08.933 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.934 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 76.0\n",
      "2021-09-07 17:18:08.934 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 134.5\n",
      "2021-09-07 17:18:08.935 | WARNING  | src.policies:train:144 - The actual batch size is 269, instead of 200\n",
      "2021-09-07 17:18:08.937 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:08.940 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5223464965820312, 'baseline_loss': 1.2433695793151855, 'total_loss': 0.09933829307556152}\n",
      "2021-09-07 17:18:08.940 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14066578447818756\n",
      "2021-09-07 17:18:08.942 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2511906623840332\n",
      "2021-09-07 17:18:08.943 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14066578447818756\n",
      "2021-09-07 17:18:08.944 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:08.945 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:08.946 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5210040211677551, 'baseline_loss': 1.1469279527664185, 'total_loss': 0.0524599552154541}\n",
      "2021-09-07 17:18:08.947 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17385004460811615\n",
      "2021-09-07 17:18:08.948 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.810030460357666\n",
      "2021-09-07 17:18:08.949 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17385004460811615\n",
      "2021-09-07 17:18:08.950 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:08.951 | INFO     | src.policies:train:123 - Epoch 125 / 800\n",
      "2021-09-07 17:18:08.952 | INFO     | src.policies:collect_trajectories:221 - Episode 750\n",
      "2021-09-07 17:18:08.958 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:08.958 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 33.0\n",
      "2021-09-07 17:18:08.959 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 33.0\n",
      "2021-09-07 17:18:08.959 | INFO     | src.policies:collect_trajectories:221 - Episode 751\n",
      "2021-09-07 17:18:09.030 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.031 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 81.0\n",
      "2021-09-07 17:18:09.031 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 57.0\n",
      "2021-09-07 17:18:09.031 | INFO     | src.policies:collect_trajectories:221 - Episode 752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:09.055 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.055 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 146.0\n",
      "2021-09-07 17:18:09.056 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 86.66666666666667\n",
      "2021-09-07 17:18:09.057 | WARNING  | src.policies:train:144 - The actual batch size is 260, instead of 200\n",
      "2021-09-07 17:18:09.060 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:09.061 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41632840037345886, 'baseline_loss': 0.7949144840240479, 'total_loss': -0.018871158361434937}\n",
      "2021-09-07 17:18:09.062 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21490542590618134\n",
      "2021-09-07 17:18:09.063 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.30727341771125793\n",
      "2021-09-07 17:18:09.065 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21490542590618134\n",
      "2021-09-07 17:18:09.067 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.30727341771125793\n",
      "2021-09-07 17:18:09.068 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:09.070 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5010277032852173, 'baseline_loss': 0.9833192229270935, 'total_loss': -0.009368091821670532}\n",
      "2021-09-07 17:18:09.071 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4014245867729187\n",
      "2021-09-07 17:18:09.072 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4394567310810089\n",
      "2021-09-07 17:18:09.073 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4014245867729187\n",
      "2021-09-07 17:18:09.074 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4394567310810089\n",
      "2021-09-07 17:18:09.075 | INFO     | src.policies:train:123 - Epoch 126 / 800\n",
      "2021-09-07 17:18:09.075 | INFO     | src.policies:collect_trajectories:221 - Episode 753\n",
      "2021-09-07 17:18:09.094 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.094 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 109.0\n",
      "2021-09-07 17:18:09.095 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 109.0\n",
      "2021-09-07 17:18:09.096 | INFO     | src.policies:collect_trajectories:221 - Episode 754\n",
      "2021-09-07 17:18:09.126 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.127 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 187.0\n",
      "2021-09-07 17:18:09.127 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 148.0\n",
      "2021-09-07 17:18:09.128 | WARNING  | src.policies:train:144 - The actual batch size is 296, instead of 200\n",
      "2021-09-07 17:18:09.130 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:09.132 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4216300845146179, 'baseline_loss': 0.9013655185699463, 'total_loss': 0.029052674770355225}\n",
      "2021-09-07 17:18:09.133 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2974168658256531\n",
      "2021-09-07 17:18:09.134 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7957936525344849\n",
      "2021-09-07 17:18:09.135 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2974168658256531\n",
      "2021-09-07 17:18:09.136 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:09.137 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:09.139 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3538132905960083, 'baseline_loss': 0.9581196308135986, 'total_loss': 0.12524652481079102}\n",
      "2021-09-07 17:18:09.140 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2933396100997925\n",
      "2021-09-07 17:18:09.140 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0462870597839355\n",
      "2021-09-07 17:18:09.142 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2933396100997925\n",
      "2021-09-07 17:18:09.143 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:09.145 | INFO     | src.policies:train:123 - Epoch 127 / 800\n",
      "2021-09-07 17:18:09.145 | INFO     | src.policies:collect_trajectories:221 - Episode 755\n",
      "2021-09-07 17:18:09.176 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.176 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:09.177 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:09.180 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:09.183 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5444219708442688, 'baseline_loss': 1.4070024490356445, 'total_loss': 0.15907925367355347}\n",
      "2021-09-07 17:18:09.184 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.28409096598625183\n",
      "2021-09-07 17:18:09.185 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0509339570999146\n",
      "2021-09-07 17:18:09.187 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.28409096598625183\n",
      "2021-09-07 17:18:09.188 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:09.190 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:09.191 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.604276716709137, 'baseline_loss': 1.7287498712539673, 'total_loss': 0.2600982189178467}\n",
      "2021-09-07 17:18:09.192 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17036348581314087\n",
      "2021-09-07 17:18:09.193 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3474570512771606\n",
      "2021-09-07 17:18:09.194 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17036348581314087\n",
      "2021-09-07 17:18:09.195 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:09.197 | INFO     | src.policies:train:123 - Epoch 128 / 800\n",
      "2021-09-07 17:18:09.197 | INFO     | src.policies:collect_trajectories:221 - Episode 756\n",
      "2021-09-07 17:18:09.214 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.214 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 106.0\n",
      "2021-09-07 17:18:09.215 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 106.0\n",
      "2021-09-07 17:18:09.215 | INFO     | src.policies:collect_trajectories:221 - Episode 757\n",
      "2021-09-07 17:18:09.248 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.248 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 197.0\n",
      "2021-09-07 17:18:09.249 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 151.5\n",
      "2021-09-07 17:18:09.250 | WARNING  | src.policies:train:144 - The actual batch size is 303, instead of 200\n",
      "2021-09-07 17:18:09.252 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:09.254 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5700523257255554, 'baseline_loss': 1.0861587524414062, 'total_loss': -0.026972949504852295}\n",
      "2021-09-07 17:18:09.255 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4362315833568573\n",
      "2021-09-07 17:18:09.256 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.764154314994812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:09.257 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4362315833568573\n",
      "2021-09-07 17:18:09.258 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:09.260 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:09.261 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4691557288169861, 'baseline_loss': 1.071799874305725, 'total_loss': 0.06674420833587646}\n",
      "2021-09-07 17:18:09.262 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2041151374578476\n",
      "2021-09-07 17:18:09.263 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9188663959503174\n",
      "2021-09-07 17:18:09.264 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2041151374578476\n",
      "2021-09-07 17:18:09.265 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:09.267 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:09.269 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5446629524230957, 'baseline_loss': 1.0998362302780151, 'total_loss': 0.005255162715911865}\n",
      "2021-09-07 17:18:09.270 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1326819509267807\n",
      "2021-09-07 17:18:09.271 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7477616667747498\n",
      "2021-09-07 17:18:09.273 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1326819509267807\n",
      "2021-09-07 17:18:09.274 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:09.275 | INFO     | src.policies:train:123 - Epoch 129 / 800\n",
      "2021-09-07 17:18:09.276 | INFO     | src.policies:collect_trajectories:221 - Episode 758\n",
      "2021-09-07 17:18:09.293 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.294 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 111.0\n",
      "2021-09-07 17:18:09.295 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 111.0\n",
      "2021-09-07 17:18:09.295 | INFO     | src.policies:collect_trajectories:221 - Episode 759\n",
      "2021-09-07 17:18:09.326 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.327 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:09.328 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 155.5\n",
      "2021-09-07 17:18:09.328 | WARNING  | src.policies:train:144 - The actual batch size is 311, instead of 200\n",
      "2021-09-07 17:18:09.332 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:09.334 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3323846459388733, 'baseline_loss': 0.6166794300079346, 'total_loss': -0.024044930934906006}\n",
      "2021-09-07 17:18:09.335 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10811769962310791\n",
      "2021-09-07 17:18:09.336 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.755794107913971\n",
      "2021-09-07 17:18:09.337 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10811769962310791\n",
      "2021-09-07 17:18:09.338 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:09.339 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:09.340 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.26861581206321716, 'baseline_loss': 0.6456625461578369, 'total_loss': 0.054215461015701294}\n",
      "2021-09-07 17:18:09.341 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18209199607372284\n",
      "2021-09-07 17:18:09.343 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8184533715248108\n",
      "2021-09-07 17:18:09.345 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18209199607372284\n",
      "2021-09-07 17:18:09.347 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:09.349 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:09.350 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3581182062625885, 'baseline_loss': 0.7894301414489746, 'total_loss': 0.036596864461898804}\n",
      "2021-09-07 17:18:09.352 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.179351806640625\n",
      "2021-09-07 17:18:09.353 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7175412774085999\n",
      "2021-09-07 17:18:09.354 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.179351806640625\n",
      "2021-09-07 17:18:09.355 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:09.357 | INFO     | src.policies:train:123 - Epoch 130 / 800\n",
      "2021-09-07 17:18:09.357 | INFO     | src.policies:collect_trajectories:221 - Episode 760\n",
      "2021-09-07 17:18:09.370 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.371 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 80.0\n",
      "2021-09-07 17:18:09.371 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 80.0\n",
      "2021-09-07 17:18:09.372 | INFO     | src.policies:collect_trajectories:221 - Episode 761\n",
      "2021-09-07 17:18:09.396 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.397 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 139.0\n",
      "2021-09-07 17:18:09.397 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 109.5\n",
      "2021-09-07 17:18:09.398 | WARNING  | src.policies:train:144 - The actual batch size is 219, instead of 200\n",
      "2021-09-07 17:18:09.400 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:09.404 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.25478705763816833, 'baseline_loss': 0.48094844818115234, 'total_loss': -0.014312833547592163}\n",
      "2021-09-07 17:18:09.405 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22676774859428406\n",
      "2021-09-07 17:18:09.406 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0043983459472656\n",
      "2021-09-07 17:18:09.407 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22676774859428406\n",
      "2021-09-07 17:18:09.408 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:09.409 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:09.410 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2688782215118408, 'baseline_loss': 0.49186480045318604, 'total_loss': -0.022945821285247803}\n",
      "2021-09-07 17:18:09.411 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3689507842063904\n",
      "2021-09-07 17:18:09.412 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0219990015029907\n",
      "2021-09-07 17:18:09.413 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3689507842063904\n",
      "2021-09-07 17:18:09.414 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:09.416 | INFO     | src.policies:train:123 - Epoch 131 / 800\n",
      "2021-09-07 17:18:09.416 | INFO     | src.policies:collect_trajectories:221 - Episode 762\n",
      "2021-09-07 17:18:09.430 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:09.431 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 95.0\n",
      "2021-09-07 17:18:09.431 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 95.0\n",
      "2021-09-07 17:18:09.432 | INFO     | src.policies:collect_trajectories:221 - Episode 763\n",
      "2021-09-07 17:18:09.453 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.454 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 113.0\n",
      "2021-09-07 17:18:09.455 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 104.0\n",
      "2021-09-07 17:18:09.456 | WARNING  | src.policies:train:144 - The actual batch size is 208, instead of 200\n",
      "2021-09-07 17:18:09.458 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:09.461 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3032114803791046, 'baseline_loss': 0.4971197545528412, 'total_loss': -0.05465160310268402}\n",
      "2021-09-07 17:18:09.462 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20543482899665833\n",
      "2021-09-07 17:18:09.463 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9587584733963013\n",
      "2021-09-07 17:18:09.464 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20543482899665833\n",
      "2021-09-07 17:18:09.465 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:09.467 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:09.468 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2883758842945099, 'baseline_loss': 0.4920782148838043, 'total_loss': -0.04233677685260773}\n",
      "2021-09-07 17:18:09.469 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1264766901731491\n",
      "2021-09-07 17:18:09.470 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.244206190109253\n",
      "2021-09-07 17:18:09.471 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1264766901731491\n",
      "2021-09-07 17:18:09.472 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:09.474 | INFO     | src.policies:train:123 - Epoch 132 / 800\n",
      "2021-09-07 17:18:09.474 | INFO     | src.policies:collect_trajectories:221 - Episode 764\n",
      "2021-09-07 17:18:09.503 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.503 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 166.0\n",
      "2021-09-07 17:18:09.504 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 166.0\n",
      "2021-09-07 17:18:09.505 | INFO     | src.policies:collect_trajectories:221 - Episode 765\n",
      "2021-09-07 17:18:09.519 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.520 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 68.0\n",
      "2021-09-07 17:18:09.520 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 117.0\n",
      "2021-09-07 17:18:09.521 | WARNING  | src.policies:train:144 - The actual batch size is 234, instead of 200\n",
      "2021-09-07 17:18:09.524 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:09.526 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3790622055530548, 'baseline_loss': 0.7301904559135437, 'total_loss': -0.013966977596282959}\n",
      "2021-09-07 17:18:09.527 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2392149418592453\n",
      "2021-09-07 17:18:09.528 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8552700281143188\n",
      "2021-09-07 17:18:09.530 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2392149418592453\n",
      "2021-09-07 17:18:09.531 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:09.532 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:09.533 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.28593701124191284, 'baseline_loss': 0.6412928104400635, 'total_loss': 0.034709393978118896}\n",
      "2021-09-07 17:18:09.714 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17558084428310394\n",
      "2021-09-07 17:18:09.715 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.906915009021759\n",
      "2021-09-07 17:18:09.717 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17558084428310394\n",
      "2021-09-07 17:18:09.718 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:09.720 | INFO     | src.policies:train:123 - Epoch 133 / 800\n",
      "2021-09-07 17:18:09.720 | INFO     | src.policies:collect_trajectories:221 - Episode 766\n",
      "2021-09-07 17:18:09.728 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.729 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 39.0\n",
      "2021-09-07 17:18:09.729 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 39.0\n",
      "2021-09-07 17:18:09.730 | INFO     | src.policies:collect_trajectories:221 - Episode 767\n",
      "2021-09-07 17:18:09.747 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.748 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 95.0\n",
      "2021-09-07 17:18:09.748 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 67.0\n",
      "2021-09-07 17:18:09.749 | INFO     | src.policies:collect_trajectories:221 - Episode 768\n",
      "2021-09-07 17:18:09.782 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.782 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:09.783 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 111.33333333333333\n",
      "2021-09-07 17:18:09.784 | WARNING  | src.policies:train:144 - The actual batch size is 334, instead of 200\n",
      "2021-09-07 17:18:09.788 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:09.790 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4276016354560852, 'baseline_loss': 0.9304096102714539, 'total_loss': 0.037603169679641724}\n",
      "2021-09-07 17:18:09.791 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10105112195014954\n",
      "2021-09-07 17:18:09.791 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.410191148519516\n",
      "2021-09-07 17:18:09.793 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10105112195014954\n",
      "2021-09-07 17:18:09.794 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.410191148519516\n",
      "2021-09-07 17:18:09.795 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:09.796 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.46994224190711975, 'baseline_loss': 1.060682773590088, 'total_loss': 0.060399144887924194}\n",
      "2021-09-07 17:18:09.797 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23657160997390747\n",
      "2021-09-07 17:18:09.799 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3882727324962616\n",
      "2021-09-07 17:18:09.801 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23657160997390747\n",
      "2021-09-07 17:18:09.803 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3882727324962616\n",
      "2021-09-07 17:18:09.804 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:09.805 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4736103117465973, 'baseline_loss': 1.0448375940322876, 'total_loss': 0.04880848526954651}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:09.806 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19782021641731262\n",
      "2021-09-07 17:18:09.807 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.519811749458313\n",
      "2021-09-07 17:18:09.808 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19782021641731262\n",
      "2021-09-07 17:18:09.810 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999989867210388\n",
      "2021-09-07 17:18:09.811 | INFO     | src.policies:train:123 - Epoch 134 / 800\n",
      "2021-09-07 17:18:09.812 | INFO     | src.policies:collect_trajectories:221 - Episode 769\n",
      "2021-09-07 17:18:09.822 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.822 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 54.0\n",
      "2021-09-07 17:18:09.823 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 54.0\n",
      "2021-09-07 17:18:09.823 | INFO     | src.policies:collect_trajectories:221 - Episode 770\n",
      "2021-09-07 17:18:09.839 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.840 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 83.0\n",
      "2021-09-07 17:18:09.840 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 68.5\n",
      "2021-09-07 17:18:09.840 | INFO     | src.policies:collect_trajectories:221 - Episode 771\n",
      "2021-09-07 17:18:09.847 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.848 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 27.0\n",
      "2021-09-07 17:18:09.849 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 54.666666666666664\n",
      "2021-09-07 17:18:09.849 | INFO     | src.policies:collect_trajectories:221 - Episode 772\n",
      "2021-09-07 17:18:09.877 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.878 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 168.0\n",
      "2021-09-07 17:18:09.878 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 83.0\n",
      "2021-09-07 17:18:09.879 | WARNING  | src.policies:train:144 - The actual batch size is 332, instead of 200\n",
      "2021-09-07 17:18:09.882 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:09.884 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32821550965309143, 'baseline_loss': 0.7889757752418518, 'total_loss': 0.06627237796783447}\n",
      "2021-09-07 17:18:09.885 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13491122424602509\n",
      "2021-09-07 17:18:09.886 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3855443298816681\n",
      "2021-09-07 17:18:09.887 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13491122424602509\n",
      "2021-09-07 17:18:09.888 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3855443298816681\n",
      "2021-09-07 17:18:09.890 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:09.891 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3867246210575104, 'baseline_loss': 0.7778205871582031, 'total_loss': 0.0021856725215911865}\n",
      "2021-09-07 17:18:09.892 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16661782562732697\n",
      "2021-09-07 17:18:09.892 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5985366106033325\n",
      "2021-09-07 17:18:09.893 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16661782562732697\n",
      "2021-09-07 17:18:09.894 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:09.896 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:09.897 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36971673369407654, 'baseline_loss': 0.7935681343078613, 'total_loss': 0.027067333459854126}\n",
      "2021-09-07 17:18:09.898 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.26020047068595886\n",
      "2021-09-07 17:18:09.899 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9195852279663086\n",
      "2021-09-07 17:18:09.900 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.26020047068595886\n",
      "2021-09-07 17:18:09.901 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:09.902 | INFO     | src.policies:train:123 - Epoch 135 / 800\n",
      "2021-09-07 17:18:09.903 | INFO     | src.policies:collect_trajectories:221 - Episode 773\n",
      "2021-09-07 17:18:09.909 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.909 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 33.0\n",
      "2021-09-07 17:18:09.910 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 33.0\n",
      "2021-09-07 17:18:09.910 | INFO     | src.policies:collect_trajectories:221 - Episode 774\n",
      "2021-09-07 17:18:09.918 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.918 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 35.0\n",
      "2021-09-07 17:18:09.918 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 34.0\n",
      "2021-09-07 17:18:09.919 | INFO     | src.policies:collect_trajectories:221 - Episode 775\n",
      "2021-09-07 17:18:09.934 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.935 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 81.0\n",
      "2021-09-07 17:18:09.936 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 49.666666666666664\n",
      "2021-09-07 17:18:09.936 | INFO     | src.policies:collect_trajectories:221 - Episode 776\n",
      "2021-09-07 17:18:09.954 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.954 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 102.0\n",
      "2021-09-07 17:18:09.955 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 62.75\n",
      "2021-09-07 17:18:09.955 | WARNING  | src.policies:train:144 - The actual batch size is 251, instead of 200\n",
      "2021-09-07 17:18:09.958 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:09.960 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4139236807823181, 'baseline_loss': 1.2034611701965332, 'total_loss': 0.1878069043159485}\n",
      "2021-09-07 17:18:09.961 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19619576632976532\n",
      "2021-09-07 17:18:09.962 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6244538426399231\n",
      "2021-09-07 17:18:09.963 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19619576632976532\n",
      "2021-09-07 17:18:09.964 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:09.965 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:09.966 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5870524048805237, 'baseline_loss': 1.2732436656951904, 'total_loss': 0.04956942796707153}\n",
      "2021-09-07 17:18:09.967 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.39429742097854614\n",
      "2021-09-07 17:18:09.967 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6557491421699524\n",
      "2021-09-07 17:18:09.968 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.39429742097854614\n",
      "2021-09-07 17:18:09.969 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:09.971 | INFO     | src.policies:train:123 - Epoch 136 / 800\n",
      "2021-09-07 17:18:09.971 | INFO     | src.policies:collect_trajectories:221 - Episode 777\n",
      "2021-09-07 17:18:09.985 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.986 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 87.0\n",
      "2021-09-07 17:18:09.986 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 87.0\n",
      "2021-09-07 17:18:09.987 | INFO     | src.policies:collect_trajectories:221 - Episode 778\n",
      "2021-09-07 17:18:09.994 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:09.994 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 28.0\n",
      "2021-09-07 17:18:09.995 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 57.5\n",
      "2021-09-07 17:18:09.995 | INFO     | src.policies:collect_trajectories:221 - Episode 779\n",
      "2021-09-07 17:18:10.005 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.005 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 54.0\n",
      "2021-09-07 17:18:10.006 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 56.333333333333336\n",
      "2021-09-07 17:18:10.006 | INFO     | src.policies:collect_trajectories:221 - Episode 780\n",
      "2021-09-07 17:18:10.020 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.021 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 89.0\n",
      "2021-09-07 17:18:10.021 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 64.5\n",
      "2021-09-07 17:18:10.022 | WARNING  | src.policies:train:144 - The actual batch size is 258, instead of 200\n",
      "2021-09-07 17:18:10.024 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:10.026 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6790799498558044, 'baseline_loss': 1.5282090902328491, 'total_loss': 0.08502459526062012}\n",
      "2021-09-07 17:18:10.027 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6447885632514954\n",
      "2021-09-07 17:18:10.028 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8680872321128845\n",
      "2021-09-07 17:18:10.029 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:10.030 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:10.031 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:10.032 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6072237491607666, 'baseline_loss': 1.3502976894378662, 'total_loss': 0.0679250955581665}\n",
      "2021-09-07 17:18:10.033 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19667620956897736\n",
      "2021-09-07 17:18:10.035 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8249956965446472\n",
      "2021-09-07 17:18:10.036 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19667620956897736\n",
      "2021-09-07 17:18:10.037 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:10.039 | INFO     | src.policies:train:123 - Epoch 137 / 800\n",
      "2021-09-07 17:18:10.039 | INFO     | src.policies:collect_trajectories:221 - Episode 781\n",
      "2021-09-07 17:18:10.052 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.052 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 80.0\n",
      "2021-09-07 17:18:10.053 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 80.0\n",
      "2021-09-07 17:18:10.053 | INFO     | src.policies:collect_trajectories:221 - Episode 782\n",
      "2021-09-07 17:18:10.083 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.083 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 181.0\n",
      "2021-09-07 17:18:10.084 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 130.5\n",
      "2021-09-07 17:18:10.084 | WARNING  | src.policies:train:144 - The actual batch size is 261, instead of 200\n",
      "2021-09-07 17:18:10.087 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:10.089 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6303738951683044, 'baseline_loss': 1.8427565097808838, 'total_loss': 0.29100435972213745}\n",
      "2021-09-07 17:18:10.090 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.45613014698028564\n",
      "2021-09-07 17:18:10.091 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4933762550354004\n",
      "2021-09-07 17:18:10.092 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.45613014698028564\n",
      "2021-09-07 17:18:10.093 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:10.094 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:10.095 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6696690917015076, 'baseline_loss': 1.8482294082641602, 'total_loss': 0.2544456124305725}\n",
      "2021-09-07 17:18:10.096 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.37967145442962646\n",
      "2021-09-07 17:18:10.097 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8813782930374146\n",
      "2021-09-07 17:18:10.098 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.37967145442962646\n",
      "2021-09-07 17:18:10.099 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:10.101 | INFO     | src.policies:train:123 - Epoch 138 / 800\n",
      "2021-09-07 17:18:10.102 | INFO     | src.policies:collect_trajectories:221 - Episode 783\n",
      "2021-09-07 17:18:10.118 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.118 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 99.0\n",
      "2021-09-07 17:18:10.119 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 99.0\n",
      "2021-09-07 17:18:10.119 | INFO     | src.policies:collect_trajectories:221 - Episode 784\n",
      "2021-09-07 17:18:10.152 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.153 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:10.153 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 149.5\n",
      "2021-09-07 17:18:10.154 | WARNING  | src.policies:train:144 - The actual batch size is 299, instead of 200\n",
      "2021-09-07 17:18:10.156 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:10.159 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4359382390975952, 'baseline_loss': 1.2670352458953857, 'total_loss': 0.19757938385009766}\n",
      "2021-09-07 17:18:10.160 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.34296274185180664\n",
      "2021-09-07 17:18:10.161 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6526103615760803\n",
      "2021-09-07 17:18:10.163 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.34296274185180664\n",
      "2021-09-07 17:18:10.164 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:10.165 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:10.166 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31958889961242676, 'baseline_loss': 1.1870781183242798, 'total_loss': 0.27395015954971313}\n",
      "2021-09-07 17:18:10.167 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2238411158323288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:10.168 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8260717988014221\n",
      "2021-09-07 17:18:10.170 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2238411158323288\n",
      "2021-09-07 17:18:10.172 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:10.173 | INFO     | src.policies:train:123 - Epoch 139 / 800\n",
      "2021-09-07 17:18:10.173 | INFO     | src.policies:collect_trajectories:221 - Episode 785\n",
      "2021-09-07 17:18:10.184 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.184 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 62.0\n",
      "2021-09-07 17:18:10.185 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 62.0\n",
      "2021-09-07 17:18:10.185 | INFO     | src.policies:collect_trajectories:221 - Episode 786\n",
      "2021-09-07 17:18:10.202 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.202 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 99.0\n",
      "2021-09-07 17:18:10.203 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 80.5\n",
      "2021-09-07 17:18:10.203 | INFO     | src.policies:collect_trajectories:221 - Episode 787\n",
      "2021-09-07 17:18:10.218 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.276 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 81.0\n",
      "2021-09-07 17:18:10.277 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 80.66666666666667\n",
      "2021-09-07 17:18:10.278 | WARNING  | src.policies:train:144 - The actual batch size is 242, instead of 200\n",
      "2021-09-07 17:18:10.281 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:10.283 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4439293146133423, 'baseline_loss': 0.9024614691734314, 'total_loss': 0.007301419973373413}\n",
      "2021-09-07 17:18:10.284 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4751635193824768\n",
      "2021-09-07 17:18:10.285 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8706191182136536\n",
      "2021-09-07 17:18:10.286 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4751635193824768\n",
      "2021-09-07 17:18:10.287 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:10.288 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:10.289 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43935146927833557, 'baseline_loss': 0.8168326616287231, 'total_loss': -0.030935138463974}\n",
      "2021-09-07 17:18:10.290 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19302472472190857\n",
      "2021-09-07 17:18:10.291 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8508170247077942\n",
      "2021-09-07 17:18:10.293 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19302472472190857\n",
      "2021-09-07 17:18:10.294 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:10.295 | INFO     | src.policies:train:123 - Epoch 140 / 800\n",
      "2021-09-07 17:18:10.296 | INFO     | src.policies:collect_trajectories:221 - Episode 788\n",
      "2021-09-07 17:18:10.324 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.324 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 174.0\n",
      "2021-09-07 17:18:10.325 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 174.0\n",
      "2021-09-07 17:18:10.326 | INFO     | src.policies:collect_trajectories:221 - Episode 789\n",
      "2021-09-07 17:18:10.330 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.331 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 15.0\n",
      "2021-09-07 17:18:10.331 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 94.5\n",
      "2021-09-07 17:18:10.332 | INFO     | src.policies:collect_trajectories:221 - Episode 790\n",
      "2021-09-07 17:18:10.352 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.353 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 136.0\n",
      "2021-09-07 17:18:10.354 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 108.33333333333333\n",
      "2021-09-07 17:18:10.354 | WARNING  | src.policies:train:144 - The actual batch size is 325, instead of 200\n",
      "2021-09-07 17:18:10.357 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:10.358 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5740148425102234, 'baseline_loss': 1.3835864067077637, 'total_loss': 0.11777836084365845}\n",
      "2021-09-07 17:18:10.359 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24069635570049286\n",
      "2021-09-07 17:18:10.361 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.077816367149353\n",
      "2021-09-07 17:18:10.363 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24069635570049286\n",
      "2021-09-07 17:18:10.364 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:10.365 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:10.367 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6049249172210693, 'baseline_loss': 1.4649810791015625, 'total_loss': 0.12756562232971191}\n",
      "2021-09-07 17:18:10.368 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4803895652294159\n",
      "2021-09-07 17:18:10.368 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1442852020263672\n",
      "2021-09-07 17:18:10.369 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4803895652294159\n",
      "2021-09-07 17:18:10.371 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:10.372 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:10.374 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6312645673751831, 'baseline_loss': 1.5788986682891846, 'total_loss': 0.15818476676940918}\n",
      "2021-09-07 17:18:10.374 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24377362430095673\n",
      "2021-09-07 17:18:10.375 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5111076831817627\n",
      "2021-09-07 17:18:10.377 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24377362430095673\n",
      "2021-09-07 17:18:10.379 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:10.381 | INFO     | src.policies:train:123 - Epoch 141 / 800\n",
      "2021-09-07 17:18:10.382 | INFO     | src.policies:collect_trajectories:221 - Episode 791\n",
      "2021-09-07 17:18:10.402 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.403 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 129.0\n",
      "2021-09-07 17:18:10.403 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 129.0\n",
      "2021-09-07 17:18:10.404 | INFO     | src.policies:collect_trajectories:221 - Episode 792\n",
      "2021-09-07 17:18:10.425 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.426 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 114.0\n",
      "2021-09-07 17:18:10.427 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 121.5\n",
      "2021-09-07 17:18:10.428 | WARNING  | src.policies:train:144 - The actual batch size is 243, instead of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:10.430 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:10.431 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27866441011428833, 'baseline_loss': 0.4892826974391937, 'total_loss': -0.03402306139469147}\n",
      "2021-09-07 17:18:10.432 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13048571348190308\n",
      "2021-09-07 17:18:10.433 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9919720888137817\n",
      "2021-09-07 17:18:10.434 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13048571348190308\n",
      "2021-09-07 17:18:10.435 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:10.436 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:10.437 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2058754861354828, 'baseline_loss': 0.42670586705207825, 'total_loss': 0.0074774473905563354}\n",
      "2021-09-07 17:18:10.439 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16224972903728485\n",
      "2021-09-07 17:18:10.440 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2149536609649658\n",
      "2021-09-07 17:18:10.441 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16224972903728485\n",
      "2021-09-07 17:18:10.442 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:10.444 | INFO     | src.policies:train:123 - Epoch 142 / 800\n",
      "2021-09-07 17:18:10.444 | INFO     | src.policies:collect_trajectories:221 - Episode 793\n",
      "2021-09-07 17:18:10.454 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.455 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 57.0\n",
      "2021-09-07 17:18:10.455 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 57.0\n",
      "2021-09-07 17:18:10.456 | INFO     | src.policies:collect_trajectories:221 - Episode 794\n",
      "2021-09-07 17:18:10.480 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.481 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 152.0\n",
      "2021-09-07 17:18:10.481 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 104.5\n",
      "2021-09-07 17:18:10.482 | WARNING  | src.policies:train:144 - The actual batch size is 209, instead of 200\n",
      "2021-09-07 17:18:10.484 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:10.487 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29157784581184387, 'baseline_loss': 0.7363345623016357, 'total_loss': 0.076589435338974}\n",
      "2021-09-07 17:18:10.488 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1085730493068695\n",
      "2021-09-07 17:18:10.489 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0545979738235474\n",
      "2021-09-07 17:18:10.491 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1085730493068695\n",
      "2021-09-07 17:18:10.492 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:10.493 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:10.495 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4106893837451935, 'baseline_loss': 0.7491324543952942, 'total_loss': -0.03612315654754639}\n",
      "2021-09-07 17:18:10.496 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25480714440345764\n",
      "2021-09-07 17:18:10.497 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0073254108428955\n",
      "2021-09-07 17:18:10.498 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25480714440345764\n",
      "2021-09-07 17:18:10.499 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:10.500 | INFO     | src.policies:train:123 - Epoch 143 / 800\n",
      "2021-09-07 17:18:10.501 | INFO     | src.policies:collect_trajectories:221 - Episode 795\n",
      "2021-09-07 17:18:10.514 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.515 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 82.0\n",
      "2021-09-07 17:18:10.515 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 82.0\n",
      "2021-09-07 17:18:10.516 | INFO     | src.policies:collect_trajectories:221 - Episode 796\n",
      "2021-09-07 17:18:10.549 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.551 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:10.551 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 141.0\n",
      "2021-09-07 17:18:10.552 | WARNING  | src.policies:train:144 - The actual batch size is 282, instead of 200\n",
      "2021-09-07 17:18:10.554 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:10.557 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24170318245887756, 'baseline_loss': 0.5382716059684753, 'total_loss': 0.027432620525360107}\n",
      "2021-09-07 17:18:10.558 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2943272292613983\n",
      "2021-09-07 17:18:10.558 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2817296981811523\n",
      "2021-09-07 17:18:10.560 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2943272292613983\n",
      "2021-09-07 17:18:10.561 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:10.562 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:10.564 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.16821716725826263, 'baseline_loss': 0.510738730430603, 'total_loss': 0.08715219795703888}\n",
      "2021-09-07 17:18:10.565 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14683596789836884\n",
      "2021-09-07 17:18:10.567 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2655245065689087\n",
      "2021-09-07 17:18:10.568 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14683596789836884\n",
      "2021-09-07 17:18:10.570 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:10.571 | INFO     | src.policies:train:123 - Epoch 144 / 800\n",
      "2021-09-07 17:18:10.572 | INFO     | src.policies:collect_trajectories:221 - Episode 797\n",
      "2021-09-07 17:18:10.601 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.601 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 178.0\n",
      "2021-09-07 17:18:10.602 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 178.0\n",
      "2021-09-07 17:18:10.602 | INFO     | src.policies:collect_trajectories:221 - Episode 798\n",
      "2021-09-07 17:18:10.615 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.616 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 54.0\n",
      "2021-09-07 17:18:10.616 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 116.0\n",
      "2021-09-07 17:18:10.617 | WARNING  | src.policies:train:144 - The actual batch size is 232, instead of 200\n",
      "2021-09-07 17:18:10.619 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:10.621 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2994486391544342, 'baseline_loss': 0.9961645603179932, 'total_loss': 0.19863364100456238}\n",
      "2021-09-07 17:18:10.622 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16904424130916595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:10.623 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7120163440704346\n",
      "2021-09-07 17:18:10.624 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16904424130916595\n",
      "2021-09-07 17:18:10.625 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:10.626 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:10.627 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.25482800602912903, 'baseline_loss': 0.7666146159172058, 'total_loss': 0.12847930192947388}\n",
      "2021-09-07 17:18:10.628 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19430610537528992\n",
      "2021-09-07 17:18:10.629 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7946175932884216\n",
      "2021-09-07 17:18:10.631 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19430610537528992\n",
      "2021-09-07 17:18:10.632 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:10.633 | INFO     | src.policies:train:123 - Epoch 145 / 800\n",
      "2021-09-07 17:18:10.634 | INFO     | src.policies:collect_trajectories:221 - Episode 799\n",
      "2021-09-07 17:18:10.666 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.667 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:10.667 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:10.669 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:10.671 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45910361409187317, 'baseline_loss': 1.3629651069641113, 'total_loss': 0.2223789393901825}\n",
      "2021-09-07 17:18:10.672 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15230366587638855\n",
      "2021-09-07 17:18:10.673 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1265658140182495\n",
      "2021-09-07 17:18:10.674 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15230366587638855\n",
      "2021-09-07 17:18:10.675 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:10.677 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:10.678 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4792766273021698, 'baseline_loss': 1.3082574605941772, 'total_loss': 0.17485210299491882}\n",
      "2021-09-07 17:18:10.679 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.291074275970459\n",
      "2021-09-07 17:18:10.680 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9362279772758484\n",
      "2021-09-07 17:18:10.681 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.291074275970459\n",
      "2021-09-07 17:18:10.682 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:10.684 | INFO     | src.policies:train:123 - Epoch 146 / 800\n",
      "2021-09-07 17:18:10.684 | INFO     | src.policies:collect_trajectories:221 - Episode 800\n",
      "2021-09-07 17:18:10.707 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.708 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 128.0\n",
      "2021-09-07 17:18:10.709 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 128.0\n",
      "2021-09-07 17:18:10.709 | INFO     | src.policies:collect_trajectories:221 - Episode 801\n",
      "2021-09-07 17:18:10.716 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.716 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 31.0\n",
      "2021-09-07 17:18:10.716 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 79.5\n",
      "2021-09-07 17:18:10.717 | INFO     | src.policies:collect_trajectories:221 - Episode 802\n",
      "2021-09-07 17:18:10.741 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.743 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 147.0\n",
      "2021-09-07 17:18:10.743 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 102.0\n",
      "2021-09-07 17:18:10.744 | WARNING  | src.policies:train:144 - The actual batch size is 306, instead of 200\n",
      "2021-09-07 17:18:10.748 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:10.749 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.38814979791641235, 'baseline_loss': 0.805088222026825, 'total_loss': 0.014394313097000122}\n",
      "2021-09-07 17:18:10.750 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25173303484916687\n",
      "2021-09-07 17:18:10.751 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7437610030174255\n",
      "2021-09-07 17:18:10.752 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25173303484916687\n",
      "2021-09-07 17:18:10.753 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:10.755 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:10.756 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2514110803604126, 'baseline_loss': 0.6196314096450806, 'total_loss': 0.058404624462127686}\n",
      "2021-09-07 17:18:10.757 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15913991630077362\n",
      "2021-09-07 17:18:10.758 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9532537460327148\n",
      "2021-09-07 17:18:10.759 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15913991630077362\n",
      "2021-09-07 17:18:10.761 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:10.762 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:10.764 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3578755557537079, 'baseline_loss': 0.7521347999572754, 'total_loss': 0.01819184422492981}\n",
      "2021-09-07 17:18:10.765 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24409396946430206\n",
      "2021-09-07 17:18:10.766 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8707039952278137\n",
      "2021-09-07 17:18:10.767 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24409396946430206\n",
      "2021-09-07 17:18:10.769 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:10.770 | INFO     | src.policies:train:123 - Epoch 147 / 800\n",
      "2021-09-07 17:18:10.771 | INFO     | src.policies:collect_trajectories:221 - Episode 803\n",
      "2021-09-07 17:18:10.939 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.940 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 153.0\n",
      "2021-09-07 17:18:10.940 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 153.0\n",
      "2021-09-07 17:18:10.941 | INFO     | src.policies:collect_trajectories:221 - Episode 804\n",
      "2021-09-07 17:18:10.973 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:10.974 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 167.0\n",
      "2021-09-07 17:18:10.975 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 160.0\n",
      "2021-09-07 17:18:10.975 | WARNING  | src.policies:train:144 - The actual batch size is 320, instead of 200\n",
      "2021-09-07 17:18:10.978 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:10.980 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4009104073047638, 'baseline_loss': 0.6585647463798523, 'total_loss': -0.07162803411483765}\n",
      "2021-09-07 17:18:10.981 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4507552981376648\n",
      "2021-09-07 17:18:10.982 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6161497831344604\n",
      "2021-09-07 17:18:10.983 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4507552981376648\n",
      "2021-09-07 17:18:10.985 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:10.986 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:10.987 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32568567991256714, 'baseline_loss': 0.5754920840263367, 'total_loss': -0.037939637899398804}\n",
      "2021-09-07 17:18:10.988 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13419704139232635\n",
      "2021-09-07 17:18:10.989 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.48173338174819946\n",
      "2021-09-07 17:18:10.991 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13419704139232635\n",
      "2021-09-07 17:18:10.992 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.48173338174819946\n",
      "2021-09-07 17:18:10.993 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:10.994 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4223902225494385, 'baseline_loss': 0.8052082061767578, 'total_loss': -0.01978611946105957}\n",
      "2021-09-07 17:18:10.995 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17575766146183014\n",
      "2021-09-07 17:18:10.996 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2030183970928192\n",
      "2021-09-07 17:18:10.997 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17575766146183014\n",
      "2021-09-07 17:18:10.998 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2030183970928192\n",
      "2021-09-07 17:18:11.000 | INFO     | src.policies:train:123 - Epoch 148 / 800\n",
      "2021-09-07 17:18:11.001 | INFO     | src.policies:collect_trajectories:221 - Episode 805\n",
      "2021-09-07 17:18:11.021 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.022 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 113.0\n",
      "2021-09-07 17:18:11.022 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 113.0\n",
      "2021-09-07 17:18:11.023 | INFO     | src.policies:collect_trajectories:221 - Episode 806\n",
      "2021-09-07 17:18:11.044 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.045 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 111.0\n",
      "2021-09-07 17:18:11.046 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 112.0\n",
      "2021-09-07 17:18:11.047 | WARNING  | src.policies:train:144 - The actual batch size is 224, instead of 200\n",
      "2021-09-07 17:18:11.050 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:11.052 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.205537810921669, 'baseline_loss': 0.42123281955718994, 'total_loss': 0.005078598856925964}\n",
      "2021-09-07 17:18:11.053 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2130214422941208\n",
      "2021-09-07 17:18:11.054 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5496323108673096\n",
      "2021-09-07 17:18:11.055 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2130214422941208\n",
      "2021-09-07 17:18:11.056 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:11.057 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:11.059 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.28365427255630493, 'baseline_loss': 0.47863996028900146, 'total_loss': -0.0443342924118042}\n",
      "2021-09-07 17:18:11.060 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22205868363380432\n",
      "2021-09-07 17:18:11.061 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2222286462783813\n",
      "2021-09-07 17:18:11.062 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22205868363380432\n",
      "2021-09-07 17:18:11.063 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:11.065 | INFO     | src.policies:train:123 - Epoch 149 / 800\n",
      "2021-09-07 17:18:11.066 | INFO     | src.policies:collect_trajectories:221 - Episode 807\n",
      "2021-09-07 17:18:11.096 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.097 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 187.0\n",
      "2021-09-07 17:18:11.098 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 187.0\n",
      "2021-09-07 17:18:11.098 | INFO     | src.policies:collect_trajectories:221 - Episode 808\n",
      "2021-09-07 17:18:11.118 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.119 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 103.0\n",
      "2021-09-07 17:18:11.119 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 145.0\n",
      "2021-09-07 17:18:11.120 | WARNING  | src.policies:train:144 - The actual batch size is 290, instead of 200\n",
      "2021-09-07 17:18:11.123 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:11.125 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3930666744709015, 'baseline_loss': 0.6951141953468323, 'total_loss': -0.04550957679748535}\n",
      "2021-09-07 17:18:11.126 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2850288152694702\n",
      "2021-09-07 17:18:11.127 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1753427982330322\n",
      "2021-09-07 17:18:11.128 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2850288152694702\n",
      "2021-09-07 17:18:11.129 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:11.130 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:11.132 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3326418995857239, 'baseline_loss': 0.7140200138092041, 'total_loss': 0.024368107318878174}\n",
      "2021-09-07 17:18:11.133 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1055430918931961\n",
      "2021-09-07 17:18:11.134 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9627623558044434\n",
      "2021-09-07 17:18:11.135 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1055430918931961\n",
      "2021-09-07 17:18:11.136 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:11.137 | INFO     | src.policies:train:123 - Epoch 150 / 800\n",
      "2021-09-07 17:18:11.138 | INFO     | src.policies:collect_trajectories:221 - Episode 809\n",
      "2021-09-07 17:18:11.170 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.170 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 193.0\n",
      "2021-09-07 17:18:11.171 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 193.0\n",
      "2021-09-07 17:18:11.171 | INFO     | src.policies:collect_trajectories:221 - Episode 810\n",
      "2021-09-07 17:18:11.202 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:11.203 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 188.0\n",
      "2021-09-07 17:18:11.204 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 190.5\n",
      "2021-09-07 17:18:11.205 | WARNING  | src.policies:train:144 - The actual batch size is 381, instead of 200\n",
      "2021-09-07 17:18:11.208 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:11.210 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2520011067390442, 'baseline_loss': 0.5222777724266052, 'total_loss': 0.009137779474258423}\n",
      "2021-09-07 17:18:11.211 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.27723222970962524\n",
      "2021-09-07 17:18:11.212 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0586267709732056\n",
      "2021-09-07 17:18:11.213 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.27723222970962524\n",
      "2021-09-07 17:18:11.214 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:11.215 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:11.217 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23669657111167908, 'baseline_loss': 0.4573902189731598, 'total_loss': -0.008001461625099182}\n",
      "2021-09-07 17:18:11.217 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22720567882061005\n",
      "2021-09-07 17:18:11.219 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6436659693717957\n",
      "2021-09-07 17:18:11.220 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22720567882061005\n",
      "2021-09-07 17:18:11.221 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:11.222 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:11.224 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27428701519966125, 'baseline_loss': 0.5230110287666321, 'total_loss': -0.012781500816345215}\n",
      "2021-09-07 17:18:11.225 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32829809188842773\n",
      "2021-09-07 17:18:11.227 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9000394344329834\n",
      "2021-09-07 17:18:11.228 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32829809188842773\n",
      "2021-09-07 17:18:11.230 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:11.231 | INFO     | src.policies:train:123 - Epoch 151 / 800\n",
      "2021-09-07 17:18:11.232 | INFO     | src.policies:collect_trajectories:221 - Episode 811\n",
      "2021-09-07 17:18:11.250 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.251 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 116.0\n",
      "2021-09-07 17:18:11.251 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 116.0\n",
      "2021-09-07 17:18:11.252 | INFO     | src.policies:collect_trajectories:221 - Episode 812\n",
      "2021-09-07 17:18:11.279 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.280 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 175.0\n",
      "2021-09-07 17:18:11.280 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 145.5\n",
      "2021-09-07 17:18:11.281 | WARNING  | src.policies:train:144 - The actual batch size is 291, instead of 200\n",
      "2021-09-07 17:18:11.284 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:11.288 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.26616528630256653, 'baseline_loss': 0.41365742683410645, 'total_loss': -0.059336572885513306}\n",
      "2021-09-07 17:18:11.289 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31449300050735474\n",
      "2021-09-07 17:18:11.290 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8128969073295593\n",
      "2021-09-07 17:18:11.291 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31449300050735474\n",
      "2021-09-07 17:18:11.293 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:11.294 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:11.295 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.39973410964012146, 'baseline_loss': 0.5513918399810791, 'total_loss': -0.12403818964958191}\n",
      "2021-09-07 17:18:11.296 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5302866101264954\n",
      "2021-09-07 17:18:11.297 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.503745436668396\n",
      "2021-09-07 17:18:11.298 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:11.299 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999892711639404\n",
      "2021-09-07 17:18:11.301 | INFO     | src.policies:train:123 - Epoch 152 / 800\n",
      "2021-09-07 17:18:11.302 | INFO     | src.policies:collect_trajectories:221 - Episode 813\n",
      "2021-09-07 17:18:11.313 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.313 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 59.0\n",
      "2021-09-07 17:18:11.314 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 59.0\n",
      "2021-09-07 17:18:11.314 | INFO     | src.policies:collect_trajectories:221 - Episode 814\n",
      "2021-09-07 17:18:11.345 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.345 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 169.0\n",
      "2021-09-07 17:18:11.346 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 114.0\n",
      "2021-09-07 17:18:11.347 | WARNING  | src.policies:train:144 - The actual batch size is 228, instead of 200\n",
      "2021-09-07 17:18:11.349 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:11.351 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7891610860824585, 'baseline_loss': 2.1436004638671875, 'total_loss': 0.28263914585113525}\n",
      "2021-09-07 17:18:11.352 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.910481333732605\n",
      "2021-09-07 17:18:11.353 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.5871567726135254\n",
      "2021-09-07 17:18:11.354 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:11.355 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:11.356 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:11.357 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.791648805141449, 'baseline_loss': 1.885125994682312, 'total_loss': 0.15091419219970703}\n",
      "2021-09-07 17:18:11.358 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.46958687901496887\n",
      "2021-09-07 17:18:11.359 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.134324073791504\n",
      "2021-09-07 17:18:11.361 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.46958687901496887\n",
      "2021-09-07 17:18:11.362 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:11.364 | INFO     | src.policies:train:123 - Epoch 153 / 800\n",
      "2021-09-07 17:18:11.365 | INFO     | src.policies:collect_trajectories:221 - Episode 815\n",
      "2021-09-07 17:18:11.377 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:11.378 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 71.0\n",
      "2021-09-07 17:18:11.379 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 71.0\n",
      "2021-09-07 17:18:11.379 | INFO     | src.policies:collect_trajectories:221 - Episode 816\n",
      "2021-09-07 17:18:11.397 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.398 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 98.0\n",
      "2021-09-07 17:18:11.398 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 84.5\n",
      "2021-09-07 17:18:11.399 | INFO     | src.policies:collect_trajectories:221 - Episode 817\n",
      "2021-09-07 17:18:11.406 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.407 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 31.0\n",
      "2021-09-07 17:18:11.408 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 66.66666666666667\n",
      "2021-09-07 17:18:11.410 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:11.411 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5623703598976135, 'baseline_loss': 1.1469398736953735, 'total_loss': 0.011099576950073242}\n",
      "2021-09-07 17:18:11.412 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3726412057876587\n",
      "2021-09-07 17:18:11.413 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.60686856508255\n",
      "2021-09-07 17:18:11.414 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3726412057876587\n",
      "2021-09-07 17:18:11.415 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:11.416 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:11.417 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5140897631645203, 'baseline_loss': 1.0795084238052368, 'total_loss': 0.025664448738098145}\n",
      "2021-09-07 17:18:11.418 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18342946469783783\n",
      "2021-09-07 17:18:11.419 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.844904899597168\n",
      "2021-09-07 17:18:11.421 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18342946469783783\n",
      "2021-09-07 17:18:11.422 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:11.479 | INFO     | src.policies:train:123 - Epoch 154 / 800\n",
      "2021-09-07 17:18:11.480 | INFO     | src.policies:collect_trajectories:221 - Episode 818\n",
      "2021-09-07 17:18:11.490 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.490 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 52.0\n",
      "2021-09-07 17:18:11.491 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 52.0\n",
      "2021-09-07 17:18:11.491 | INFO     | src.policies:collect_trajectories:221 - Episode 819\n",
      "2021-09-07 17:18:11.513 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.515 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 135.0\n",
      "2021-09-07 17:18:11.515 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 93.5\n",
      "2021-09-07 17:18:11.516 | INFO     | src.policies:collect_trajectories:221 - Episode 820\n",
      "2021-09-07 17:18:11.539 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.540 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 140.0\n",
      "2021-09-07 17:18:11.541 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 109.0\n",
      "2021-09-07 17:18:11.542 | WARNING  | src.policies:train:144 - The actual batch size is 327, instead of 200\n",
      "2021-09-07 17:18:11.545 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:11.548 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5469551086425781, 'baseline_loss': 1.0483722686767578, 'total_loss': -0.02276897430419922}\n",
      "2021-09-07 17:18:11.549 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16478978097438812\n",
      "2021-09-07 17:18:11.550 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7038924694061279\n",
      "2021-09-07 17:18:11.552 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16478978097438812\n",
      "2021-09-07 17:18:11.553 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:11.554 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:11.555 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4907079041004181, 'baseline_loss': 0.9179693460464478, 'total_loss': -0.031723231077194214}\n",
      "2021-09-07 17:18:11.556 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30487656593322754\n",
      "2021-09-07 17:18:11.557 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.34026598930358887\n",
      "2021-09-07 17:18:11.559 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30487656593322754\n",
      "2021-09-07 17:18:11.560 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.34026598930358887\n",
      "2021-09-07 17:18:11.561 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:11.563 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5185548663139343, 'baseline_loss': 1.0065412521362305, 'total_loss': -0.015284240245819092}\n",
      "2021-09-07 17:18:11.564 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.47332343459129333\n",
      "2021-09-07 17:18:11.565 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7393439412117004\n",
      "2021-09-07 17:18:11.567 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.47332343459129333\n",
      "2021-09-07 17:18:11.568 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:11.569 | INFO     | src.policies:train:123 - Epoch 155 / 800\n",
      "2021-09-07 17:18:11.570 | INFO     | src.policies:collect_trajectories:221 - Episode 821\n",
      "2021-09-07 17:18:11.588 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.589 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 103.0\n",
      "2021-09-07 17:18:11.589 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 103.0\n",
      "2021-09-07 17:18:11.590 | INFO     | src.policies:collect_trajectories:221 - Episode 822\n",
      "2021-09-07 17:18:11.614 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.614 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 133.0\n",
      "2021-09-07 17:18:11.615 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 118.0\n",
      "2021-09-07 17:18:11.616 | WARNING  | src.policies:train:144 - The actual batch size is 236, instead of 200\n",
      "2021-09-07 17:18:11.618 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:11.620 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5319662690162659, 'baseline_loss': 0.8911125063896179, 'total_loss': -0.08641001582145691}\n",
      "2021-09-07 17:18:11.621 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.309950590133667\n",
      "2021-09-07 17:18:11.622 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.45383164286613464\n",
      "2021-09-07 17:18:11.623 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.309950590133667\n",
      "2021-09-07 17:18:11.624 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.45383164286613464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:11.626 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:11.627 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.410436749458313, 'baseline_loss': 0.6089588403701782, 'total_loss': -0.10595732927322388}\n",
      "2021-09-07 17:18:11.628 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12385214865207672\n",
      "2021-09-07 17:18:11.629 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3309799134731293\n",
      "2021-09-07 17:18:11.630 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12385214865207672\n",
      "2021-09-07 17:18:11.631 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3309799134731293\n",
      "2021-09-07 17:18:11.632 | INFO     | src.policies:train:123 - Epoch 156 / 800\n",
      "2021-09-07 17:18:11.633 | INFO     | src.policies:collect_trajectories:221 - Episode 823\n",
      "2021-09-07 17:18:11.650 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.651 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 109.0\n",
      "2021-09-07 17:18:11.651 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 109.0\n",
      "2021-09-07 17:18:11.653 | INFO     | src.policies:collect_trajectories:221 - Episode 824\n",
      "2021-09-07 17:18:11.684 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.685 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:11.686 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 154.5\n",
      "2021-09-07 17:18:11.688 | WARNING  | src.policies:train:144 - The actual batch size is 309, instead of 200\n",
      "2021-09-07 17:18:11.691 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:11.693 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.793192446231842, 'baseline_loss': 2.047100305557251, 'total_loss': 0.23035770654678345}\n",
      "2021-09-07 17:18:11.694 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5360519289970398\n",
      "2021-09-07 17:18:11.695 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.5929453372955322\n",
      "2021-09-07 17:18:11.696 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:11.697 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:11.698 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:11.700 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.717732846736908, 'baseline_loss': 2.066519260406494, 'total_loss': 0.3155267834663391}\n",
      "2021-09-07 17:18:11.701 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19391696155071259\n",
      "2021-09-07 17:18:11.702 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.8738749027252197\n",
      "2021-09-07 17:18:11.703 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19391696155071259\n",
      "2021-09-07 17:18:11.704 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:11.706 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:11.707 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.9663317799568176, 'baseline_loss': 2.7991411685943604, 'total_loss': 0.43323880434036255}\n",
      "2021-09-07 17:18:11.708 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7050026655197144\n",
      "2021-09-07 17:18:11.709 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.4078924655914307\n",
      "2021-09-07 17:18:11.710 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:11.711 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:11.713 | INFO     | src.policies:train:123 - Epoch 157 / 800\n",
      "2021-09-07 17:18:11.713 | INFO     | src.policies:collect_trajectories:221 - Episode 825\n",
      "2021-09-07 17:18:11.831 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.832 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 118.0\n",
      "2021-09-07 17:18:11.832 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 118.0\n",
      "2021-09-07 17:18:11.833 | INFO     | src.policies:collect_trajectories:221 - Episode 826\n",
      "2021-09-07 17:18:11.856 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.856 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 126.0\n",
      "2021-09-07 17:18:11.857 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 122.0\n",
      "2021-09-07 17:18:11.857 | WARNING  | src.policies:train:144 - The actual batch size is 244, instead of 200\n",
      "2021-09-07 17:18:11.860 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:11.862 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2764643430709839, 'baseline_loss': 0.4370957314968109, 'total_loss': -0.05791647732257843}\n",
      "2021-09-07 17:18:11.863 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.47622549533843994\n",
      "2021-09-07 17:18:11.864 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1422667503356934\n",
      "2021-09-07 17:18:11.865 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.47622549533843994\n",
      "2021-09-07 17:18:11.866 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:11.868 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:11.869 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3306046724319458, 'baseline_loss': 0.4558155834674835, 'total_loss': -0.10269688069820404}\n",
      "2021-09-07 17:18:11.870 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16088159382343292\n",
      "2021-09-07 17:18:11.871 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9707264304161072\n",
      "2021-09-07 17:18:11.872 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16088159382343292\n",
      "2021-09-07 17:18:11.873 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:11.875 | INFO     | src.policies:train:123 - Epoch 158 / 800\n",
      "2021-09-07 17:18:11.875 | INFO     | src.policies:collect_trajectories:221 - Episode 827\n",
      "2021-09-07 17:18:11.907 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.907 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 175.0\n",
      "2021-09-07 17:18:11.908 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 175.0\n",
      "2021-09-07 17:18:11.908 | INFO     | src.policies:collect_trajectories:221 - Episode 828\n",
      "2021-09-07 17:18:11.921 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.922 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 73.0\n",
      "2021-09-07 17:18:11.922 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 124.0\n",
      "2021-09-07 17:18:11.924 | WARNING  | src.policies:train:144 - The actual batch size is 248, instead of 200\n",
      "2021-09-07 17:18:11.928 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:11.929 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5429980158805847, 'baseline_loss': 1.3536338806152344, 'total_loss': 0.13381892442703247}\n",
      "2021-09-07 17:18:11.930 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.41065138578414917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:11.931 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7860007286071777\n",
      "2021-09-07 17:18:11.932 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.41065138578414917\n",
      "2021-09-07 17:18:11.934 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:11.935 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:11.936 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5896462798118591, 'baseline_loss': 1.580430030822754, 'total_loss': 0.20056873559951782}\n",
      "2021-09-07 17:18:11.937 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2952743470668793\n",
      "2021-09-07 17:18:11.938 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0590906143188477\n",
      "2021-09-07 17:18:11.939 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2952743470668793\n",
      "2021-09-07 17:18:11.941 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:11.943 | INFO     | src.policies:train:123 - Epoch 159 / 800\n",
      "2021-09-07 17:18:11.943 | INFO     | src.policies:collect_trajectories:221 - Episode 829\n",
      "2021-09-07 17:18:11.967 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:11.968 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 135.0\n",
      "2021-09-07 17:18:11.969 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 135.0\n",
      "2021-09-07 17:18:11.969 | INFO     | src.policies:collect_trajectories:221 - Episode 830\n",
      "2021-09-07 17:18:12.040 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.041 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 169.0\n",
      "2021-09-07 17:18:12.041 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 152.0\n",
      "2021-09-07 17:18:12.042 | WARNING  | src.policies:train:144 - The actual batch size is 304, instead of 200\n",
      "2021-09-07 17:18:12.046 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:12.049 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42820557951927185, 'baseline_loss': 0.6473729610443115, 'total_loss': -0.10451909899711609}\n",
      "2021-09-07 17:18:12.050 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1629665046930313\n",
      "2021-09-07 17:18:12.051 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2801414132118225\n",
      "2021-09-07 17:18:12.052 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1629665046930313\n",
      "2021-09-07 17:18:12.053 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2801414132118225\n",
      "2021-09-07 17:18:12.055 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:12.056 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4281274378299713, 'baseline_loss': 0.6458641886711121, 'total_loss': -0.10519534349441528}\n",
      "2021-09-07 17:18:12.057 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3037252724170685\n",
      "2021-09-07 17:18:12.058 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.1601124256849289\n",
      "2021-09-07 17:18:12.059 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3037252724170685\n",
      "2021-09-07 17:18:12.060 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.1601124256849289\n",
      "2021-09-07 17:18:12.061 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:12.063 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42985644936561584, 'baseline_loss': 0.6266559362411499, 'total_loss': -0.1165284812450409}\n",
      "2021-09-07 17:18:12.064 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5410996675491333\n",
      "2021-09-07 17:18:12.065 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.26932093501091003\n",
      "2021-09-07 17:18:12.066 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:12.067 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.26932093501091003\n",
      "2021-09-07 17:18:12.069 | INFO     | src.policies:train:123 - Epoch 160 / 800\n",
      "2021-09-07 17:18:12.069 | INFO     | src.policies:collect_trajectories:221 - Episode 831\n",
      "2021-09-07 17:18:12.087 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.088 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 107.0\n",
      "2021-09-07 17:18:12.088 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 107.0\n",
      "2021-09-07 17:18:12.089 | INFO     | src.policies:collect_trajectories:221 - Episode 832\n",
      "2021-09-07 17:18:12.099 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.099 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 45.0\n",
      "2021-09-07 17:18:12.100 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 76.0\n",
      "2021-09-07 17:18:12.100 | INFO     | src.policies:collect_trajectories:221 - Episode 833\n",
      "2021-09-07 17:18:12.132 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.133 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 164.0\n",
      "2021-09-07 17:18:12.134 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 105.33333333333333\n",
      "2021-09-07 17:18:12.134 | WARNING  | src.policies:train:144 - The actual batch size is 316, instead of 200\n",
      "2021-09-07 17:18:12.137 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:12.139 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3997592628002167, 'baseline_loss': 0.8395416736602783, 'total_loss': 0.020011574029922485}\n",
      "2021-09-07 17:18:12.140 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16741202771663666\n",
      "2021-09-07 17:18:12.141 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.41466912627220154\n",
      "2021-09-07 17:18:12.142 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16741202771663666\n",
      "2021-09-07 17:18:12.143 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.41466912627220154\n",
      "2021-09-07 17:18:12.144 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:12.146 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4222252666950226, 'baseline_loss': 0.8041889071464539, 'total_loss': -0.020130813121795654}\n",
      "2021-09-07 17:18:12.147 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7465993762016296\n",
      "2021-09-07 17:18:12.148 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.33443859219551086\n",
      "2021-09-07 17:18:12.149 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:12.150 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.33443859219551086\n",
      "2021-09-07 17:18:12.151 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:12.152 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4586525857448578, 'baseline_loss': 0.979910135269165, 'total_loss': 0.03130248188972473}\n",
      "2021-09-07 17:18:12.153 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3692125082015991\n",
      "2021-09-07 17:18:12.154 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.38704851269721985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:12.155 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3692125082015991\n",
      "2021-09-07 17:18:12.156 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.38704851269721985\n",
      "2021-09-07 17:18:12.158 | INFO     | src.policies:train:123 - Epoch 161 / 800\n",
      "2021-09-07 17:18:12.158 | INFO     | src.policies:collect_trajectories:221 - Episode 834\n",
      "2021-09-07 17:18:12.184 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.185 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 167.0\n",
      "2021-09-07 17:18:12.185 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 167.0\n",
      "2021-09-07 17:18:12.186 | INFO     | src.policies:collect_trajectories:221 - Episode 835\n",
      "2021-09-07 17:18:12.208 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.209 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 130.0\n",
      "2021-09-07 17:18:12.210 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 148.5\n",
      "2021-09-07 17:18:12.210 | WARNING  | src.policies:train:144 - The actual batch size is 297, instead of 200\n",
      "2021-09-07 17:18:12.213 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:12.214 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3102920353412628, 'baseline_loss': 0.4838011562824249, 'total_loss': -0.06839145720005035}\n",
      "2021-09-07 17:18:12.216 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13671383261680603\n",
      "2021-09-07 17:18:12.216 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6158298850059509\n",
      "2021-09-07 17:18:12.218 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13671383261680603\n",
      "2021-09-07 17:18:12.219 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:12.220 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:12.221 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2786955237388611, 'baseline_loss': 0.45628488063812256, 'total_loss': -0.050553083419799805}\n",
      "2021-09-07 17:18:12.222 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.271484911441803\n",
      "2021-09-07 17:18:12.223 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8830567002296448\n",
      "2021-09-07 17:18:12.224 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.271484911441803\n",
      "2021-09-07 17:18:12.226 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:12.228 | INFO     | src.policies:train:123 - Epoch 162 / 800\n",
      "2021-09-07 17:18:12.229 | INFO     | src.policies:collect_trajectories:221 - Episode 836\n",
      "2021-09-07 17:18:12.245 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.246 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 100.0\n",
      "2021-09-07 17:18:12.246 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 100.0\n",
      "2021-09-07 17:18:12.247 | INFO     | src.policies:collect_trajectories:221 - Episode 837\n",
      "2021-09-07 17:18:12.273 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.274 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 134.0\n",
      "2021-09-07 17:18:12.274 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 117.0\n",
      "2021-09-07 17:18:12.275 | WARNING  | src.policies:train:144 - The actual batch size is 234, instead of 200\n",
      "2021-09-07 17:18:12.277 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:12.279 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41604161262512207, 'baseline_loss': 0.7006938457489014, 'total_loss': -0.06569468975067139}\n",
      "2021-09-07 17:18:12.280 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3005412518978119\n",
      "2021-09-07 17:18:12.281 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2957828640937805\n",
      "2021-09-07 17:18:12.283 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3005412518978119\n",
      "2021-09-07 17:18:12.284 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2957828640937805\n",
      "2021-09-07 17:18:12.285 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:12.287 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5720160603523254, 'baseline_loss': 0.8414962291717529, 'total_loss': -0.15126794576644897}\n",
      "2021-09-07 17:18:12.288 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4341737926006317\n",
      "2021-09-07 17:18:12.289 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5895969867706299\n",
      "2021-09-07 17:18:12.290 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4341737926006317\n",
      "2021-09-07 17:18:12.291 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:12.292 | INFO     | src.policies:train:123 - Epoch 163 / 800\n",
      "2021-09-07 17:18:12.293 | INFO     | src.policies:collect_trajectories:221 - Episode 838\n",
      "2021-09-07 17:18:12.310 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.311 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 95.0\n",
      "2021-09-07 17:18:12.311 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 95.0\n",
      "2021-09-07 17:18:12.312 | INFO     | src.policies:collect_trajectories:221 - Episode 839\n",
      "2021-09-07 17:18:12.335 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.336 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 136.0\n",
      "2021-09-07 17:18:12.336 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 115.5\n",
      "2021-09-07 17:18:12.337 | WARNING  | src.policies:train:144 - The actual batch size is 231, instead of 200\n",
      "2021-09-07 17:18:12.341 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:12.344 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3625189960002899, 'baseline_loss': 0.5980451703071594, 'total_loss': -0.0634964108467102}\n",
      "2021-09-07 17:18:12.345 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3168907165527344\n",
      "2021-09-07 17:18:12.346 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3490908145904541\n",
      "2021-09-07 17:18:12.348 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3168907165527344\n",
      "2021-09-07 17:18:12.349 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3490908145904541\n",
      "2021-09-07 17:18:12.350 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:12.351 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42315474152565, 'baseline_loss': 0.7522379159927368, 'total_loss': -0.047035783529281616}\n",
      "2021-09-07 17:18:12.352 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2598230838775635\n",
      "2021-09-07 17:18:12.353 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2892666161060333\n",
      "2021-09-07 17:18:12.354 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2598230838775635\n",
      "2021-09-07 17:18:12.356 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2892666161060333\n",
      "2021-09-07 17:18:12.357 | INFO     | src.policies:train:123 - Epoch 164 / 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:12.358 | INFO     | src.policies:collect_trajectories:221 - Episode 840\n",
      "2021-09-07 17:18:12.378 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.379 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 127.0\n",
      "2021-09-07 17:18:12.379 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 127.0\n",
      "2021-09-07 17:18:12.380 | INFO     | src.policies:collect_trajectories:221 - Episode 841\n",
      "2021-09-07 17:18:12.398 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.398 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 85.0\n",
      "2021-09-07 17:18:12.399 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 106.0\n",
      "2021-09-07 17:18:12.400 | WARNING  | src.policies:train:144 - The actual batch size is 212, instead of 200\n",
      "2021-09-07 17:18:12.403 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:12.405 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3676336407661438, 'baseline_loss': 0.6126266717910767, 'total_loss': -0.06132030487060547}\n",
      "2021-09-07 17:18:12.406 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11905617266893387\n",
      "2021-09-07 17:18:12.407 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.17371849715709686\n",
      "2021-09-07 17:18:12.408 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11905617266893387\n",
      "2021-09-07 17:18:12.410 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.17371849715709686\n",
      "2021-09-07 17:18:12.411 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:12.412 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3522091209888458, 'baseline_loss': 0.5693492889404297, 'total_loss': -0.06753447651863098}\n",
      "2021-09-07 17:18:12.413 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18569016456604004\n",
      "2021-09-07 17:18:12.414 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.38886094093322754\n",
      "2021-09-07 17:18:12.415 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18569016456604004\n",
      "2021-09-07 17:18:12.416 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.38886094093322754\n",
      "2021-09-07 17:18:12.418 | INFO     | src.policies:train:123 - Epoch 165 / 800\n",
      "2021-09-07 17:18:12.419 | INFO     | src.policies:collect_trajectories:221 - Episode 842\n",
      "2021-09-07 17:18:12.430 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.431 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 56.0\n",
      "2021-09-07 17:18:12.431 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 56.0\n",
      "2021-09-07 17:18:12.431 | INFO     | src.policies:collect_trajectories:221 - Episode 843\n",
      "2021-09-07 17:18:12.449 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.450 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 104.0\n",
      "2021-09-07 17:18:12.450 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 80.0\n",
      "2021-09-07 17:18:12.451 | INFO     | src.policies:collect_trajectories:221 - Episode 844\n",
      "2021-09-07 17:18:12.469 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.469 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 99.0\n",
      "2021-09-07 17:18:12.470 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 86.33333333333333\n",
      "2021-09-07 17:18:12.470 | WARNING  | src.policies:train:144 - The actual batch size is 259, instead of 200\n",
      "2021-09-07 17:18:12.472 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:12.475 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5854685306549072, 'baseline_loss': 0.9259330630302429, 'total_loss': -0.12250199913978577}\n",
      "2021-09-07 17:18:12.476 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2163120061159134\n",
      "2021-09-07 17:18:12.476 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4724713861942291\n",
      "2021-09-07 17:18:12.478 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2163120061159134\n",
      "2021-09-07 17:18:12.479 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4724713861942291\n",
      "2021-09-07 17:18:12.480 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:12.481 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5126489400863647, 'baseline_loss': 1.1090255975723267, 'total_loss': 0.041863858699798584}\n",
      "2021-09-07 17:18:12.483 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.380440890789032\n",
      "2021-09-07 17:18:12.484 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0404672622680664\n",
      "2021-09-07 17:18:12.485 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.380440890789032\n",
      "2021-09-07 17:18:12.487 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:12.488 | INFO     | src.policies:train:123 - Epoch 166 / 800\n",
      "2021-09-07 17:18:12.489 | INFO     | src.policies:collect_trajectories:221 - Episode 845\n",
      "2021-09-07 17:18:12.519 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.520 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:12.521 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:12.530 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:12.581 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.974256694316864, 'baseline_loss': 3.144075632095337, 'total_loss': 0.5977811217308044}\n",
      "2021-09-07 17:18:12.583 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31729862093925476\n",
      "2021-09-07 17:18:12.585 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.7533411979675293\n",
      "2021-09-07 17:18:12.586 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31729862093925476\n",
      "2021-09-07 17:18:12.588 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:12.589 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:12.590 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7989301085472107, 'baseline_loss': 2.995549201965332, 'total_loss': 0.6988444924354553}\n",
      "2021-09-07 17:18:12.591 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3934192359447479\n",
      "2021-09-07 17:18:12.592 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.8831968307495117\n",
      "2021-09-07 17:18:12.593 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3934192359447479\n",
      "2021-09-07 17:18:12.594 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:12.596 | INFO     | src.policies:train:123 - Epoch 167 / 800\n",
      "2021-09-07 17:18:12.596 | INFO     | src.policies:collect_trajectories:221 - Episode 846\n",
      "2021-09-07 17:18:12.603 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.604 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 35.0\n",
      "2021-09-07 17:18:12.604 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 35.0\n",
      "2021-09-07 17:18:12.605 | INFO     | src.policies:collect_trajectories:221 - Episode 847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:12.615 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.615 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 43.0\n",
      "2021-09-07 17:18:12.616 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 39.0\n",
      "2021-09-07 17:18:12.616 | INFO     | src.policies:collect_trajectories:221 - Episode 848\n",
      "2021-09-07 17:18:12.640 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.641 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 141.0\n",
      "2021-09-07 17:18:12.641 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 73.0\n",
      "2021-09-07 17:18:12.642 | WARNING  | src.policies:train:144 - The actual batch size is 219, instead of 200\n",
      "2021-09-07 17:18:12.645 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:12.648 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6406899094581604, 'baseline_loss': 1.4064102172851562, 'total_loss': 0.06251519918441772}\n",
      "2021-09-07 17:18:12.649 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31117117404937744\n",
      "2021-09-07 17:18:12.650 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1894521713256836\n",
      "2021-09-07 17:18:12.651 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31117117404937744\n",
      "2021-09-07 17:18:12.652 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:12.653 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:12.655 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6132903099060059, 'baseline_loss': 1.6056714057922363, 'total_loss': 0.1895453929901123}\n",
      "2021-09-07 17:18:12.656 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1348949670791626\n",
      "2021-09-07 17:18:12.657 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5559889078140259\n",
      "2021-09-07 17:18:12.658 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1348949670791626\n",
      "2021-09-07 17:18:12.659 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:12.661 | INFO     | src.policies:train:123 - Epoch 168 / 800\n",
      "2021-09-07 17:18:12.661 | INFO     | src.policies:collect_trajectories:221 - Episode 849\n",
      "2021-09-07 17:18:12.674 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.675 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 79.0\n",
      "2021-09-07 17:18:12.675 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 79.0\n",
      "2021-09-07 17:18:12.675 | INFO     | src.policies:collect_trajectories:221 - Episode 850\n",
      "2021-09-07 17:18:12.691 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.691 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 75.0\n",
      "2021-09-07 17:18:12.692 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 77.0\n",
      "2021-09-07 17:18:12.692 | INFO     | src.policies:collect_trajectories:221 - Episode 851\n",
      "2021-09-07 17:18:12.710 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.711 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 107.0\n",
      "2021-09-07 17:18:12.711 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 87.0\n",
      "2021-09-07 17:18:12.712 | WARNING  | src.policies:train:144 - The actual batch size is 261, instead of 200\n",
      "2021-09-07 17:18:12.714 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:12.716 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4253080487251282, 'baseline_loss': 0.9202430248260498, 'total_loss': 0.03481346368789673}\n",
      "2021-09-07 17:18:12.717 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5725985169410706\n",
      "2021-09-07 17:18:12.718 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8498690128326416\n",
      "2021-09-07 17:18:12.719 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:12.720 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:12.722 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:12.723 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41097840666770935, 'baseline_loss': 0.7958313822746277, 'total_loss': -0.013062715530395508}\n",
      "2021-09-07 17:18:12.725 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18075819313526154\n",
      "2021-09-07 17:18:12.726 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7664082646369934\n",
      "2021-09-07 17:18:12.727 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18075819313526154\n",
      "2021-09-07 17:18:12.729 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:12.730 | INFO     | src.policies:train:123 - Epoch 169 / 800\n",
      "2021-09-07 17:18:12.731 | INFO     | src.policies:collect_trajectories:221 - Episode 852\n",
      "2021-09-07 17:18:12.752 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.752 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 134.0\n",
      "2021-09-07 17:18:12.753 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 134.0\n",
      "2021-09-07 17:18:12.753 | INFO     | src.policies:collect_trajectories:221 - Episode 853\n",
      "2021-09-07 17:18:12.780 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.781 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 163.0\n",
      "2021-09-07 17:18:12.781 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 148.5\n",
      "2021-09-07 17:18:12.782 | WARNING  | src.policies:train:144 - The actual batch size is 297, instead of 200\n",
      "2021-09-07 17:18:12.785 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:12.788 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5269501209259033, 'baseline_loss': 1.056542992591858, 'total_loss': 0.0013213753700256348}\n",
      "2021-09-07 17:18:12.789 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.49457189440727234\n",
      "2021-09-07 17:18:12.790 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0926568508148193\n",
      "2021-09-07 17:18:12.791 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49457189440727234\n",
      "2021-09-07 17:18:12.793 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:12.794 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:12.795 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5262625217437744, 'baseline_loss': 1.009748935699463, 'total_loss': -0.02138805389404297}\n",
      "2021-09-07 17:18:12.797 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17950323224067688\n",
      "2021-09-07 17:18:12.799 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7261915802955627\n",
      "2021-09-07 17:18:12.800 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17950323224067688\n",
      "2021-09-07 17:18:12.801 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:12.803 | INFO     | src.policies:train:123 - Epoch 170 / 800\n",
      "2021-09-07 17:18:12.803 | INFO     | src.policies:collect_trajectories:221 - Episode 854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:12.816 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.817 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 70.0\n",
      "2021-09-07 17:18:12.818 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 70.0\n",
      "2021-09-07 17:18:12.818 | INFO     | src.policies:collect_trajectories:221 - Episode 855\n",
      "2021-09-07 17:18:12.835 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.836 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 96.0\n",
      "2021-09-07 17:18:12.837 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 83.0\n",
      "2021-09-07 17:18:12.837 | INFO     | src.policies:collect_trajectories:221 - Episode 856\n",
      "2021-09-07 17:18:12.855 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.855 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 89.0\n",
      "2021-09-07 17:18:12.856 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 85.0\n",
      "2021-09-07 17:18:12.857 | WARNING  | src.policies:train:144 - The actual batch size is 255, instead of 200\n",
      "2021-09-07 17:18:12.861 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:12.863 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6119554042816162, 'baseline_loss': 1.3453471660614014, 'total_loss': 0.06071817874908447}\n",
      "2021-09-07 17:18:12.864 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6196580529212952\n",
      "2021-09-07 17:18:12.865 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2108368873596191\n",
      "2021-09-07 17:18:12.866 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:12.867 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:12.869 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:12.870 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42756029963493347, 'baseline_loss': 0.8802457451820374, 'total_loss': 0.012562572956085205}\n",
      "2021-09-07 17:18:12.871 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.48232972621917725\n",
      "2021-09-07 17:18:12.872 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5391078591346741\n",
      "2021-09-07 17:18:12.873 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.48232972621917725\n",
      "2021-09-07 17:18:12.874 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:12.876 | INFO     | src.policies:train:123 - Epoch 171 / 800\n",
      "2021-09-07 17:18:12.877 | INFO     | src.policies:collect_trajectories:221 - Episode 857\n",
      "2021-09-07 17:18:12.895 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.896 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 120.0\n",
      "2021-09-07 17:18:12.897 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 120.0\n",
      "2021-09-07 17:18:12.897 | INFO     | src.policies:collect_trajectories:221 - Episode 858\n",
      "2021-09-07 17:18:12.918 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.919 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 110.0\n",
      "2021-09-07 17:18:12.919 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 115.0\n",
      "2021-09-07 17:18:12.920 | WARNING  | src.policies:train:144 - The actual batch size is 230, instead of 200\n",
      "2021-09-07 17:18:12.922 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:12.924 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41765421628952026, 'baseline_loss': 0.6273839473724365, 'total_loss': -0.103962242603302}\n",
      "2021-09-07 17:18:12.925 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4958004951477051\n",
      "2021-09-07 17:18:12.926 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2444755584001541\n",
      "2021-09-07 17:18:12.927 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4958004951477051\n",
      "2021-09-07 17:18:12.928 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2444755584001541\n",
      "2021-09-07 17:18:12.929 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:12.930 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.35232871770858765, 'baseline_loss': 0.6394007802009583, 'total_loss': -0.03262832760810852}\n",
      "2021-09-07 17:18:12.931 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14333787560462952\n",
      "2021-09-07 17:18:12.932 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3281608521938324\n",
      "2021-09-07 17:18:12.933 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14333787560462952\n",
      "2021-09-07 17:18:12.934 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3281608521938324\n",
      "2021-09-07 17:18:12.936 | INFO     | src.policies:train:123 - Epoch 172 / 800\n",
      "2021-09-07 17:18:12.936 | INFO     | src.policies:collect_trajectories:221 - Episode 859\n",
      "2021-09-07 17:18:12.966 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:12.967 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:12.967 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:12.970 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:12.973 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6897673010826111, 'baseline_loss': 1.6209253072738647, 'total_loss': 0.12069535255432129}\n",
      "2021-09-07 17:18:12.974 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4885023832321167\n",
      "2021-09-07 17:18:12.975 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6341800689697266\n",
      "2021-09-07 17:18:12.976 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4885023832321167\n",
      "2021-09-07 17:18:12.977 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:12.978 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:12.979 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4846854507923126, 'baseline_loss': 1.1339470148086548, 'total_loss': 0.08228805661201477}\n",
      "2021-09-07 17:18:12.981 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21190007030963898\n",
      "2021-09-07 17:18:12.982 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1322160959243774\n",
      "2021-09-07 17:18:12.983 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21190007030963898\n",
      "2021-09-07 17:18:12.984 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:12.986 | INFO     | src.policies:train:123 - Epoch 173 / 800\n",
      "2021-09-07 17:18:12.987 | INFO     | src.policies:collect_trajectories:221 - Episode 860\n",
      "2021-09-07 17:18:13.006 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.007 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 110.0\n",
      "2021-09-07 17:18:13.007 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 110.0\n",
      "2021-09-07 17:18:13.008 | INFO     | src.policies:collect_trajectories:221 - Episode 861\n",
      "2021-09-07 17:18:13.025 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:13.025 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 100.0\n",
      "2021-09-07 17:18:13.026 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 105.0\n",
      "2021-09-07 17:18:13.027 | WARNING  | src.policies:train:144 - The actual batch size is 210, instead of 200\n",
      "2021-09-07 17:18:13.029 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:13.031 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2076948583126068, 'baseline_loss': 0.45004206895828247, 'total_loss': 0.017326176166534424}\n",
      "2021-09-07 17:18:13.032 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.9417464137077332\n",
      "2021-09-07 17:18:13.033 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1202471256256104\n",
      "2021-09-07 17:18:13.034 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:13.035 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:13.037 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:13.038 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23636597394943237, 'baseline_loss': 0.4902658760547638, 'total_loss': 0.008766964077949524}\n",
      "2021-09-07 17:18:13.039 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1318805068731308\n",
      "2021-09-07 17:18:13.041 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1807454824447632\n",
      "2021-09-07 17:18:13.042 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1318805068731308\n",
      "2021-09-07 17:18:13.043 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:13.045 | INFO     | src.policies:train:123 - Epoch 174 / 800\n",
      "2021-09-07 17:18:13.045 | INFO     | src.policies:collect_trajectories:221 - Episode 862\n",
      "2021-09-07 17:18:13.058 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.059 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 77.0\n",
      "2021-09-07 17:18:13.060 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 77.0\n",
      "2021-09-07 17:18:13.060 | INFO     | src.policies:collect_trajectories:221 - Episode 863\n",
      "2021-09-07 17:18:13.065 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.066 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 17.0\n",
      "2021-09-07 17:18:13.066 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 47.0\n",
      "2021-09-07 17:18:13.067 | INFO     | src.policies:collect_trajectories:221 - Episode 864\n",
      "2021-09-07 17:18:13.151 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.151 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 157.0\n",
      "2021-09-07 17:18:13.152 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 83.66666666666667\n",
      "2021-09-07 17:18:13.152 | WARNING  | src.policies:train:144 - The actual batch size is 251, instead of 200\n",
      "2021-09-07 17:18:13.155 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:13.157 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.276541143655777, 'baseline_loss': 0.5769883990287781, 'total_loss': 0.01195305585861206}\n",
      "2021-09-07 17:18:13.158 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.33766135573387146\n",
      "2021-09-07 17:18:13.159 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5664077401161194\n",
      "2021-09-07 17:18:13.160 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.33766135573387146\n",
      "2021-09-07 17:18:13.161 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:13.162 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:13.164 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4978163242340088, 'baseline_loss': 0.9544715285301208, 'total_loss': -0.020580559968948364}\n",
      "2021-09-07 17:18:13.165 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32571420073509216\n",
      "2021-09-07 17:18:13.165 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0326018333435059\n",
      "2021-09-07 17:18:13.167 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32571420073509216\n",
      "2021-09-07 17:18:13.168 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:13.169 | INFO     | src.policies:train:123 - Epoch 175 / 800\n",
      "2021-09-07 17:18:13.170 | INFO     | src.policies:collect_trajectories:221 - Episode 865\n",
      "2021-09-07 17:18:13.177 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.178 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 38.0\n",
      "2021-09-07 17:18:13.178 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 38.0\n",
      "2021-09-07 17:18:13.179 | INFO     | src.policies:collect_trajectories:221 - Episode 866\n",
      "2021-09-07 17:18:13.192 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.193 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 70.0\n",
      "2021-09-07 17:18:13.194 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 54.0\n",
      "2021-09-07 17:18:13.195 | INFO     | src.policies:collect_trajectories:221 - Episode 867\n",
      "2021-09-07 17:18:13.200 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.200 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:13.201 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 42.666666666666664\n",
      "2021-09-07 17:18:13.201 | INFO     | src.policies:collect_trajectories:221 - Episode 868\n",
      "2021-09-07 17:18:13.219 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.219 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 113.0\n",
      "2021-09-07 17:18:13.220 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 60.25\n",
      "2021-09-07 17:18:13.220 | WARNING  | src.policies:train:144 - The actual batch size is 241, instead of 200\n",
      "2021-09-07 17:18:13.224 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:13.227 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5649415850639343, 'baseline_loss': 1.2588248252868652, 'total_loss': 0.06447082757949829}\n",
      "2021-09-07 17:18:13.228 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7192724347114563\n",
      "2021-09-07 17:18:13.229 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0790431499481201\n",
      "2021-09-07 17:18:13.230 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:13.231 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:13.233 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:13.234 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5652458667755127, 'baseline_loss': 1.1811232566833496, 'total_loss': 0.02531576156616211}\n",
      "2021-09-07 17:18:13.235 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2408723384141922\n",
      "2021-09-07 17:18:13.236 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5436121225357056\n",
      "2021-09-07 17:18:13.237 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2408723384141922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:13.238 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:13.240 | INFO     | src.policies:train:123 - Epoch 176 / 800\n",
      "2021-09-07 17:18:13.241 | INFO     | src.policies:collect_trajectories:221 - Episode 869\n",
      "2021-09-07 17:18:13.255 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.256 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 89.0\n",
      "2021-09-07 17:18:13.256 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 89.0\n",
      "2021-09-07 17:18:13.257 | INFO     | src.policies:collect_trajectories:221 - Episode 870\n",
      "2021-09-07 17:18:13.283 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.284 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 122.0\n",
      "2021-09-07 17:18:13.284 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 105.5\n",
      "2021-09-07 17:18:13.285 | WARNING  | src.policies:train:144 - The actual batch size is 211, instead of 200\n",
      "2021-09-07 17:18:13.287 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:13.289 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3251217007637024, 'baseline_loss': 0.5215333700180054, 'total_loss': -0.06435501575469971}\n",
      "2021-09-07 17:18:13.290 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11907059699296951\n",
      "2021-09-07 17:18:13.291 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.46739861369132996\n",
      "2021-09-07 17:18:13.292 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11907059699296951\n",
      "2021-09-07 17:18:13.293 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.46739861369132996\n",
      "2021-09-07 17:18:13.295 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:13.296 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3404511511325836, 'baseline_loss': 0.5990604758262634, 'total_loss': -0.040920913219451904}\n",
      "2021-09-07 17:18:13.297 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2739017605781555\n",
      "2021-09-07 17:18:13.298 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4915618300437927\n",
      "2021-09-07 17:18:13.299 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2739017605781555\n",
      "2021-09-07 17:18:13.300 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4915618300437927\n",
      "2021-09-07 17:18:13.302 | INFO     | src.policies:train:123 - Epoch 177 / 800\n",
      "2021-09-07 17:18:13.302 | INFO     | src.policies:collect_trajectories:221 - Episode 871\n",
      "2021-09-07 17:18:13.410 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.411 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 164.0\n",
      "2021-09-07 17:18:13.412 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 164.0\n",
      "2021-09-07 17:18:13.413 | INFO     | src.policies:collect_trajectories:221 - Episode 872\n",
      "2021-09-07 17:18:13.439 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.440 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 102.0\n",
      "2021-09-07 17:18:13.440 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 133.0\n",
      "2021-09-07 17:18:13.441 | WARNING  | src.policies:train:144 - The actual batch size is 266, instead of 200\n",
      "2021-09-07 17:18:13.443 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:13.446 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3181028366088867, 'baseline_loss': 0.5944097638130188, 'total_loss': -0.02089795470237732}\n",
      "2021-09-07 17:18:13.447 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.8195323944091797\n",
      "2021-09-07 17:18:13.447 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4857594966888428\n",
      "2021-09-07 17:18:13.449 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:13.450 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4857594966888428\n",
      "2021-09-07 17:18:13.451 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:13.453 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.35477694869041443, 'baseline_loss': 0.6808744668960571, 'total_loss': -0.014339715242385864}\n",
      "2021-09-07 17:18:13.454 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32208898663520813\n",
      "2021-09-07 17:18:13.455 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3394090533256531\n",
      "2021-09-07 17:18:13.456 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32208898663520813\n",
      "2021-09-07 17:18:13.457 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3394090533256531\n",
      "2021-09-07 17:18:13.458 | INFO     | src.policies:train:123 - Epoch 178 / 800\n",
      "2021-09-07 17:18:13.459 | INFO     | src.policies:collect_trajectories:221 - Episode 873\n",
      "2021-09-07 17:18:13.476 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.477 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 112.0\n",
      "2021-09-07 17:18:13.477 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 112.0\n",
      "2021-09-07 17:18:13.478 | INFO     | src.policies:collect_trajectories:221 - Episode 874\n",
      "2021-09-07 17:18:13.503 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.503 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 136.0\n",
      "2021-09-07 17:18:13.504 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 124.0\n",
      "2021-09-07 17:18:13.505 | WARNING  | src.policies:train:144 - The actual batch size is 248, instead of 200\n",
      "2021-09-07 17:18:13.507 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:13.509 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.30935168266296387, 'baseline_loss': 0.456680566072464, 'total_loss': -0.08101139962673187}\n",
      "2021-09-07 17:18:13.510 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.06480926275253296\n",
      "2021-09-07 17:18:13.511 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7271823287010193\n",
      "2021-09-07 17:18:13.512 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.06480926275253296\n",
      "2021-09-07 17:18:13.513 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:13.514 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:13.515 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3398157060146332, 'baseline_loss': 0.5237187743186951, 'total_loss': -0.07795631885528564}\n",
      "2021-09-07 17:18:13.516 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24448366463184357\n",
      "2021-09-07 17:18:13.517 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5903303027153015\n",
      "2021-09-07 17:18:13.518 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24448366463184357\n",
      "2021-09-07 17:18:13.519 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:13.521 | INFO     | src.policies:train:123 - Epoch 179 / 800\n",
      "2021-09-07 17:18:13.522 | INFO     | src.policies:collect_trajectories:221 - Episode 875\n",
      "2021-09-07 17:18:13.536 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:13.537 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 97.0\n",
      "2021-09-07 17:18:13.537 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 97.0\n",
      "2021-09-07 17:18:13.538 | INFO     | src.policies:collect_trajectories:221 - Episode 876\n",
      "2021-09-07 17:18:13.554 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.555 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 93.0\n",
      "2021-09-07 17:18:13.555 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 95.0\n",
      "2021-09-07 17:18:13.556 | INFO     | src.policies:collect_trajectories:221 - Episode 877\n",
      "2021-09-07 17:18:13.578 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.579 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 142.0\n",
      "2021-09-07 17:18:13.580 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 110.66666666666667\n",
      "2021-09-07 17:18:13.581 | WARNING  | src.policies:train:144 - The actual batch size is 332, instead of 200\n",
      "2021-09-07 17:18:13.583 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:13.585 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5420787334442139, 'baseline_loss': 0.9745094180107117, 'total_loss': -0.05482402443885803}\n",
      "2021-09-07 17:18:13.586 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23773503303527832\n",
      "2021-09-07 17:18:13.587 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5370940566062927\n",
      "2021-09-07 17:18:13.589 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23773503303527832\n",
      "2021-09-07 17:18:13.590 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999989867210388\n",
      "2021-09-07 17:18:13.592 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:13.594 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3984546959400177, 'baseline_loss': 0.7387044429779053, 'total_loss': -0.029102474451065063}\n",
      "2021-09-07 17:18:13.595 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3672710359096527\n",
      "2021-09-07 17:18:13.596 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3278697431087494\n",
      "2021-09-07 17:18:13.597 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3672710359096527\n",
      "2021-09-07 17:18:13.598 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3278697431087494\n",
      "2021-09-07 17:18:13.601 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:13.602 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3861967921257019, 'baseline_loss': 0.61259925365448, 'total_loss': -0.07989716529846191}\n",
      "2021-09-07 17:18:13.604 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21581654250621796\n",
      "2021-09-07 17:18:13.605 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.26783227920532227\n",
      "2021-09-07 17:18:13.606 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21581654250621796\n",
      "2021-09-07 17:18:13.607 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.26783227920532227\n",
      "2021-09-07 17:18:13.609 | INFO     | src.policies:train:123 - Epoch 180 / 800\n",
      "2021-09-07 17:18:13.609 | INFO     | src.policies:collect_trajectories:221 - Episode 878\n",
      "2021-09-07 17:18:13.621 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.622 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 65.0\n",
      "2021-09-07 17:18:13.623 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 65.0\n",
      "2021-09-07 17:18:13.623 | INFO     | src.policies:collect_trajectories:221 - Episode 879\n",
      "2021-09-07 17:18:13.700 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.701 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 129.0\n",
      "2021-09-07 17:18:13.701 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 97.0\n",
      "2021-09-07 17:18:13.702 | INFO     | src.policies:collect_trajectories:221 - Episode 880\n",
      "2021-09-07 17:18:13.727 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.728 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 149.0\n",
      "2021-09-07 17:18:13.728 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 114.33333333333333\n",
      "2021-09-07 17:18:13.729 | WARNING  | src.policies:train:144 - The actual batch size is 343, instead of 200\n",
      "2021-09-07 17:18:13.732 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:13.734 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.333833783864975, 'baseline_loss': 0.5843179225921631, 'total_loss': -0.04167482256889343}\n",
      "2021-09-07 17:18:13.735 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17477641999721527\n",
      "2021-09-07 17:18:13.736 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.740616500377655\n",
      "2021-09-07 17:18:13.737 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17477641999721527\n",
      "2021-09-07 17:18:13.738 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:13.740 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:13.741 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.34390124678611755, 'baseline_loss': 0.6211525797843933, 'total_loss': -0.0333249568939209}\n",
      "2021-09-07 17:18:13.742 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5367838144302368\n",
      "2021-09-07 17:18:13.743 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6438297033309937\n",
      "2021-09-07 17:18:13.744 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:13.745 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:13.746 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:13.748 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4005482494831085, 'baseline_loss': 0.6937179565429688, 'total_loss': -0.053689271211624146}\n",
      "2021-09-07 17:18:13.748 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3027578294277191\n",
      "2021-09-07 17:18:13.749 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.437919557094574\n",
      "2021-09-07 17:18:13.750 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3027578294277191\n",
      "2021-09-07 17:18:13.751 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.437919557094574\n",
      "2021-09-07 17:18:13.753 | INFO     | src.policies:train:123 - Epoch 181 / 800\n",
      "2021-09-07 17:18:13.753 | INFO     | src.policies:collect_trajectories:221 - Episode 881\n",
      "2021-09-07 17:18:13.781 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.782 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 192.0\n",
      "2021-09-07 17:18:13.782 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 192.0\n",
      "2021-09-07 17:18:13.783 | INFO     | src.policies:collect_trajectories:221 - Episode 882\n",
      "2021-09-07 17:18:13.815 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.815 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:13.815 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 196.0\n",
      "2021-09-07 17:18:13.816 | WARNING  | src.policies:train:144 - The actual batch size is 392, instead of 200\n",
      "2021-09-07 17:18:13.927 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:13.928 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3746868073940277, 'baseline_loss': 0.9085872769355774, 'total_loss': 0.07960683107376099}\n",
      "2021-09-07 17:18:13.929 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10403470695018768\n",
      "2021-09-07 17:18:13.930 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0704869031906128\n",
      "2021-09-07 17:18:13.931 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10403470695018768\n",
      "2021-09-07 17:18:13.932 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:13.933 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:13.935 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41085612773895264, 'baseline_loss': 1.002536654472351, 'total_loss': 0.0904121994972229}\n",
      "2021-09-07 17:18:13.936 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.450621098279953\n",
      "2021-09-07 17:18:13.936 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4662057161331177\n",
      "2021-09-07 17:18:13.938 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.450621098279953\n",
      "2021-09-07 17:18:13.939 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:13.940 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:13.941 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45655396580696106, 'baseline_loss': 0.9433301687240601, 'total_loss': 0.01511111855506897}\n",
      "2021-09-07 17:18:13.942 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18211762607097626\n",
      "2021-09-07 17:18:13.942 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9411669969558716\n",
      "2021-09-07 17:18:13.943 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18211762607097626\n",
      "2021-09-07 17:18:13.944 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:13.946 | INFO     | src.policies:train:123 - Epoch 182 / 800\n",
      "2021-09-07 17:18:13.946 | INFO     | src.policies:collect_trajectories:221 - Episode 883\n",
      "2021-09-07 17:18:13.975 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:13.976 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:13.976 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:13.978 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:13.980 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8993765115737915, 'baseline_loss': 3.2497398853302, 'total_loss': 0.7254934310913086}\n",
      "2021-09-07 17:18:13.981 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4636152684688568\n",
      "2021-09-07 17:18:13.982 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.726264476776123\n",
      "2021-09-07 17:18:13.983 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4636152684688568\n",
      "2021-09-07 17:18:13.984 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:13.986 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:13.987 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.9797742366790771, 'baseline_loss': 3.2609353065490723, 'total_loss': 0.650693416595459}\n",
      "2021-09-07 17:18:13.987 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.38944557309150696\n",
      "2021-09-07 17:18:13.988 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.4946088790893555\n",
      "2021-09-07 17:18:13.989 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.38944557309150696\n",
      "2021-09-07 17:18:13.990 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:13.992 | INFO     | src.policies:train:123 - Epoch 183 / 800\n",
      "2021-09-07 17:18:13.992 | INFO     | src.policies:collect_trajectories:221 - Episode 884\n",
      "2021-09-07 17:18:14.009 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.009 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 108.0\n",
      "2021-09-07 17:18:14.010 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 108.0\n",
      "2021-09-07 17:18:14.011 | INFO     | src.policies:collect_trajectories:221 - Episode 885\n",
      "2021-09-07 17:18:14.032 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.033 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 133.0\n",
      "2021-09-07 17:18:14.033 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 120.5\n",
      "2021-09-07 17:18:14.034 | WARNING  | src.policies:train:144 - The actual batch size is 241, instead of 200\n",
      "2021-09-07 17:18:14.036 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:14.038 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.22445979714393616, 'baseline_loss': 0.525646984577179, 'total_loss': 0.03836369514465332}\n",
      "2021-09-07 17:18:14.039 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23071043193340302\n",
      "2021-09-07 17:18:14.040 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4768537282943726\n",
      "2021-09-07 17:18:14.041 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23071043193340302\n",
      "2021-09-07 17:18:14.042 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:14.043 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:14.044 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.26227590441703796, 'baseline_loss': 0.5381380915641785, 'total_loss': 0.0067931413650512695}\n",
      "2021-09-07 17:18:14.045 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16135476529598236\n",
      "2021-09-07 17:18:14.046 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0067640542984009\n",
      "2021-09-07 17:18:14.047 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16135476529598236\n",
      "2021-09-07 17:18:14.048 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:14.049 | INFO     | src.policies:train:123 - Epoch 184 / 800\n",
      "2021-09-07 17:18:14.050 | INFO     | src.policies:collect_trajectories:221 - Episode 886\n",
      "2021-09-07 17:18:14.077 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.077 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 153.0\n",
      "2021-09-07 17:18:14.078 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 153.0\n",
      "2021-09-07 17:18:14.078 | INFO     | src.policies:collect_trajectories:221 - Episode 887\n",
      "2021-09-07 17:18:14.109 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.109 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 193.0\n",
      "2021-09-07 17:18:14.110 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 173.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:14.111 | WARNING  | src.policies:train:144 - The actual batch size is 346, instead of 200\n",
      "2021-09-07 17:18:14.114 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:14.116 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5142244100570679, 'baseline_loss': 0.997755229473114, 'total_loss': -0.015346795320510864}\n",
      "2021-09-07 17:18:14.118 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14766205847263336\n",
      "2021-09-07 17:18:14.119 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1928467750549316\n",
      "2021-09-07 17:18:14.120 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14766205847263336\n",
      "2021-09-07 17:18:14.121 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:14.123 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:14.124 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5965644717216492, 'baseline_loss': 1.1775563955307007, 'total_loss': -0.007786273956298828}\n",
      "2021-09-07 17:18:14.124 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09662890434265137\n",
      "2021-09-07 17:18:14.125 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6472402811050415\n",
      "2021-09-07 17:18:14.126 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09662890434265137\n",
      "2021-09-07 17:18:14.127 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:14.129 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:14.130 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4852854013442993, 'baseline_loss': 1.0228489637374878, 'total_loss': 0.02613908052444458}\n",
      "2021-09-07 17:18:14.131 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24826958775520325\n",
      "2021-09-07 17:18:14.132 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.498923659324646\n",
      "2021-09-07 17:18:14.133 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24826958775520325\n",
      "2021-09-07 17:18:14.134 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:14.135 | INFO     | src.policies:train:123 - Epoch 185 / 800\n",
      "2021-09-07 17:18:14.136 | INFO     | src.policies:collect_trajectories:221 - Episode 888\n",
      "2021-09-07 17:18:14.164 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.165 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 180.0\n",
      "2021-09-07 17:18:14.166 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 180.0\n",
      "2021-09-07 17:18:14.166 | INFO     | src.policies:collect_trajectories:221 - Episode 889\n",
      "2021-09-07 17:18:14.197 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.198 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 177.0\n",
      "2021-09-07 17:18:14.198 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 178.5\n",
      "2021-09-07 17:18:14.199 | WARNING  | src.policies:train:144 - The actual batch size is 357, instead of 200\n",
      "2021-09-07 17:18:14.248 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:14.251 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4126889705657959, 'baseline_loss': 0.5977765321731567, 'total_loss': -0.11380070447921753}\n",
      "2021-09-07 17:18:14.252 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4572332501411438\n",
      "2021-09-07 17:18:14.254 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.16132000088691711\n",
      "2021-09-07 17:18:14.255 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4572332501411438\n",
      "2021-09-07 17:18:14.256 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.16132000088691711\n",
      "2021-09-07 17:18:14.257 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:14.258 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42409420013427734, 'baseline_loss': 0.7274320721626282, 'total_loss': -0.06037816405296326}\n",
      "2021-09-07 17:18:14.260 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22422946989536285\n",
      "2021-09-07 17:18:14.260 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5097913146018982\n",
      "2021-09-07 17:18:14.261 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22422946989536285\n",
      "2021-09-07 17:18:14.263 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:14.264 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:14.265 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3553205132484436, 'baseline_loss': 0.5616552233695984, 'total_loss': -0.07449290156364441}\n",
      "2021-09-07 17:18:14.266 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21267125010490417\n",
      "2021-09-07 17:18:14.267 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2763972580432892\n",
      "2021-09-07 17:18:14.268 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21267125010490417\n",
      "2021-09-07 17:18:14.269 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2763972580432892\n",
      "2021-09-07 17:18:14.271 | INFO     | src.policies:train:123 - Epoch 186 / 800\n",
      "2021-09-07 17:18:14.271 | INFO     | src.policies:collect_trajectories:221 - Episode 890\n",
      "2021-09-07 17:18:14.303 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.304 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:14.305 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:14.307 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:14.309 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8097938299179077, 'baseline_loss': 2.6104140281677246, 'total_loss': 0.4954131841659546}\n",
      "2021-09-07 17:18:14.310 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6181859374046326\n",
      "2021-09-07 17:18:14.311 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.488667011260986\n",
      "2021-09-07 17:18:14.312 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:14.313 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:14.315 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:14.316 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8422456979751587, 'baseline_loss': 2.5794517993927, 'total_loss': 0.4474802017211914}\n",
      "2021-09-07 17:18:14.316 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.183632031083107\n",
      "2021-09-07 17:18:14.317 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.528246879577637\n",
      "2021-09-07 17:18:14.319 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.183632031083107\n",
      "2021-09-07 17:18:14.321 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999999403953552\n",
      "2021-09-07 17:18:14.323 | INFO     | src.policies:train:123 - Epoch 187 / 800\n",
      "2021-09-07 17:18:14.324 | INFO     | src.policies:collect_trajectories:221 - Episode 891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:14.350 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.350 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 162.0\n",
      "2021-09-07 17:18:14.351 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 162.0\n",
      "2021-09-07 17:18:14.351 | INFO     | src.policies:collect_trajectories:221 - Episode 892\n",
      "2021-09-07 17:18:14.385 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.386 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:14.386 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 181.0\n",
      "2021-09-07 17:18:14.387 | WARNING  | src.policies:train:144 - The actual batch size is 362, instead of 200\n",
      "2021-09-07 17:18:14.389 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:14.391 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6169216632843018, 'baseline_loss': 1.7305978536605835, 'total_loss': 0.24837726354599}\n",
      "2021-09-07 17:18:14.391 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25450563430786133\n",
      "2021-09-07 17:18:14.392 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.3645095825195312\n",
      "2021-09-07 17:18:14.393 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25450563430786133\n",
      "2021-09-07 17:18:14.394 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:14.396 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:14.397 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.48373618721961975, 'baseline_loss': 1.2557834386825562, 'total_loss': 0.14415553212165833}\n",
      "2021-09-07 17:18:14.398 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09642954170703888\n",
      "2021-09-07 17:18:14.399 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.618669867515564\n",
      "2021-09-07 17:18:14.400 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09642954170703888\n",
      "2021-09-07 17:18:14.402 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:14.404 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:14.405 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5796761512756348, 'baseline_loss': 1.4512673616409302, 'total_loss': 0.14595752954483032}\n",
      "2021-09-07 17:18:14.406 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1297786980867386\n",
      "2021-09-07 17:18:14.407 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.1260087490081787\n",
      "2021-09-07 17:18:14.408 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1297786980867386\n",
      "2021-09-07 17:18:14.409 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:14.411 | INFO     | src.policies:train:123 - Epoch 188 / 800\n",
      "2021-09-07 17:18:14.411 | INFO     | src.policies:collect_trajectories:221 - Episode 893\n",
      "2021-09-07 17:18:14.442 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.443 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:14.444 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:14.446 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:14.447 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.9403820037841797, 'baseline_loss': 3.161804437637329, 'total_loss': 0.6405202150344849}\n",
      "2021-09-07 17:18:14.448 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.43236687779426575\n",
      "2021-09-07 17:18:14.449 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.1857218742370605\n",
      "2021-09-07 17:18:14.450 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.43236687779426575\n",
      "2021-09-07 17:18:14.451 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:14.452 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:14.453 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7539209723472595, 'baseline_loss': 3.0625836849212646, 'total_loss': 0.7773708701133728}\n",
      "2021-09-07 17:18:14.455 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.28720396757125854\n",
      "2021-09-07 17:18:14.456 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.441601276397705\n",
      "2021-09-07 17:18:14.457 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.28720396757125854\n",
      "2021-09-07 17:18:14.458 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:14.460 | INFO     | src.policies:train:123 - Epoch 189 / 800\n",
      "2021-09-07 17:18:14.460 | INFO     | src.policies:collect_trajectories:221 - Episode 894\n",
      "2021-09-07 17:18:14.479 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.479 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 112.0\n",
      "2021-09-07 17:18:14.480 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 112.0\n",
      "2021-09-07 17:18:14.481 | INFO     | src.policies:collect_trajectories:221 - Episode 895\n",
      "2021-09-07 17:18:14.516 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.517 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:14.517 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 156.0\n",
      "2021-09-07 17:18:14.518 | WARNING  | src.policies:train:144 - The actual batch size is 312, instead of 200\n",
      "2021-09-07 17:18:14.522 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:14.525 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6952765583992004, 'baseline_loss': 1.8375047445297241, 'total_loss': 0.22347581386566162}\n",
      "2021-09-07 17:18:14.526 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.39046478271484375\n",
      "2021-09-07 17:18:14.527 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6984384059906006\n",
      "2021-09-07 17:18:14.528 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.39046478271484375\n",
      "2021-09-07 17:18:14.530 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:14.531 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:14.532 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6973803639411926, 'baseline_loss': 1.9979568719863892, 'total_loss': 0.30159807205200195}\n",
      "2021-09-07 17:18:14.533 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.34029945731163025\n",
      "2021-09-07 17:18:14.534 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.2647578716278076\n",
      "2021-09-07 17:18:14.535 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.34029945731163025\n",
      "2021-09-07 17:18:14.536 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:14.538 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:14.539 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6429897546768188, 'baseline_loss': 1.9115430116653442, 'total_loss': 0.31278175115585327}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:14.541 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4163171648979187\n",
      "2021-09-07 17:18:14.542 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.1008622646331787\n",
      "2021-09-07 17:18:14.543 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4163171648979187\n",
      "2021-09-07 17:18:14.544 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:14.546 | INFO     | src.policies:train:123 - Epoch 190 / 800\n",
      "2021-09-07 17:18:14.546 | INFO     | src.policies:collect_trajectories:221 - Episode 896\n",
      "2021-09-07 17:18:14.562 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.563 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 100.0\n",
      "2021-09-07 17:18:14.563 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 100.0\n",
      "2021-09-07 17:18:14.563 | INFO     | src.policies:collect_trajectories:221 - Episode 897\n",
      "2021-09-07 17:18:14.590 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.590 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 162.0\n",
      "2021-09-07 17:18:14.591 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 131.0\n",
      "2021-09-07 17:18:14.591 | WARNING  | src.policies:train:144 - The actual batch size is 262, instead of 200\n",
      "2021-09-07 17:18:14.594 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:14.595 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.22297824919223785, 'baseline_loss': 0.4278446137905121, 'total_loss': -0.009055942296981812}\n",
      "2021-09-07 17:18:14.596 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6308603882789612\n",
      "2021-09-07 17:18:14.597 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0158007144927979\n",
      "2021-09-07 17:18:14.598 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:14.599 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:14.601 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:14.603 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2407865971326828, 'baseline_loss': 0.43613964319229126, 'total_loss': -0.02271677553653717}\n",
      "2021-09-07 17:18:14.604 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7209612131118774\n",
      "2021-09-07 17:18:14.605 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2037469148635864\n",
      "2021-09-07 17:18:14.606 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:14.607 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:14.609 | INFO     | src.policies:train:123 - Epoch 191 / 800\n",
      "2021-09-07 17:18:14.609 | INFO     | src.policies:collect_trajectories:221 - Episode 898\n",
      "2021-09-07 17:18:14.633 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.633 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 147.0\n",
      "2021-09-07 17:18:14.634 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 147.0\n",
      "2021-09-07 17:18:14.634 | INFO     | src.policies:collect_trajectories:221 - Episode 899\n",
      "2021-09-07 17:18:14.669 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.670 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:14.671 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 173.5\n",
      "2021-09-07 17:18:14.671 | WARNING  | src.policies:train:144 - The actual batch size is 347, instead of 200\n",
      "2021-09-07 17:18:14.673 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:14.675 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3717436194419861, 'baseline_loss': 0.6878923177719116, 'total_loss': -0.027797460556030273}\n",
      "2021-09-07 17:18:14.676 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4865058660507202\n",
      "2021-09-07 17:18:14.677 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5075223445892334\n",
      "2021-09-07 17:18:14.678 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4865058660507202\n",
      "2021-09-07 17:18:14.679 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:14.681 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:14.683 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2697092890739441, 'baseline_loss': 0.5954751372337341, 'total_loss': 0.028028279542922974}\n",
      "2021-09-07 17:18:14.684 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.28702011704444885\n",
      "2021-09-07 17:18:14.685 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9336389303207397\n",
      "2021-09-07 17:18:14.686 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.28702011704444885\n",
      "2021-09-07 17:18:14.688 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:14.689 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:14.690 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3296780288219452, 'baseline_loss': 0.6513750553131104, 'total_loss': -0.003990501165390015}\n",
      "2021-09-07 17:18:14.691 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15442314743995667\n",
      "2021-09-07 17:18:14.692 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4031360149383545\n",
      "2021-09-07 17:18:14.693 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15442314743995667\n",
      "2021-09-07 17:18:14.694 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4031360149383545\n",
      "2021-09-07 17:18:14.696 | INFO     | src.policies:train:123 - Epoch 192 / 800\n",
      "2021-09-07 17:18:14.696 | INFO     | src.policies:collect_trajectories:221 - Episode 900\n",
      "2021-09-07 17:18:14.723 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.724 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 172.0\n",
      "2021-09-07 17:18:14.724 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 172.0\n",
      "2021-09-07 17:18:14.725 | INFO     | src.policies:collect_trajectories:221 - Episode 901\n",
      "2021-09-07 17:18:14.815 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.816 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:14.816 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 186.0\n",
      "2021-09-07 17:18:14.817 | WARNING  | src.policies:train:144 - The actual batch size is 372, instead of 200\n",
      "2021-09-07 17:18:14.821 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:14.823 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4972296953201294, 'baseline_loss': 1.176486611366272, 'total_loss': 0.09101361036300659}\n",
      "2021-09-07 17:18:14.824 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19504965841770172\n",
      "2021-09-07 17:18:14.825 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.068860411643982\n",
      "2021-09-07 17:18:14.826 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19504965841770172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:14.827 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:14.829 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:14.830 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4271913766860962, 'baseline_loss': 1.1676315069198608, 'total_loss': 0.15662437677383423}\n",
      "2021-09-07 17:18:14.831 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24686473608016968\n",
      "2021-09-07 17:18:14.832 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0573545694351196\n",
      "2021-09-07 17:18:14.833 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24686473608016968\n",
      "2021-09-07 17:18:14.834 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:14.835 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:14.837 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4849987328052521, 'baseline_loss': 1.1461542844772339, 'total_loss': 0.08807840943336487}\n",
      "2021-09-07 17:18:14.837 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5842149257659912\n",
      "2021-09-07 17:18:14.838 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1939916610717773\n",
      "2021-09-07 17:18:14.840 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:14.841 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:14.843 | INFO     | src.policies:train:123 - Epoch 193 / 800\n",
      "2021-09-07 17:18:14.843 | INFO     | src.policies:collect_trajectories:221 - Episode 902\n",
      "2021-09-07 17:18:14.870 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.870 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 171.0\n",
      "2021-09-07 17:18:14.871 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 171.0\n",
      "2021-09-07 17:18:14.871 | INFO     | src.policies:collect_trajectories:221 - Episode 903\n",
      "2021-09-07 17:18:14.901 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.902 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 181.0\n",
      "2021-09-07 17:18:14.902 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 176.0\n",
      "2021-09-07 17:18:14.903 | WARNING  | src.policies:train:144 - The actual batch size is 352, instead of 200\n",
      "2021-09-07 17:18:14.905 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:14.907 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2829933166503906, 'baseline_loss': 0.5229290127754211, 'total_loss': -0.021528810262680054}\n",
      "2021-09-07 17:18:14.908 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2663409411907196\n",
      "2021-09-07 17:18:14.909 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.38156574964523315\n",
      "2021-09-07 17:18:14.910 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2663409411907196\n",
      "2021-09-07 17:18:14.911 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.38156574964523315\n",
      "2021-09-07 17:18:14.912 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:14.913 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3889000415802002, 'baseline_loss': 0.565873920917511, 'total_loss': -0.1059630811214447}\n",
      "2021-09-07 17:18:14.914 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1616114228963852\n",
      "2021-09-07 17:18:14.915 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.43680813908576965\n",
      "2021-09-07 17:18:14.916 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1616114228963852\n",
      "2021-09-07 17:18:14.917 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.43680813908576965\n",
      "2021-09-07 17:18:14.918 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:14.919 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3256376385688782, 'baseline_loss': 0.5386861562728882, 'total_loss': -0.05629456043243408}\n",
      "2021-09-07 17:18:14.920 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.36087897419929504\n",
      "2021-09-07 17:18:14.921 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9126508235931396\n",
      "2021-09-07 17:18:14.922 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.36087897419929504\n",
      "2021-09-07 17:18:14.924 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:14.926 | INFO     | src.policies:train:123 - Epoch 194 / 800\n",
      "2021-09-07 17:18:14.927 | INFO     | src.policies:collect_trajectories:221 - Episode 904\n",
      "2021-09-07 17:18:14.952 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.952 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 174.0\n",
      "2021-09-07 17:18:14.953 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 174.0\n",
      "2021-09-07 17:18:14.953 | INFO     | src.policies:collect_trajectories:221 - Episode 905\n",
      "2021-09-07 17:18:14.990 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:14.990 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:14.991 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 187.0\n",
      "2021-09-07 17:18:14.991 | WARNING  | src.policies:train:144 - The actual batch size is 374, instead of 200\n",
      "2021-09-07 17:18:14.994 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:14.996 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5596256852149963, 'baseline_loss': 1.62565016746521, 'total_loss': 0.25319939851760864}\n",
      "2021-09-07 17:18:14.997 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 1.145135521888733\n",
      "2021-09-07 17:18:14.998 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7976490259170532\n",
      "2021-09-07 17:18:14.999 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:15.000 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:15.002 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:15.003 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4062126576900482, 'baseline_loss': 1.1770962476730347, 'total_loss': 0.18233546614646912}\n",
      "2021-09-07 17:18:15.004 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5000375509262085\n",
      "2021-09-07 17:18:15.005 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9772164225578308\n",
      "2021-09-07 17:18:15.006 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:15.008 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:15.010 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:15.011 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4742165803909302, 'baseline_loss': 1.243743658065796, 'total_loss': 0.14765524864196777}\n",
      "2021-09-07 17:18:15.013 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4099530279636383\n",
      "2021-09-07 17:18:15.014 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7627500295639038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:15.016 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4099530279636383\n",
      "2021-09-07 17:18:15.017 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:15.020 | INFO     | src.policies:train:123 - Epoch 195 / 800\n",
      "2021-09-07 17:18:15.020 | INFO     | src.policies:collect_trajectories:221 - Episode 906\n",
      "2021-09-07 17:18:15.056 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.057 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 197.0\n",
      "2021-09-07 17:18:15.058 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 197.0\n",
      "2021-09-07 17:18:15.059 | INFO     | src.policies:collect_trajectories:221 - Episode 907\n",
      "2021-09-07 17:18:15.090 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.091 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:15.091 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 198.5\n",
      "2021-09-07 17:18:15.092 | WARNING  | src.policies:train:144 - The actual batch size is 397, instead of 200\n",
      "2021-09-07 17:18:15.095 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:15.097 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.39457929134368896, 'baseline_loss': 0.577572226524353, 'total_loss': -0.10579317808151245}\n",
      "2021-09-07 17:18:15.098 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.34013137221336365\n",
      "2021-09-07 17:18:15.099 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7764909863471985\n",
      "2021-09-07 17:18:15.100 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.34013137221336365\n",
      "2021-09-07 17:18:15.101 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:15.102 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:15.103 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4892925024032593, 'baseline_loss': 0.7126134634017944, 'total_loss': -0.13298577070236206}\n",
      "2021-09-07 17:18:15.104 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1696329563856125\n",
      "2021-09-07 17:18:15.105 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3940107524394989\n",
      "2021-09-07 17:18:15.107 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1696329563856125\n",
      "2021-09-07 17:18:15.108 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3940107524394989\n",
      "2021-09-07 17:18:15.109 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:15.111 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42605116963386536, 'baseline_loss': 0.6905274391174316, 'total_loss': -0.08078745007514954}\n",
      "2021-09-07 17:18:15.112 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2155904918909073\n",
      "2021-09-07 17:18:15.113 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5857322812080383\n",
      "2021-09-07 17:18:15.114 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2155904918909073\n",
      "2021-09-07 17:18:15.115 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:15.116 | INFO     | src.policies:train:123 - Epoch 196 / 800\n",
      "2021-09-07 17:18:15.117 | INFO     | src.policies:collect_trajectories:221 - Episode 908\n",
      "2021-09-07 17:18:15.147 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.148 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 174.0\n",
      "2021-09-07 17:18:15.148 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 174.0\n",
      "2021-09-07 17:18:15.149 | INFO     | src.policies:collect_trajectories:221 - Episode 909\n",
      "2021-09-07 17:18:15.168 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.169 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 103.0\n",
      "2021-09-07 17:18:15.169 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 138.5\n",
      "2021-09-07 17:18:15.170 | WARNING  | src.policies:train:144 - The actual batch size is 277, instead of 200\n",
      "2021-09-07 17:18:15.171 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:15.174 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.33904916048049927, 'baseline_loss': 0.6413623094558716, 'total_loss': -0.018368005752563477}\n",
      "2021-09-07 17:18:15.175 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.27986836433410645\n",
      "2021-09-07 17:18:15.176 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0139961242675781\n",
      "2021-09-07 17:18:15.177 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.27986836433410645\n",
      "2021-09-07 17:18:15.178 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:15.179 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:15.182 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40793460607528687, 'baseline_loss': 0.8111204504966736, 'total_loss': -0.0023743808269500732}\n",
      "2021-09-07 17:18:15.183 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2999483644962311\n",
      "2021-09-07 17:18:15.184 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6044520139694214\n",
      "2021-09-07 17:18:15.186 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2999483644962311\n",
      "2021-09-07 17:18:15.187 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:15.189 | INFO     | src.policies:train:123 - Epoch 197 / 800\n",
      "2021-09-07 17:18:15.189 | INFO     | src.policies:collect_trajectories:221 - Episode 910\n",
      "2021-09-07 17:18:15.205 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.206 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 88.0\n",
      "2021-09-07 17:18:15.207 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 88.0\n",
      "2021-09-07 17:18:15.207 | INFO     | src.policies:collect_trajectories:221 - Episode 911\n",
      "2021-09-07 17:18:15.227 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.228 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 106.0\n",
      "2021-09-07 17:18:15.228 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 97.0\n",
      "2021-09-07 17:18:15.229 | INFO     | src.policies:collect_trajectories:221 - Episode 912\n",
      "2021-09-07 17:18:15.251 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.252 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 124.0\n",
      "2021-09-07 17:18:15.252 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 106.0\n",
      "2021-09-07 17:18:15.253 | WARNING  | src.policies:train:144 - The actual batch size is 318, instead of 200\n",
      "2021-09-07 17:18:15.256 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:15.257 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3677944540977478, 'baseline_loss': 0.7875887155532837, 'total_loss': 0.025999903678894043}\n",
      "2021-09-07 17:18:15.258 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2836533486843109\n",
      "2021-09-07 17:18:15.260 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8144040703773499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:15.261 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2836533486843109\n",
      "2021-09-07 17:18:15.262 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:15.263 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:15.265 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3039223253726959, 'baseline_loss': 0.7863755822181702, 'total_loss': 0.08926546573638916}\n",
      "2021-09-07 17:18:15.266 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3370254933834076\n",
      "2021-09-07 17:18:15.267 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9874021410942078\n",
      "2021-09-07 17:18:15.268 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3370254933834076\n",
      "2021-09-07 17:18:15.269 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:15.270 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:15.271 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42397603392601013, 'baseline_loss': 0.8165839910507202, 'total_loss': -0.015684038400650024}\n",
      "2021-09-07 17:18:15.272 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.27080923318862915\n",
      "2021-09-07 17:18:15.273 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6279633641242981\n",
      "2021-09-07 17:18:15.274 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.27080923318862915\n",
      "2021-09-07 17:18:15.275 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:15.276 | INFO     | src.policies:train:123 - Epoch 198 / 800\n",
      "2021-09-07 17:18:15.277 | INFO     | src.policies:collect_trajectories:221 - Episode 913\n",
      "2021-09-07 17:18:15.293 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.294 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 101.0\n",
      "2021-09-07 17:18:15.295 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 101.0\n",
      "2021-09-07 17:18:15.295 | INFO     | src.policies:collect_trajectories:221 - Episode 914\n",
      "2021-09-07 17:18:15.367 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.367 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 92.0\n",
      "2021-09-07 17:18:15.368 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 96.5\n",
      "2021-09-07 17:18:15.368 | INFO     | src.policies:collect_trajectories:221 - Episode 915\n",
      "2021-09-07 17:18:15.403 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.404 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:15.404 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 131.0\n",
      "2021-09-07 17:18:15.405 | WARNING  | src.policies:train:144 - The actual batch size is 393, instead of 200\n",
      "2021-09-07 17:18:15.408 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:15.410 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.39637330174446106, 'baseline_loss': 1.0521053075790405, 'total_loss': 0.1296793520450592}\n",
      "2021-09-07 17:18:15.411 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3035300672054291\n",
      "2021-09-07 17:18:15.412 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0754367113113403\n",
      "2021-09-07 17:18:15.413 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3035300672054291\n",
      "2021-09-07 17:18:15.414 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:15.415 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:15.416 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.46157538890838623, 'baseline_loss': 1.1483752727508545, 'total_loss': 0.11261224746704102}\n",
      "2021-09-07 17:18:15.417 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19059497117996216\n",
      "2021-09-07 17:18:15.418 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1886353492736816\n",
      "2021-09-07 17:18:15.420 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19059497117996216\n",
      "2021-09-07 17:18:15.421 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:15.422 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:15.424 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43009018898010254, 'baseline_loss': 1.0734819173812866, 'total_loss': 0.10665076971054077}\n",
      "2021-09-07 17:18:15.426 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15514357388019562\n",
      "2021-09-07 17:18:15.427 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6731663942337036\n",
      "2021-09-07 17:18:15.428 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15514357388019562\n",
      "2021-09-07 17:18:15.429 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:15.430 | INFO     | src.policies:train:123 - Epoch 199 / 800\n",
      "2021-09-07 17:18:15.431 | INFO     | src.policies:collect_trajectories:221 - Episode 916\n",
      "2021-09-07 17:18:15.462 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.462 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:15.463 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:15.465 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:15.467 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5642690062522888, 'baseline_loss': 1.7053115367889404, 'total_loss': 0.2883867621421814}\n",
      "2021-09-07 17:18:15.468 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.43800950050354004\n",
      "2021-09-07 17:18:15.469 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7967772483825684\n",
      "2021-09-07 17:18:15.471 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.43800950050354004\n",
      "2021-09-07 17:18:15.473 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:15.474 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:15.475 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3854423463344574, 'baseline_loss': 1.652382493019104, 'total_loss': 0.4407489001750946}\n",
      "2021-09-07 17:18:15.476 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32534897327423096\n",
      "2021-09-07 17:18:15.477 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9751031398773193\n",
      "2021-09-07 17:18:15.478 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32534897327423096\n",
      "2021-09-07 17:18:15.479 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:15.481 | INFO     | src.policies:train:123 - Epoch 200 / 800\n",
      "2021-09-07 17:18:15.481 | INFO     | src.policies:collect_trajectories:221 - Episode 917\n",
      "2021-09-07 17:18:15.503 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.504 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 136.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:15.504 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 136.0\n",
      "2021-09-07 17:18:15.505 | INFO     | src.policies:collect_trajectories:221 - Episode 918\n",
      "2021-09-07 17:18:15.527 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.528 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 122.0\n",
      "2021-09-07 17:18:15.528 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 129.0\n",
      "2021-09-07 17:18:15.529 | WARNING  | src.policies:train:144 - The actual batch size is 258, instead of 200\n",
      "2021-09-07 17:18:15.531 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:15.533 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6269293427467346, 'baseline_loss': 1.3814196586608887, 'total_loss': 0.06378048658370972}\n",
      "2021-09-07 17:18:15.534 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31059348583221436\n",
      "2021-09-07 17:18:15.535 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.014424204826355\n",
      "2021-09-07 17:18:15.536 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31059348583221436\n",
      "2021-09-07 17:18:15.537 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:15.538 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:15.539 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5357824563980103, 'baseline_loss': 1.7693231105804443, 'total_loss': 0.3488790988922119}\n",
      "2021-09-07 17:18:15.541 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5208597183227539\n",
      "2021-09-07 17:18:15.542 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7571313381195068\n",
      "2021-09-07 17:18:15.543 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:15.545 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:15.546 | INFO     | src.policies:train:123 - Epoch 201 / 800\n",
      "2021-09-07 17:18:15.547 | INFO     | src.policies:collect_trajectories:221 - Episode 919\n",
      "2021-09-07 17:18:15.567 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.568 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 135.0\n",
      "2021-09-07 17:18:15.569 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 135.0\n",
      "2021-09-07 17:18:15.569 | INFO     | src.policies:collect_trajectories:221 - Episode 920\n",
      "2021-09-07 17:18:15.589 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.590 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 132.0\n",
      "2021-09-07 17:18:15.590 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 133.5\n",
      "2021-09-07 17:18:15.591 | WARNING  | src.policies:train:144 - The actual batch size is 267, instead of 200\n",
      "2021-09-07 17:18:15.594 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:15.595 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24238604307174683, 'baseline_loss': 0.4971095025539398, 'total_loss': 0.0061687082052230835}\n",
      "2021-09-07 17:18:15.596 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10841234773397446\n",
      "2021-09-07 17:18:15.597 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8020119667053223\n",
      "2021-09-07 17:18:15.598 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10841234773397446\n",
      "2021-09-07 17:18:15.599 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:15.601 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:15.602 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.34721288084983826, 'baseline_loss': 0.5746859908103943, 'total_loss': -0.05986988544464111}\n",
      "2021-09-07 17:18:15.604 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31506359577178955\n",
      "2021-09-07 17:18:15.605 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6490912437438965\n",
      "2021-09-07 17:18:15.607 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31506359577178955\n",
      "2021-09-07 17:18:15.608 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:15.609 | INFO     | src.policies:train:123 - Epoch 202 / 800\n",
      "2021-09-07 17:18:15.610 | INFO     | src.policies:collect_trajectories:221 - Episode 921\n",
      "2021-09-07 17:18:15.641 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.642 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:15.642 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:15.645 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:15.646 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3517252802848816, 'baseline_loss': 1.3187865018844604, 'total_loss': 0.30766797065734863}\n",
      "2021-09-07 17:18:15.647 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1304619014263153\n",
      "2021-09-07 17:18:15.648 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.772966980934143\n",
      "2021-09-07 17:18:15.650 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1304619014263153\n",
      "2021-09-07 17:18:15.652 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:15.653 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:15.655 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4729546010494232, 'baseline_loss': 1.39556086063385, 'total_loss': 0.22482582926750183}\n",
      "2021-09-07 17:18:15.656 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5841947197914124\n",
      "2021-09-07 17:18:15.656 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.766325831413269\n",
      "2021-09-07 17:18:15.658 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:15.659 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:15.661 | INFO     | src.policies:train:123 - Epoch 203 / 800\n",
      "2021-09-07 17:18:15.661 | INFO     | src.policies:collect_trajectories:221 - Episode 922\n",
      "2021-09-07 17:18:15.685 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.686 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 141.0\n",
      "2021-09-07 17:18:15.686 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 141.0\n",
      "2021-09-07 17:18:15.686 | INFO     | src.policies:collect_trajectories:221 - Episode 923\n",
      "2021-09-07 17:18:15.716 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.717 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 192.0\n",
      "2021-09-07 17:18:15.717 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 166.5\n",
      "2021-09-07 17:18:15.718 | WARNING  | src.policies:train:144 - The actual batch size is 333, instead of 200\n",
      "2021-09-07 17:18:15.721 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:15.724 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24614094197750092, 'baseline_loss': 0.510772168636322, 'total_loss': 0.009245142340660095}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:15.725 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25861579179763794\n",
      "2021-09-07 17:18:15.727 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1243698596954346\n",
      "2021-09-07 17:18:15.728 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25861579179763794\n",
      "2021-09-07 17:18:15.730 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:15.731 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:15.732 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2396404892206192, 'baseline_loss': 0.513653576374054, 'total_loss': 0.017186298966407776}\n",
      "2021-09-07 17:18:15.733 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.307763010263443\n",
      "2021-09-07 17:18:15.734 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9292632341384888\n",
      "2021-09-07 17:18:15.735 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.307763010263443\n",
      "2021-09-07 17:18:15.736 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:15.738 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:15.739 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32169055938720703, 'baseline_loss': 0.5019049644470215, 'total_loss': -0.07073807716369629}\n",
      "2021-09-07 17:18:15.741 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17525681853294373\n",
      "2021-09-07 17:18:15.742 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0705595016479492\n",
      "2021-09-07 17:18:15.743 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17525681853294373\n",
      "2021-09-07 17:18:15.744 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:15.746 | INFO     | src.policies:train:123 - Epoch 204 / 800\n",
      "2021-09-07 17:18:15.746 | INFO     | src.policies:collect_trajectories:221 - Episode 924\n",
      "2021-09-07 17:18:15.768 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.769 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 120.0\n",
      "2021-09-07 17:18:15.770 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 120.0\n",
      "2021-09-07 17:18:15.770 | INFO     | src.policies:collect_trajectories:221 - Episode 925\n",
      "2021-09-07 17:18:15.800 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.801 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 182.0\n",
      "2021-09-07 17:18:15.801 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 151.0\n",
      "2021-09-07 17:18:15.802 | WARNING  | src.policies:train:144 - The actual batch size is 302, instead of 200\n",
      "2021-09-07 17:18:15.805 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:15.808 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.14949528872966766, 'baseline_loss': 0.4691770672798157, 'total_loss': 0.08509324491024017}\n",
      "2021-09-07 17:18:15.810 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2744465470314026\n",
      "2021-09-07 17:18:15.811 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1836057901382446\n",
      "2021-09-07 17:18:15.812 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2744465470314026\n",
      "2021-09-07 17:18:15.813 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:15.815 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:15.816 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.19546149671077728, 'baseline_loss': 0.44577398896217346, 'total_loss': 0.027425497770309448}\n",
      "2021-09-07 17:18:15.817 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2838612496852875\n",
      "2021-09-07 17:18:15.818 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1656144857406616\n",
      "2021-09-07 17:18:15.819 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2838612496852875\n",
      "2021-09-07 17:18:15.820 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:15.821 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:15.823 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.18769755959510803, 'baseline_loss': 0.4260493516921997, 'total_loss': 0.02532711625099182}\n",
      "2021-09-07 17:18:15.824 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14117765426635742\n",
      "2021-09-07 17:18:15.825 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1861096620559692\n",
      "2021-09-07 17:18:15.826 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14117765426635742\n",
      "2021-09-07 17:18:15.828 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:15.830 | INFO     | src.policies:train:123 - Epoch 205 / 800\n",
      "2021-09-07 17:18:15.830 | INFO     | src.policies:collect_trajectories:221 - Episode 926\n",
      "2021-09-07 17:18:15.866 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:15.867 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:15.868 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:15.869 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:16.119 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4904901385307312, 'baseline_loss': 1.1675541400909424, 'total_loss': 0.09328693151473999}\n",
      "2021-09-07 17:18:16.120 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24065878987312317\n",
      "2021-09-07 17:18:16.123 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5997111797332764\n",
      "2021-09-07 17:18:16.124 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24065878987312317\n",
      "2021-09-07 17:18:16.125 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:16.127 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:16.128 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6060970425605774, 'baseline_loss': 1.4786971807479858, 'total_loss': 0.13325154781341553}\n",
      "2021-09-07 17:18:16.130 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.46199896931648254\n",
      "2021-09-07 17:18:16.131 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5778212547302246\n",
      "2021-09-07 17:18:16.132 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.46199896931648254\n",
      "2021-09-07 17:18:16.133 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:16.134 | INFO     | src.policies:train:123 - Epoch 206 / 800\n",
      "2021-09-07 17:18:16.135 | INFO     | src.policies:collect_trajectories:221 - Episode 927\n",
      "2021-09-07 17:18:16.148 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.149 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 75.0\n",
      "2021-09-07 17:18:16.149 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 75.0\n",
      "2021-09-07 17:18:16.150 | INFO     | src.policies:collect_trajectories:221 - Episode 928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:16.183 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.184 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:16.185 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 137.5\n",
      "2021-09-07 17:18:16.185 | WARNING  | src.policies:train:144 - The actual batch size is 275, instead of 200\n",
      "2021-09-07 17:18:16.188 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:16.190 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4739943742752075, 'baseline_loss': 1.0751060247421265, 'total_loss': 0.06355863809585571}\n",
      "2021-09-07 17:18:16.191 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08597654849290848\n",
      "2021-09-07 17:18:16.192 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9974623918533325\n",
      "2021-09-07 17:18:16.194 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08597654849290848\n",
      "2021-09-07 17:18:16.195 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:16.196 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:16.198 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5973312258720398, 'baseline_loss': 1.2344369888305664, 'total_loss': 0.019887268543243408}\n",
      "2021-09-07 17:18:16.199 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.38242578506469727\n",
      "2021-09-07 17:18:16.201 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0578378438949585\n",
      "2021-09-07 17:18:16.202 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.38242578506469727\n",
      "2021-09-07 17:18:16.203 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:16.205 | INFO     | src.policies:train:123 - Epoch 207 / 800\n",
      "2021-09-07 17:18:16.205 | INFO     | src.policies:collect_trajectories:221 - Episode 929\n",
      "2021-09-07 17:18:16.238 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.239 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:16.239 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:16.243 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:16.246 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32638266682624817, 'baseline_loss': 0.5999193787574768, 'total_loss': -0.026422977447509766}\n",
      "2021-09-07 17:18:16.247 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25042107701301575\n",
      "2021-09-07 17:18:16.248 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7251551747322083\n",
      "2021-09-07 17:18:16.249 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25042107701301575\n",
      "2021-09-07 17:18:16.250 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:16.251 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:16.253 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36675816774368286, 'baseline_loss': 0.5452488660812378, 'total_loss': -0.09413373470306396}\n",
      "2021-09-07 17:18:16.254 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.44168663024902344\n",
      "2021-09-07 17:18:16.255 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5583593845367432\n",
      "2021-09-07 17:18:16.256 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.44168663024902344\n",
      "2021-09-07 17:18:16.257 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:16.258 | INFO     | src.policies:train:123 - Epoch 208 / 800\n",
      "2021-09-07 17:18:16.259 | INFO     | src.policies:collect_trajectories:221 - Episode 930\n",
      "2021-09-07 17:18:16.278 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.279 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 114.0\n",
      "2021-09-07 17:18:16.279 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 114.0\n",
      "2021-09-07 17:18:16.280 | INFO     | src.policies:collect_trajectories:221 - Episode 931\n",
      "2021-09-07 17:18:16.308 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.309 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 142.0\n",
      "2021-09-07 17:18:16.309 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 128.0\n",
      "2021-09-07 17:18:16.310 | WARNING  | src.policies:train:144 - The actual batch size is 256, instead of 200\n",
      "2021-09-07 17:18:16.312 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:16.314 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23921537399291992, 'baseline_loss': 0.46783646941185, 'total_loss': -0.005297139286994934}\n",
      "2021-09-07 17:18:16.315 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.06083083152770996\n",
      "2021-09-07 17:18:16.316 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.292921543121338\n",
      "2021-09-07 17:18:16.317 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.06083083152770996\n",
      "2021-09-07 17:18:16.318 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:16.319 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:16.322 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.296978622674942, 'baseline_loss': 0.4942617416381836, 'total_loss': -0.04984775185585022}\n",
      "2021-09-07 17:18:16.323 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.554962158203125\n",
      "2021-09-07 17:18:16.325 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9049897193908691\n",
      "2021-09-07 17:18:16.326 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:16.327 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:16.329 | INFO     | src.policies:train:123 - Epoch 209 / 800\n",
      "2021-09-07 17:18:16.329 | INFO     | src.policies:collect_trajectories:221 - Episode 932\n",
      "2021-09-07 17:18:16.351 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.351 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 122.0\n",
      "2021-09-07 17:18:16.352 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 122.0\n",
      "2021-09-07 17:18:16.352 | INFO     | src.policies:collect_trajectories:221 - Episode 933\n",
      "2021-09-07 17:18:16.371 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.371 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 101.0\n",
      "2021-09-07 17:18:16.372 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 111.5\n",
      "2021-09-07 17:18:16.373 | WARNING  | src.policies:train:144 - The actual batch size is 223, instead of 200\n",
      "2021-09-07 17:18:16.375 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:16.377 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.33467426896095276, 'baseline_loss': 0.5135596990585327, 'total_loss': -0.0778944194316864}\n",
      "2021-09-07 17:18:16.378 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3526490032672882\n",
      "2021-09-07 17:18:16.379 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0056016445159912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:16.380 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3526490032672882\n",
      "2021-09-07 17:18:16.381 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:16.383 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:16.384 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.287180095911026, 'baseline_loss': 0.48187899589538574, 'total_loss': -0.04624059796333313}\n",
      "2021-09-07 17:18:16.385 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25190550088882446\n",
      "2021-09-07 17:18:16.386 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9173980355262756\n",
      "2021-09-07 17:18:16.387 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25190550088882446\n",
      "2021-09-07 17:18:16.389 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:16.390 | INFO     | src.policies:train:123 - Epoch 210 / 800\n",
      "2021-09-07 17:18:16.391 | INFO     | src.policies:collect_trajectories:221 - Episode 934\n",
      "2021-09-07 17:18:16.425 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.426 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:16.426 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:16.428 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:16.430 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41543468832969666, 'baseline_loss': 0.6759471893310547, 'total_loss': -0.07746109366416931}\n",
      "2021-09-07 17:18:16.431 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.26135551929473877\n",
      "2021-09-07 17:18:16.432 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4950006604194641\n",
      "2021-09-07 17:18:16.433 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.26135551929473877\n",
      "2021-09-07 17:18:16.434 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4950006604194641\n",
      "2021-09-07 17:18:16.436 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:16.437 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.38406088948249817, 'baseline_loss': 0.6640651226043701, 'total_loss': -0.05202832818031311}\n",
      "2021-09-07 17:18:16.437 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17284642159938812\n",
      "2021-09-07 17:18:16.438 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5275179743766785\n",
      "2021-09-07 17:18:16.440 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17284642159938812\n",
      "2021-09-07 17:18:16.441 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:16.443 | INFO     | src.policies:train:123 - Epoch 211 / 800\n",
      "2021-09-07 17:18:16.444 | INFO     | src.policies:collect_trajectories:221 - Episode 935\n",
      "2021-09-07 17:18:16.469 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.470 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 155.0\n",
      "2021-09-07 17:18:16.471 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 155.0\n",
      "2021-09-07 17:18:16.471 | INFO     | src.policies:collect_trajectories:221 - Episode 936\n",
      "2021-09-07 17:18:16.489 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.490 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 108.0\n",
      "2021-09-07 17:18:16.490 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 131.5\n",
      "2021-09-07 17:18:16.491 | WARNING  | src.policies:train:144 - The actual batch size is 263, instead of 200\n",
      "2021-09-07 17:18:16.493 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:16.495 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2225290834903717, 'baseline_loss': 0.4186488091945648, 'total_loss': -0.013204678893089294}\n",
      "2021-09-07 17:18:16.495 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30504417419433594\n",
      "2021-09-07 17:18:16.496 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1163419485092163\n",
      "2021-09-07 17:18:16.497 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30504417419433594\n",
      "2021-09-07 17:18:16.498 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:16.500 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:16.502 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20375294983386993, 'baseline_loss': 0.4398971199989319, 'total_loss': 0.01619561016559601}\n",
      "2021-09-07 17:18:16.503 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.52544766664505\n",
      "2021-09-07 17:18:16.504 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.094181776046753\n",
      "2021-09-07 17:18:16.505 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:16.507 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:16.508 | INFO     | src.policies:train:123 - Epoch 212 / 800\n",
      "2021-09-07 17:18:16.508 | INFO     | src.policies:collect_trajectories:221 - Episode 937\n",
      "2021-09-07 17:18:16.538 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.539 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 179.0\n",
      "2021-09-07 17:18:16.539 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 179.0\n",
      "2021-09-07 17:18:16.540 | INFO     | src.policies:collect_trajectories:221 - Episode 938\n",
      "2021-09-07 17:18:16.570 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.571 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 170.0\n",
      "2021-09-07 17:18:16.571 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 174.5\n",
      "2021-09-07 17:18:16.572 | WARNING  | src.policies:train:144 - The actual batch size is 349, instead of 200\n",
      "2021-09-07 17:18:16.575 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:16.577 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5131479501724243, 'baseline_loss': 0.9198031425476074, 'total_loss': -0.053246378898620605}\n",
      "2021-09-07 17:18:16.577 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24546533823013306\n",
      "2021-09-07 17:18:16.578 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7341082692146301\n",
      "2021-09-07 17:18:16.580 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24546533823013306\n",
      "2021-09-07 17:18:16.582 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:16.583 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:16.584 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45158708095550537, 'baseline_loss': 0.776397168636322, 'total_loss': -0.06338849663734436}\n",
      "2021-09-07 17:18:16.586 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3906559944152832\n",
      "2021-09-07 17:18:16.587 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4654307961463928\n",
      "2021-09-07 17:18:16.588 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3906559944152832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:16.589 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4654307961463928\n",
      "2021-09-07 17:18:16.590 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:16.592 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45172202587127686, 'baseline_loss': 1.060646414756775, 'total_loss': 0.0786011815071106}\n",
      "2021-09-07 17:18:16.593 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.45834168791770935\n",
      "2021-09-07 17:18:16.593 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4597052335739136\n",
      "2021-09-07 17:18:16.594 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.45834168791770935\n",
      "2021-09-07 17:18:16.595 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:16.597 | INFO     | src.policies:train:123 - Epoch 213 / 800\n",
      "2021-09-07 17:18:16.597 | INFO     | src.policies:collect_trajectories:221 - Episode 939\n",
      "2021-09-07 17:18:16.676 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.676 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 181.0\n",
      "2021-09-07 17:18:16.677 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 181.0\n",
      "2021-09-07 17:18:16.677 | INFO     | src.policies:collect_trajectories:221 - Episode 940\n",
      "2021-09-07 17:18:16.711 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.711 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:16.712 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 190.5\n",
      "2021-09-07 17:18:16.712 | WARNING  | src.policies:train:144 - The actual batch size is 381, instead of 200\n",
      "2021-09-07 17:18:16.715 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:16.717 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.47072508931159973, 'baseline_loss': 1.0185778141021729, 'total_loss': 0.038563817739486694}\n",
      "2021-09-07 17:18:16.718 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32726216316223145\n",
      "2021-09-07 17:18:16.719 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8832437992095947\n",
      "2021-09-07 17:18:16.721 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32726216316223145\n",
      "2021-09-07 17:18:16.722 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:16.724 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:16.725 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36850571632385254, 'baseline_loss': 0.8884212374687195, 'total_loss': 0.0757049024105072}\n",
      "2021-09-07 17:18:16.727 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20584797859191895\n",
      "2021-09-07 17:18:16.728 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7376307845115662\n",
      "2021-09-07 17:18:16.729 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20584797859191895\n",
      "2021-09-07 17:18:16.730 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:16.731 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:16.732 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5503525137901306, 'baseline_loss': 1.1026923656463623, 'total_loss': 0.0009936690330505371}\n",
      "2021-09-07 17:18:16.733 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16359488666057587\n",
      "2021-09-07 17:18:16.734 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8866092562675476\n",
      "2021-09-07 17:18:16.736 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16359488666057587\n",
      "2021-09-07 17:18:16.737 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:16.738 | INFO     | src.policies:train:123 - Epoch 214 / 800\n",
      "2021-09-07 17:18:16.739 | INFO     | src.policies:collect_trajectories:221 - Episode 941\n",
      "2021-09-07 17:18:16.770 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.770 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:16.771 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:16.773 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:16.775 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6236427426338196, 'baseline_loss': 1.5176544189453125, 'total_loss': 0.13518446683883667}\n",
      "2021-09-07 17:18:16.776 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1974194496870041\n",
      "2021-09-07 17:18:16.776 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.313239336013794\n",
      "2021-09-07 17:18:16.778 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1974194496870041\n",
      "2021-09-07 17:18:16.779 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:16.780 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:16.782 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5859532356262207, 'baseline_loss': 1.2705061435699463, 'total_loss': 0.04929983615875244}\n",
      "2021-09-07 17:18:16.783 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3171374499797821\n",
      "2021-09-07 17:18:16.783 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8027198314666748\n",
      "2021-09-07 17:18:16.785 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3171374499797821\n",
      "2021-09-07 17:18:16.787 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:16.789 | INFO     | src.policies:train:123 - Epoch 215 / 800\n",
      "2021-09-07 17:18:16.789 | INFO     | src.policies:collect_trajectories:221 - Episode 942\n",
      "2021-09-07 17:18:16.815 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.816 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 158.0\n",
      "2021-09-07 17:18:16.816 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 158.0\n",
      "2021-09-07 17:18:16.816 | INFO     | src.policies:collect_trajectories:221 - Episode 943\n",
      "2021-09-07 17:18:16.824 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.825 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 28.0\n",
      "2021-09-07 17:18:16.826 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 93.0\n",
      "2021-09-07 17:18:16.827 | INFO     | src.policies:collect_trajectories:221 - Episode 944\n",
      "2021-09-07 17:18:16.850 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.850 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 142.0\n",
      "2021-09-07 17:18:16.851 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 109.33333333333333\n",
      "2021-09-07 17:18:16.851 | WARNING  | src.policies:train:144 - The actual batch size is 328, instead of 200\n",
      "2021-09-07 17:18:16.854 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:16.856 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27213361859321594, 'baseline_loss': 0.6667807102203369, 'total_loss': 0.061256736516952515}\n",
      "2021-09-07 17:18:16.857 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20917141437530518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:16.858 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3888026475906372\n",
      "2021-09-07 17:18:16.859 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20917141437530518\n",
      "2021-09-07 17:18:16.860 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:16.862 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:16.863 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20374350249767303, 'baseline_loss': 0.6509544253349304, 'total_loss': 0.12173371016979218}\n",
      "2021-09-07 17:18:16.864 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.43471044301986694\n",
      "2021-09-07 17:18:16.866 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2041435241699219\n",
      "2021-09-07 17:18:16.867 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.43471044301986694\n",
      "2021-09-07 17:18:16.868 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:16.869 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:16.871 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3332226276397705, 'baseline_loss': 0.8149599432945251, 'total_loss': 0.07425734400749207}\n",
      "2021-09-07 17:18:16.872 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16321997344493866\n",
      "2021-09-07 17:18:16.873 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4815454483032227\n",
      "2021-09-07 17:18:16.874 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16321997344493866\n",
      "2021-09-07 17:18:16.875 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:16.876 | INFO     | src.policies:train:123 - Epoch 216 / 800\n",
      "2021-09-07 17:18:16.877 | INFO     | src.policies:collect_trajectories:221 - Episode 945\n",
      "2021-09-07 17:18:16.903 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.904 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 176.0\n",
      "2021-09-07 17:18:16.904 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 176.0\n",
      "2021-09-07 17:18:16.905 | INFO     | src.policies:collect_trajectories:221 - Episode 946\n",
      "2021-09-07 17:18:16.940 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:16.941 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:16.942 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 188.0\n",
      "2021-09-07 17:18:16.943 | WARNING  | src.policies:train:144 - The actual batch size is 376, instead of 200\n",
      "2021-09-07 17:18:16.948 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:16.950 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.44538745284080505, 'baseline_loss': 0.8696233630180359, 'total_loss': -0.01057577133178711}\n",
      "2021-09-07 17:18:16.951 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1731388419866562\n",
      "2021-09-07 17:18:16.953 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8520119190216064\n",
      "2021-09-07 17:18:16.954 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1731388419866562\n",
      "2021-09-07 17:18:16.956 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:16.957 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:16.959 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40057051181793213, 'baseline_loss': 0.7678337097167969, 'total_loss': -0.01665365695953369}\n",
      "2021-09-07 17:18:16.960 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.29190269112586975\n",
      "2021-09-07 17:18:16.961 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6497594118118286\n",
      "2021-09-07 17:18:16.962 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.29190269112586975\n",
      "2021-09-07 17:18:16.964 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:16.965 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:16.967 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3947208821773529, 'baseline_loss': 0.7082882523536682, 'total_loss': -0.0405767560005188}\n",
      "2021-09-07 17:18:16.969 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12024246156215668\n",
      "2021-09-07 17:18:16.971 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.36294984817504883\n",
      "2021-09-07 17:18:16.972 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12024246156215668\n",
      "2021-09-07 17:18:16.974 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.36294984817504883\n",
      "2021-09-07 17:18:16.977 | INFO     | src.policies:train:123 - Epoch 217 / 800\n",
      "2021-09-07 17:18:16.978 | INFO     | src.policies:collect_trajectories:221 - Episode 947\n",
      "2021-09-07 17:18:17.012 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.013 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 164.0\n",
      "2021-09-07 17:18:17.013 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 164.0\n",
      "2021-09-07 17:18:17.013 | INFO     | src.policies:collect_trajectories:221 - Episode 948\n",
      "2021-09-07 17:18:17.026 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.027 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 57.0\n",
      "2021-09-07 17:18:17.027 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 110.5\n",
      "2021-09-07 17:18:17.028 | WARNING  | src.policies:train:144 - The actual batch size is 221, instead of 200\n",
      "2021-09-07 17:18:17.030 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:17.032 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4306258261203766, 'baseline_loss': 0.8711093068122864, 'total_loss': 0.0049288272857666016}\n",
      "2021-09-07 17:18:17.033 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5126696228981018\n",
      "2021-09-07 17:18:17.034 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2953460216522217\n",
      "2021-09-07 17:18:17.035 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999989867210388\n",
      "2021-09-07 17:18:17.036 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2953460216522217\n",
      "2021-09-07 17:18:17.038 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:17.039 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.37480658292770386, 'baseline_loss': 0.7996801137924194, 'total_loss': 0.02503347396850586}\n",
      "2021-09-07 17:18:17.040 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1368141919374466\n",
      "2021-09-07 17:18:17.041 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3882104754447937\n",
      "2021-09-07 17:18:17.043 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1368141919374466\n",
      "2021-09-07 17:18:17.044 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3882104754447937\n",
      "2021-09-07 17:18:17.045 | INFO     | src.policies:train:123 - Epoch 218 / 800\n",
      "2021-09-07 17:18:17.046 | INFO     | src.policies:collect_trajectories:221 - Episode 949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:17.082 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.083 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:17.083 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:17.086 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:17.087 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6841245293617249, 'baseline_loss': 2.4629721641540527, 'total_loss': 0.5473615527153015}\n",
      "2021-09-07 17:18:17.088 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1343100219964981\n",
      "2021-09-07 17:18:17.089 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.397463083267212\n",
      "2021-09-07 17:18:17.090 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1343100219964981\n",
      "2021-09-07 17:18:17.091 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:17.092 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:17.094 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6878659725189209, 'baseline_loss': 2.06714129447937, 'total_loss': 0.34570467472076416}\n",
      "2021-09-07 17:18:17.094 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4327145516872406\n",
      "2021-09-07 17:18:17.095 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.1818604469299316\n",
      "2021-09-07 17:18:17.096 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4327145516872406\n",
      "2021-09-07 17:18:17.097 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:17.100 | INFO     | src.policies:train:123 - Epoch 219 / 800\n",
      "2021-09-07 17:18:17.101 | INFO     | src.policies:collect_trajectories:221 - Episode 950\n",
      "2021-09-07 17:18:17.128 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.128 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 152.0\n",
      "2021-09-07 17:18:17.129 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 152.0\n",
      "2021-09-07 17:18:17.129 | INFO     | src.policies:collect_trajectories:221 - Episode 951\n",
      "2021-09-07 17:18:17.157 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.157 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 156.0\n",
      "2021-09-07 17:18:17.158 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 154.0\n",
      "2021-09-07 17:18:17.158 | WARNING  | src.policies:train:144 - The actual batch size is 308, instead of 200\n",
      "2021-09-07 17:18:17.162 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:17.165 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3430021405220032, 'baseline_loss': 0.4693904519081116, 'total_loss': -0.10830691456794739}\n",
      "2021-09-07 17:18:17.166 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4346062242984772\n",
      "2021-09-07 17:18:17.167 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.544169008731842\n",
      "2021-09-07 17:18:17.168 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4346062242984772\n",
      "2021-09-07 17:18:17.169 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:17.170 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:17.172 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3232101500034332, 'baseline_loss': 0.49852970242500305, 'total_loss': -0.0739452987909317}\n",
      "2021-09-07 17:18:17.173 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12672890722751617\n",
      "2021-09-07 17:18:17.216 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.793807327747345\n",
      "2021-09-07 17:18:17.232 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12672890722751617\n",
      "2021-09-07 17:18:17.233 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:17.235 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:17.236 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3586110770702362, 'baseline_loss': 0.5225351452827454, 'total_loss': -0.09734350442886353}\n",
      "2021-09-07 17:18:17.237 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09972614049911499\n",
      "2021-09-07 17:18:17.238 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6120064854621887\n",
      "2021-09-07 17:18:17.239 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09972614049911499\n",
      "2021-09-07 17:18:17.241 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:17.243 | INFO     | src.policies:train:123 - Epoch 220 / 800\n",
      "2021-09-07 17:18:17.244 | INFO     | src.policies:collect_trajectories:221 - Episode 952\n",
      "2021-09-07 17:18:17.278 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.278 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:17.279 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:17.282 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:17.284 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3575552701950073, 'baseline_loss': 0.781726062297821, 'total_loss': 0.0333077609539032}\n",
      "2021-09-07 17:18:17.285 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18515881896018982\n",
      "2021-09-07 17:18:17.286 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9489738941192627\n",
      "2021-09-07 17:18:17.287 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18515881896018982\n",
      "2021-09-07 17:18:17.288 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:17.290 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:17.291 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3451353907585144, 'baseline_loss': 0.9034641981124878, 'total_loss': 0.10659670829772949}\n",
      "2021-09-07 17:18:17.292 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24733780324459076\n",
      "2021-09-07 17:18:17.293 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8770814538002014\n",
      "2021-09-07 17:18:17.294 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24733780324459076\n",
      "2021-09-07 17:18:17.295 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:17.296 | INFO     | src.policies:train:123 - Epoch 221 / 800\n",
      "2021-09-07 17:18:17.297 | INFO     | src.policies:collect_trajectories:221 - Episode 953\n",
      "2021-09-07 17:18:17.317 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.318 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 115.0\n",
      "2021-09-07 17:18:17.318 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 115.0\n",
      "2021-09-07 17:18:17.319 | INFO     | src.policies:collect_trajectories:221 - Episode 954\n",
      "2021-09-07 17:18:17.350 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.351 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 188.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:17.351 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 151.5\n",
      "2021-09-07 17:18:17.352 | WARNING  | src.policies:train:144 - The actual batch size is 303, instead of 200\n",
      "2021-09-07 17:18:17.354 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:17.356 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40451037883758545, 'baseline_loss': 0.6178886294364929, 'total_loss': -0.09556606411933899}\n",
      "2021-09-07 17:18:17.357 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4665355384349823\n",
      "2021-09-07 17:18:17.358 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7057991623878479\n",
      "2021-09-07 17:18:17.360 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4665355384349823\n",
      "2021-09-07 17:18:17.361 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:17.362 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:17.364 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24933263659477234, 'baseline_loss': 0.5133936405181885, 'total_loss': 0.007364183664321899}\n",
      "2021-09-07 17:18:17.364 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11055240035057068\n",
      "2021-09-07 17:18:17.366 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7133840322494507\n",
      "2021-09-07 17:18:17.368 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11055240035057068\n",
      "2021-09-07 17:18:17.369 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:17.370 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:17.372 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3380858898162842, 'baseline_loss': 0.6962621212005615, 'total_loss': 0.010045170783996582}\n",
      "2021-09-07 17:18:17.372 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1527261883020401\n",
      "2021-09-07 17:18:17.373 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5671964287757874\n",
      "2021-09-07 17:18:17.374 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1527261883020401\n",
      "2021-09-07 17:18:17.375 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:17.377 | INFO     | src.policies:train:123 - Epoch 222 / 800\n",
      "2021-09-07 17:18:17.377 | INFO     | src.policies:collect_trajectories:221 - Episode 955\n",
      "2021-09-07 17:18:17.408 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.408 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:17.409 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:17.411 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:17.413 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7023319005966187, 'baseline_loss': 2.0966482162475586, 'total_loss': 0.34599220752716064}\n",
      "2021-09-07 17:18:17.414 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1326827108860016\n",
      "2021-09-07 17:18:17.415 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.7331788539886475\n",
      "2021-09-07 17:18:17.416 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1326827108860016\n",
      "2021-09-07 17:18:17.418 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:17.420 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:17.421 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.694084644317627, 'baseline_loss': 2.2005794048309326, 'total_loss': 0.40620505809783936}\n",
      "2021-09-07 17:18:17.422 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1275418996810913\n",
      "2021-09-07 17:18:17.423 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.0499958992004395\n",
      "2021-09-07 17:18:17.425 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1275418996810913\n",
      "2021-09-07 17:18:17.426 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:17.428 | INFO     | src.policies:train:123 - Epoch 223 / 800\n",
      "2021-09-07 17:18:17.428 | INFO     | src.policies:collect_trajectories:221 - Episode 956\n",
      "2021-09-07 17:18:17.461 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.462 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 190.0\n",
      "2021-09-07 17:18:17.463 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 190.0\n",
      "2021-09-07 17:18:17.463 | INFO     | src.policies:collect_trajectories:221 - Episode 957\n",
      "2021-09-07 17:18:17.494 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.494 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 184.0\n",
      "2021-09-07 17:18:17.495 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 187.0\n",
      "2021-09-07 17:18:17.496 | WARNING  | src.policies:train:144 - The actual batch size is 374, instead of 200\n",
      "2021-09-07 17:18:17.498 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:17.501 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3480464220046997, 'baseline_loss': 0.6066506505012512, 'total_loss': -0.0447210967540741}\n",
      "2021-09-07 17:18:17.502 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.28794723749160767\n",
      "2021-09-07 17:18:17.503 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.29535770416259766\n",
      "2021-09-07 17:18:17.504 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.28794723749160767\n",
      "2021-09-07 17:18:17.506 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.29535770416259766\n",
      "2021-09-07 17:18:17.508 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:17.510 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3217306435108185, 'baseline_loss': 0.4735223352909088, 'total_loss': -0.08496947586536407}\n",
      "2021-09-07 17:18:17.511 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31665027141571045\n",
      "2021-09-07 17:18:17.511 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5880346298217773\n",
      "2021-09-07 17:18:17.513 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31665027141571045\n",
      "2021-09-07 17:18:17.514 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:17.515 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:17.516 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.35793212056159973, 'baseline_loss': 0.526322603225708, 'total_loss': -0.09477081894874573}\n",
      "2021-09-07 17:18:17.517 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17674259841442108\n",
      "2021-09-07 17:18:17.518 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.36105456948280334\n",
      "2021-09-07 17:18:17.520 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17674259841442108\n",
      "2021-09-07 17:18:17.521 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.36105456948280334\n",
      "2021-09-07 17:18:17.522 | INFO     | src.policies:train:123 - Epoch 224 / 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:17.523 | INFO     | src.policies:collect_trajectories:221 - Episode 958\n",
      "2021-09-07 17:18:17.543 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.543 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 114.0\n",
      "2021-09-07 17:18:17.544 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 114.0\n",
      "2021-09-07 17:18:17.544 | INFO     | src.policies:collect_trajectories:221 - Episode 959\n",
      "2021-09-07 17:18:17.584 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.585 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:17.585 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 157.0\n",
      "2021-09-07 17:18:17.586 | WARNING  | src.policies:train:144 - The actual batch size is 314, instead of 200\n",
      "2021-09-07 17:18:17.588 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:17.590 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5763145685195923, 'baseline_loss': 1.6343202590942383, 'total_loss': 0.24084556102752686}\n",
      "2021-09-07 17:18:17.591 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1695762425661087\n",
      "2021-09-07 17:18:17.591 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.608616352081299\n",
      "2021-09-07 17:18:17.593 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1695762425661087\n",
      "2021-09-07 17:18:17.594 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:17.595 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:17.596 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6224058270454407, 'baseline_loss': 1.9701206684112549, 'total_loss': 0.36265450716018677}\n",
      "2021-09-07 17:18:17.597 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14203952252864838\n",
      "2021-09-07 17:18:17.598 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.1562716960906982\n",
      "2021-09-07 17:18:17.599 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14203952252864838\n",
      "2021-09-07 17:18:17.601 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:17.602 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:17.604 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5763974189758301, 'baseline_loss': 1.7933135032653809, 'total_loss': 0.32025933265686035}\n",
      "2021-09-07 17:18:17.605 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18043595552444458\n",
      "2021-09-07 17:18:17.607 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.560333251953125\n",
      "2021-09-07 17:18:17.608 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18043595552444458\n",
      "2021-09-07 17:18:17.609 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:17.611 | INFO     | src.policies:train:123 - Epoch 225 / 800\n",
      "2021-09-07 17:18:17.612 | INFO     | src.policies:collect_trajectories:221 - Episode 960\n",
      "2021-09-07 17:18:17.644 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.645 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 187.0\n",
      "2021-09-07 17:18:17.645 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 187.0\n",
      "2021-09-07 17:18:17.646 | INFO     | src.policies:collect_trajectories:221 - Episode 961\n",
      "2021-09-07 17:18:17.670 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.671 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 137.0\n",
      "2021-09-07 17:18:17.672 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 162.0\n",
      "2021-09-07 17:18:17.672 | WARNING  | src.policies:train:144 - The actual batch size is 324, instead of 200\n",
      "2021-09-07 17:18:17.675 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:17.676 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3894643485546112, 'baseline_loss': 0.6211205124855042, 'total_loss': -0.07890409231185913}\n",
      "2021-09-07 17:18:17.677 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2400730550289154\n",
      "2021-09-07 17:18:17.678 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3256683945655823\n",
      "2021-09-07 17:18:17.679 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2400730550289154\n",
      "2021-09-07 17:18:17.681 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3256683945655823\n",
      "2021-09-07 17:18:17.682 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:17.684 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4209423065185547, 'baseline_loss': 0.5464500188827515, 'total_loss': -0.14771729707717896}\n",
      "2021-09-07 17:18:17.685 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20407305657863617\n",
      "2021-09-07 17:18:17.686 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4280456304550171\n",
      "2021-09-07 17:18:17.687 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20407305657863617\n",
      "2021-09-07 17:18:17.688 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4280456304550171\n",
      "2021-09-07 17:18:17.690 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:17.691 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.309680700302124, 'baseline_loss': 0.4805358946323395, 'total_loss': -0.06941275298595428}\n",
      "2021-09-07 17:18:17.692 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.27482423186302185\n",
      "2021-09-07 17:18:17.692 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6086041331291199\n",
      "2021-09-07 17:18:17.693 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.27482423186302185\n",
      "2021-09-07 17:18:17.695 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:17.696 | INFO     | src.policies:train:123 - Epoch 226 / 800\n",
      "2021-09-07 17:18:17.697 | INFO     | src.policies:collect_trajectories:221 - Episode 962\n",
      "2021-09-07 17:18:17.720 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.721 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 126.0\n",
      "2021-09-07 17:18:17.721 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 126.0\n",
      "2021-09-07 17:18:17.722 | INFO     | src.policies:collect_trajectories:221 - Episode 963\n",
      "2021-09-07 17:18:17.812 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.813 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:17.813 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 163.0\n",
      "2021-09-07 17:18:17.814 | WARNING  | src.policies:train:144 - The actual batch size is 326, instead of 200\n",
      "2021-09-07 17:18:17.817 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:17.819 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6036450266838074, 'baseline_loss': 1.431971549987793, 'total_loss': 0.11234074831008911}\n",
      "2021-09-07 17:18:17.821 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.36337754130363464\n",
      "2021-09-07 17:18:17.822 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7832847833633423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:17.824 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.36337754130363464\n",
      "2021-09-07 17:18:17.825 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:17.826 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:17.828 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6087082028388977, 'baseline_loss': 1.4948841333389282, 'total_loss': 0.1387338638305664}\n",
      "2021-09-07 17:18:17.829 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17246513068675995\n",
      "2021-09-07 17:18:17.830 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7622268199920654\n",
      "2021-09-07 17:18:17.831 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17246513068675995\n",
      "2021-09-07 17:18:17.832 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:17.834 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:17.835 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.46219584345817566, 'baseline_loss': 1.3861538171768188, 'total_loss': 0.23088106513023376}\n",
      "2021-09-07 17:18:17.836 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23780514299869537\n",
      "2021-09-07 17:18:17.837 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.660477876663208\n",
      "2021-09-07 17:18:17.838 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23780514299869537\n",
      "2021-09-07 17:18:17.840 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:17.842 | INFO     | src.policies:train:123 - Epoch 227 / 800\n",
      "2021-09-07 17:18:17.843 | INFO     | src.policies:collect_trajectories:221 - Episode 964\n",
      "2021-09-07 17:18:17.866 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.866 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 124.0\n",
      "2021-09-07 17:18:17.867 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 124.0\n",
      "2021-09-07 17:18:17.867 | INFO     | src.policies:collect_trajectories:221 - Episode 965\n",
      "2021-09-07 17:18:17.904 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.904 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:17.905 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 162.0\n",
      "2021-09-07 17:18:17.906 | WARNING  | src.policies:train:144 - The actual batch size is 324, instead of 200\n",
      "2021-09-07 17:18:17.910 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:17.912 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2527143955230713, 'baseline_loss': 0.3783268332481384, 'total_loss': -0.06355097889900208}\n",
      "2021-09-07 17:18:17.913 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20055340230464935\n",
      "2021-09-07 17:18:17.914 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1323966979980469\n",
      "2021-09-07 17:18:17.915 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20055340230464935\n",
      "2021-09-07 17:18:17.916 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:17.917 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:17.919 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3270712196826935, 'baseline_loss': 0.5684708952903748, 'total_loss': -0.042835772037506104}\n",
      "2021-09-07 17:18:17.920 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3288213610649109\n",
      "2021-09-07 17:18:17.922 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.924683690071106\n",
      "2021-09-07 17:18:17.923 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3288213610649109\n",
      "2021-09-07 17:18:17.924 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:17.926 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:17.927 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2458391785621643, 'baseline_loss': 0.44565409421920776, 'total_loss': -0.023012131452560425}\n",
      "2021-09-07 17:18:17.928 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21197791397571564\n",
      "2021-09-07 17:18:17.929 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1011261940002441\n",
      "2021-09-07 17:18:17.930 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21197791397571564\n",
      "2021-09-07 17:18:17.932 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:17.933 | INFO     | src.policies:train:123 - Epoch 228 / 800\n",
      "2021-09-07 17:18:17.934 | INFO     | src.policies:collect_trajectories:221 - Episode 966\n",
      "2021-09-07 17:18:17.962 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.963 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 173.0\n",
      "2021-09-07 17:18:17.963 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 173.0\n",
      "2021-09-07 17:18:17.964 | INFO     | src.policies:collect_trajectories:221 - Episode 967\n",
      "2021-09-07 17:18:17.996 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:17.997 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:17.997 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 186.5\n",
      "2021-09-07 17:18:17.998 | WARNING  | src.policies:train:144 - The actual batch size is 373, instead of 200\n",
      "2021-09-07 17:18:18.003 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:18.005 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2359890341758728, 'baseline_loss': 0.5236928462982178, 'total_loss': 0.025857388973236084}\n",
      "2021-09-07 17:18:18.006 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30788880586624146\n",
      "2021-09-07 17:18:18.007 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7559443116188049\n",
      "2021-09-07 17:18:18.008 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30788880586624146\n",
      "2021-09-07 17:18:18.010 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:18.011 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:18.013 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.19760990142822266, 'baseline_loss': 0.4661884605884552, 'total_loss': 0.035484328866004944}\n",
      "2021-09-07 17:18:18.014 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32420361042022705\n",
      "2021-09-07 17:18:18.015 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1543902158737183\n",
      "2021-09-07 17:18:18.016 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32420361042022705\n",
      "2021-09-07 17:18:18.017 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:18.019 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:18.020 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.28010618686676025, 'baseline_loss': 0.5149991512298584, 'total_loss': -0.022606611251831055}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:18.022 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2858635187149048\n",
      "2021-09-07 17:18:18.023 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1645325422286987\n",
      "2021-09-07 17:18:18.024 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2858635187149048\n",
      "2021-09-07 17:18:18.025 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:18.027 | INFO     | src.policies:train:123 - Epoch 229 / 800\n",
      "2021-09-07 17:18:18.027 | INFO     | src.policies:collect_trajectories:221 - Episode 968\n",
      "2021-09-07 17:18:18.062 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:18.063 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:18.064 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:18.066 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:18.068 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.35287150740623474, 'baseline_loss': 0.5811524391174316, 'total_loss': -0.06229528784751892}\n",
      "2021-09-07 17:18:18.069 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25572913885116577\n",
      "2021-09-07 17:18:18.070 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7867608070373535\n",
      "2021-09-07 17:18:18.071 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25572913885116577\n",
      "2021-09-07 17:18:18.072 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:18.073 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:18.075 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5691577792167664, 'baseline_loss': 0.9571261405944824, 'total_loss': -0.09059470891952515}\n",
      "2021-09-07 17:18:18.076 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5389283895492554\n",
      "2021-09-07 17:18:18.077 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7169483304023743\n",
      "2021-09-07 17:18:18.078 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:18.079 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:18.081 | INFO     | src.policies:train:123 - Epoch 230 / 800\n",
      "2021-09-07 17:18:18.081 | INFO     | src.policies:collect_trajectories:221 - Episode 969\n",
      "2021-09-07 17:18:18.215 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:18.215 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:18.216 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:18.218 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:18.220 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.699268102645874, 'baseline_loss': 1.9047213792800903, 'total_loss': 0.25309258699417114}\n",
      "2021-09-07 17:18:18.222 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22457852959632874\n",
      "2021-09-07 17:18:18.224 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.993971824645996\n",
      "2021-09-07 17:18:18.225 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22457852959632874\n",
      "2021-09-07 17:18:18.226 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:18.228 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:18.229 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.721738338470459, 'baseline_loss': 1.6912150382995605, 'total_loss': 0.12386918067932129}\n",
      "2021-09-07 17:18:18.230 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.26354122161865234\n",
      "2021-09-07 17:18:18.231 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.585325241088867\n",
      "2021-09-07 17:18:18.232 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.26354122161865234\n",
      "2021-09-07 17:18:18.233 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:18.235 | INFO     | src.policies:train:123 - Epoch 231 / 800\n",
      "2021-09-07 17:18:18.236 | INFO     | src.policies:collect_trajectories:221 - Episode 970\n",
      "2021-09-07 17:18:18.268 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:18.269 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:18.269 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:18.271 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:18.273 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8648736476898193, 'baseline_loss': 3.8185853958129883, 'total_loss': 1.0444190502166748}\n",
      "2021-09-07 17:18:18.274 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5149526000022888\n",
      "2021-09-07 17:18:18.275 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 5.02357816696167\n",
      "2021-09-07 17:18:18.276 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:18.277 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:18.279 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:18.281 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8952683210372925, 'baseline_loss': 3.6983227729797363, 'total_loss': 0.9538930654525757}\n",
      "2021-09-07 17:18:18.282 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 1.0037572383880615\n",
      "2021-09-07 17:18:18.283 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 5.109455108642578\n",
      "2021-09-07 17:18:18.284 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:18.286 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:18.288 | INFO     | src.policies:train:123 - Epoch 232 / 800\n",
      "2021-09-07 17:18:18.289 | INFO     | src.policies:collect_trajectories:221 - Episode 971\n",
      "2021-09-07 17:18:18.362 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:18.363 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 198.0\n",
      "2021-09-07 17:18:18.364 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 198.0\n",
      "2021-09-07 17:18:18.364 | INFO     | src.policies:collect_trajectories:221 - Episode 972\n",
      "2021-09-07 17:18:18.395 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:18.396 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:18.396 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 199.0\n",
      "2021-09-07 17:18:18.396 | WARNING  | src.policies:train:144 - The actual batch size is 398, instead of 200\n",
      "2021-09-07 17:18:18.400 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:18.402 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.35990098118782043, 'baseline_loss': 0.7978521585464478, 'total_loss': 0.03902509808540344}\n",
      "2021-09-07 17:18:18.403 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23112176358699799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:18.404 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.491789847612381\n",
      "2021-09-07 17:18:18.406 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23112176358699799\n",
      "2021-09-07 17:18:18.408 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.491789847612381\n",
      "2021-09-07 17:18:18.410 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:18.412 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3467756509780884, 'baseline_loss': 0.8108263611793518, 'total_loss': 0.058637529611587524}\n",
      "2021-09-07 17:18:18.413 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.28771477937698364\n",
      "2021-09-07 17:18:18.414 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6814273595809937\n",
      "2021-09-07 17:18:18.415 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.28771477937698364\n",
      "2021-09-07 17:18:18.416 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:18.418 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:18.419 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3947012424468994, 'baseline_loss': 0.8429365754127502, 'total_loss': 0.026767045259475708}\n",
      "2021-09-07 17:18:18.420 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22074395418167114\n",
      "2021-09-07 17:18:18.421 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8701901435852051\n",
      "2021-09-07 17:18:18.422 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22074395418167114\n",
      "2021-09-07 17:18:18.423 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:18.425 | INFO     | src.policies:train:123 - Epoch 233 / 800\n",
      "2021-09-07 17:18:18.426 | INFO     | src.policies:collect_trajectories:221 - Episode 973\n",
      "2021-09-07 17:18:18.457 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:18.458 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:18.459 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:18.462 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:18.464 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.37615370750427246, 'baseline_loss': 0.5489476919174194, 'total_loss': -0.10167986154556274}\n",
      "2021-09-07 17:18:18.465 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08559948951005936\n",
      "2021-09-07 17:18:18.466 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.23167061805725098\n",
      "2021-09-07 17:18:18.468 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08559948951005936\n",
      "2021-09-07 17:18:18.469 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.23167061805725098\n",
      "2021-09-07 17:18:18.470 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:18.472 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4042642116546631, 'baseline_loss': 0.5631107091903687, 'total_loss': -0.12270885705947876}\n",
      "2021-09-07 17:18:18.473 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21577131748199463\n",
      "2021-09-07 17:18:18.474 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2907926142215729\n",
      "2021-09-07 17:18:18.475 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21577131748199463\n",
      "2021-09-07 17:18:18.476 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2907926142215729\n",
      "2021-09-07 17:18:18.478 | INFO     | src.policies:train:123 - Epoch 234 / 800\n",
      "2021-09-07 17:18:18.478 | INFO     | src.policies:collect_trajectories:221 - Episode 974\n",
      "2021-09-07 17:18:18.502 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:18.503 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 148.0\n",
      "2021-09-07 17:18:18.503 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 148.0\n",
      "2021-09-07 17:18:18.504 | INFO     | src.policies:collect_trajectories:221 - Episode 975\n",
      "2021-09-07 17:18:18.536 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:18.537 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:18.537 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 174.0\n",
      "2021-09-07 17:18:18.538 | WARNING  | src.policies:train:144 - The actual batch size is 348, instead of 200\n",
      "2021-09-07 17:18:18.542 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:18.544 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41345080733299255, 'baseline_loss': 1.0361250638961792, 'total_loss': 0.10461172461509705}\n",
      "2021-09-07 17:18:18.545 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6029072999954224\n",
      "2021-09-07 17:18:18.547 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.066348910331726\n",
      "2021-09-07 17:18:18.548 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:18.549 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:18.550 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:18.551 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43895426392555237, 'baseline_loss': 1.1248449087142944, 'total_loss': 0.12346819043159485}\n",
      "2021-09-07 17:18:18.552 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16023585200309753\n",
      "2021-09-07 17:18:18.553 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5619641542434692\n",
      "2021-09-07 17:18:18.554 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16023585200309753\n",
      "2021-09-07 17:18:18.555 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:18.557 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:18.559 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4626396596431732, 'baseline_loss': 1.0638301372528076, 'total_loss': 0.06927540898323059}\n",
      "2021-09-07 17:18:18.561 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16065388917922974\n",
      "2021-09-07 17:18:18.562 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0591316223144531\n",
      "2021-09-07 17:18:18.563 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16065388917922974\n",
      "2021-09-07 17:18:18.565 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:18.566 | INFO     | src.policies:train:123 - Epoch 235 / 800\n",
      "2021-09-07 17:18:18.567 | INFO     | src.policies:collect_trajectories:221 - Episode 976\n",
      "2021-09-07 17:18:18.596 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:18.597 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 184.0\n",
      "2021-09-07 17:18:18.598 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 184.0\n",
      "2021-09-07 17:18:18.598 | INFO     | src.policies:collect_trajectories:221 - Episode 977\n",
      "2021-09-07 17:18:18.633 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:18.634 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:18.634 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 192.0\n",
      "2021-09-07 17:18:18.635 | WARNING  | src.policies:train:144 - The actual batch size is 384, instead of 200\n",
      "2021-09-07 17:18:18.638 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:18.640 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2408805638551712, 'baseline_loss': 0.4384835362434387, 'total_loss': -0.021638795733451843}\n",
      "2021-09-07 17:18:18.642 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15399572253227234\n",
      "2021-09-07 17:18:18.642 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5763144493103027\n",
      "2021-09-07 17:18:18.643 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15399572253227234\n",
      "2021-09-07 17:18:18.645 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:18.647 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:18.648 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27579808235168457, 'baseline_loss': 0.41620174050331116, 'total_loss': -0.06769721210002899}\n",
      "2021-09-07 17:18:18.649 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.170607790350914\n",
      "2021-09-07 17:18:18.650 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.480974555015564\n",
      "2021-09-07 17:18:18.651 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.170607790350914\n",
      "2021-09-07 17:18:18.652 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:18.653 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:18.654 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.26263752579689026, 'baseline_loss': 0.43153437972068787, 'total_loss': -0.046870335936546326}\n",
      "2021-09-07 17:18:18.655 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2295558601617813\n",
      "2021-09-07 17:18:18.656 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5312633514404297\n",
      "2021-09-07 17:18:18.658 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2295558601617813\n",
      "2021-09-07 17:18:18.659 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:18.661 | INFO     | src.policies:train:123 - Epoch 236 / 800\n",
      "2021-09-07 17:18:18.662 | INFO     | src.policies:collect_trajectories:221 - Episode 978\n",
      "2021-09-07 17:18:18.694 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:18.694 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:18.695 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:18.697 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:18.699 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42593806982040405, 'baseline_loss': 0.6887824535369873, 'total_loss': -0.0815468430519104}\n",
      "2021-09-07 17:18:18.700 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17361974716186523\n",
      "2021-09-07 17:18:18.701 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3351234793663025\n",
      "2021-09-07 17:18:18.702 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17361974716186523\n",
      "2021-09-07 17:18:18.704 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3351234793663025\n",
      "2021-09-07 17:18:18.705 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:18.707 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31434300541877747, 'baseline_loss': 0.4810701608657837, 'total_loss': -0.07380792498588562}\n",
      "2021-09-07 17:18:18.708 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09363559633493423\n",
      "2021-09-07 17:18:18.709 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5779102444648743\n",
      "2021-09-07 17:18:18.710 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09363559633493423\n",
      "2021-09-07 17:18:18.711 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:18.712 | INFO     | src.policies:train:123 - Epoch 237 / 800\n",
      "2021-09-07 17:18:18.713 | INFO     | src.policies:collect_trajectories:221 - Episode 979\n",
      "2021-09-07 17:18:18.744 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:18.744 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:18.745 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:18.747 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:18.749 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5133082866668701, 'baseline_loss': 1.0261131525039673, 'total_loss': -0.0002517104148864746}\n",
      "2021-09-07 17:18:18.750 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24236680567264557\n",
      "2021-09-07 17:18:18.751 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9778024554252625\n",
      "2021-09-07 17:18:18.753 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24236680567264557\n",
      "2021-09-07 17:18:18.754 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:18.756 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:18.758 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5245302319526672, 'baseline_loss': 1.1152877807617188, 'total_loss': 0.03311365842819214}\n",
      "2021-09-07 17:18:18.760 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.427826851606369\n",
      "2021-09-07 17:18:18.761 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5138098001480103\n",
      "2021-09-07 17:18:18.762 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.427826851606369\n",
      "2021-09-07 17:18:18.763 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:18.765 | INFO     | src.policies:train:123 - Epoch 238 / 800\n",
      "2021-09-07 17:18:18.766 | INFO     | src.policies:collect_trajectories:221 - Episode 980\n",
      "2021-09-07 17:18:18.798 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:18.799 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:18.801 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:18.803 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:18.805 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4922906160354614, 'baseline_loss': 0.964637815952301, 'total_loss': -0.009971708059310913}\n",
      "2021-09-07 17:18:18.806 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5926993489265442\n",
      "2021-09-07 17:18:18.807 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.44657564163208\n",
      "2021-09-07 17:18:18.808 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:18.810 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:18.812 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:18.813 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.48540154099464417, 'baseline_loss': 0.963829517364502, 'total_loss': -0.0034867823123931885}\n",
      "2021-09-07 17:18:18.814 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19994764029979706\n",
      "2021-09-07 17:18:18.815 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4462238550186157\n",
      "2021-09-07 17:18:18.817 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19994764029979706\n",
      "2021-09-07 17:18:18.818 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:18.819 | INFO     | src.policies:train:123 - Epoch 239 / 800\n",
      "2021-09-07 17:18:18.820 | INFO     | src.policies:collect_trajectories:221 - Episode 981\n",
      "2021-09-07 17:18:18.988 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:18.988 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:18.989 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:18.991 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:18.993 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5258976221084595, 'baseline_loss': 1.4135308265686035, 'total_loss': 0.18086779117584229}\n",
      "2021-09-07 17:18:18.994 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3993583023548126\n",
      "2021-09-07 17:18:18.996 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2385014295578003\n",
      "2021-09-07 17:18:18.997 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3993583023548126\n",
      "2021-09-07 17:18:18.999 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:19.000 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:19.002 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3311600387096405, 'baseline_loss': 1.2622534036636353, 'total_loss': 0.2999666631221771}\n",
      "2021-09-07 17:18:19.003 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.28042829036712646\n",
      "2021-09-07 17:18:19.004 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9981499910354614\n",
      "2021-09-07 17:18:19.005 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.28042829036712646\n",
      "2021-09-07 17:18:19.006 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:19.008 | INFO     | src.policies:train:123 - Epoch 240 / 800\n",
      "2021-09-07 17:18:19.009 | INFO     | src.policies:collect_trajectories:221 - Episode 982\n",
      "2021-09-07 17:18:19.042 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:19.043 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:19.043 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:19.045 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:19.047 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6500247120857239, 'baseline_loss': 1.8307768106460571, 'total_loss': 0.2653636932373047}\n",
      "2021-09-07 17:18:19.048 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2524677813053131\n",
      "2021-09-07 17:18:19.050 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.3055131435394287\n",
      "2021-09-07 17:18:19.051 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2524677813053131\n",
      "2021-09-07 17:18:19.052 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:19.054 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:19.055 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8069296479225159, 'baseline_loss': 1.8683656454086304, 'total_loss': 0.12725317478179932}\n",
      "2021-09-07 17:18:19.056 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7207016944885254\n",
      "2021-09-07 17:18:19.058 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.834724187850952\n",
      "2021-09-07 17:18:19.059 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:19.061 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:19.062 | INFO     | src.policies:train:123 - Epoch 241 / 800\n",
      "2021-09-07 17:18:19.063 | INFO     | src.policies:collect_trajectories:221 - Episode 983\n",
      "2021-09-07 17:18:19.085 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:19.085 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 133.0\n",
      "2021-09-07 17:18:19.086 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 133.0\n",
      "2021-09-07 17:18:19.087 | INFO     | src.policies:collect_trajectories:221 - Episode 984\n",
      "2021-09-07 17:18:19.118 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:19.119 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:19.120 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 166.5\n",
      "2021-09-07 17:18:19.120 | WARNING  | src.policies:train:144 - The actual batch size is 333, instead of 200\n",
      "2021-09-07 17:18:19.123 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:19.125 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6131213903427124, 'baseline_loss': 1.522430419921875, 'total_loss': 0.1480938196182251}\n",
      "2021-09-07 17:18:19.126 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5797643661499023\n",
      "2021-09-07 17:18:19.127 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.006943941116333\n",
      "2021-09-07 17:18:19.130 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:19.131 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:19.133 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:19.134 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4804783761501312, 'baseline_loss': 1.5013017654418945, 'total_loss': 0.27017250657081604}\n",
      "2021-09-07 17:18:19.135 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20801930129528046\n",
      "2021-09-07 17:18:19.136 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6451131105422974\n",
      "2021-09-07 17:18:19.137 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20801930129528046\n",
      "2021-09-07 17:18:19.139 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:19.140 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:19.141 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45332375168800354, 'baseline_loss': 1.3885493278503418, 'total_loss': 0.24095091223716736}\n",
      "2021-09-07 17:18:19.142 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.288928359746933\n",
      "2021-09-07 17:18:19.143 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1266541481018066\n",
      "2021-09-07 17:18:19.145 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.288928359746933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:19.146 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:19.148 | INFO     | src.policies:train:123 - Epoch 242 / 800\n",
      "2021-09-07 17:18:19.148 | INFO     | src.policies:collect_trajectories:221 - Episode 985\n",
      "2021-09-07 17:18:19.179 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:19.180 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:19.180 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:19.182 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:19.184 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4816175103187561, 'baseline_loss': 1.1864101886749268, 'total_loss': 0.11158758401870728}\n",
      "2021-09-07 17:18:19.186 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5907188057899475\n",
      "2021-09-07 17:18:19.187 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3741672039031982\n",
      "2021-09-07 17:18:19.188 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:19.190 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:19.191 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:19.192 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5071572661399841, 'baseline_loss': 1.155650019645691, 'total_loss': 0.07066774368286133}\n",
      "2021-09-07 17:18:19.193 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22835998237133026\n",
      "2021-09-07 17:18:19.194 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3224542140960693\n",
      "2021-09-07 17:18:19.195 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22835998237133026\n",
      "2021-09-07 17:18:19.196 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:19.197 | INFO     | src.policies:train:123 - Epoch 243 / 800\n",
      "2021-09-07 17:18:19.198 | INFO     | src.policies:collect_trajectories:221 - Episode 986\n",
      "2021-09-07 17:18:19.231 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:19.232 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:19.232 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:19.234 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:19.236 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.18545441329479218, 'baseline_loss': 0.4204493761062622, 'total_loss': 0.024770274758338928}\n",
      "2021-09-07 17:18:19.237 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13013000786304474\n",
      "2021-09-07 17:18:19.238 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2512232065200806\n",
      "2021-09-07 17:18:19.240 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13013000786304474\n",
      "2021-09-07 17:18:19.241 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:19.242 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:19.244 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2120599001646042, 'baseline_loss': 0.44745078682899475, 'total_loss': 0.011665493249893188}\n",
      "2021-09-07 17:18:19.245 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10411782562732697\n",
      "2021-09-07 17:18:19.247 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4618711471557617\n",
      "2021-09-07 17:18:19.248 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10411782562732697\n",
      "2021-09-07 17:18:19.249 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:19.250 | INFO     | src.policies:train:123 - Epoch 244 / 800\n",
      "2021-09-07 17:18:19.251 | INFO     | src.policies:collect_trajectories:221 - Episode 987\n",
      "2021-09-07 17:18:19.283 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:19.284 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:19.284 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:19.287 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:19.289 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.11806507408618927, 'baseline_loss': 0.6068800687789917, 'total_loss': 0.18537496030330658}\n",
      "2021-09-07 17:18:19.290 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.28802433609962463\n",
      "2021-09-07 17:18:19.291 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2345826625823975\n",
      "2021-09-07 17:18:19.293 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.28802433609962463\n",
      "2021-09-07 17:18:19.295 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:19.296 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:19.297 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.0876915380358696, 'baseline_loss': 0.5418806672096252, 'total_loss': 0.18324878811836243}\n",
      "2021-09-07 17:18:19.299 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20307782292366028\n",
      "2021-09-07 17:18:19.300 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5377180576324463\n",
      "2021-09-07 17:18:19.301 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20307782292366028\n",
      "2021-09-07 17:18:19.303 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:19.304 | INFO     | src.policies:train:123 - Epoch 245 / 800\n",
      "2021-09-07 17:18:19.305 | INFO     | src.policies:collect_trajectories:221 - Episode 988\n",
      "2021-09-07 17:18:19.330 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:19.331 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 146.0\n",
      "2021-09-07 17:18:19.331 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 146.0\n",
      "2021-09-07 17:18:19.332 | INFO     | src.policies:collect_trajectories:221 - Episode 989\n",
      "2021-09-07 17:18:19.361 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:19.362 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 171.0\n",
      "2021-09-07 17:18:19.362 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 158.5\n",
      "2021-09-07 17:18:19.363 | WARNING  | src.policies:train:144 - The actual batch size is 317, instead of 200\n",
      "2021-09-07 17:18:19.365 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:19.367 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3265543282032013, 'baseline_loss': 0.47914794087409973, 'total_loss': -0.08698035776615143}\n",
      "2021-09-07 17:18:19.368 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1514316350221634\n",
      "2021-09-07 17:18:19.369 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2596015930175781\n",
      "2021-09-07 17:18:19.371 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1514316350221634\n",
      "2021-09-07 17:18:19.372 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:19.374 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:19.375 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29196497797966003, 'baseline_loss': 0.4633726179599762, 'total_loss': -0.060278668999671936}\n",
      "2021-09-07 17:18:19.376 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22475449740886688\n",
      "2021-09-07 17:18:19.377 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3689671754837036\n",
      "2021-09-07 17:18:19.378 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22475449740886688\n",
      "2021-09-07 17:18:19.380 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:19.381 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:19.382 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27804237604141235, 'baseline_loss': 0.49492669105529785, 'total_loss': -0.030579030513763428}\n",
      "2021-09-07 17:18:19.383 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2174622118473053\n",
      "2021-09-07 17:18:19.384 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.382698655128479\n",
      "2021-09-07 17:18:19.386 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2174622118473053\n",
      "2021-09-07 17:18:19.388 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:19.389 | INFO     | src.policies:train:123 - Epoch 246 / 800\n",
      "2021-09-07 17:18:19.390 | INFO     | src.policies:collect_trajectories:221 - Episode 990\n",
      "2021-09-07 17:18:19.421 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:19.422 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:19.422 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:19.424 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:19.426 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3193613290786743, 'baseline_loss': 0.5429201722145081, 'total_loss': -0.04790124297142029}\n",
      "2021-09-07 17:18:19.427 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1778104603290558\n",
      "2021-09-07 17:18:19.429 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.880496621131897\n",
      "2021-09-07 17:18:19.430 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1778104603290558\n",
      "2021-09-07 17:18:19.432 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:19.433 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:19.434 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2533569931983948, 'baseline_loss': 0.45523110032081604, 'total_loss': -0.025741443037986755}\n",
      "2021-09-07 17:18:19.435 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5253711938858032\n",
      "2021-09-07 17:18:19.436 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0698210000991821\n",
      "2021-09-07 17:18:19.437 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:19.439 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:19.441 | INFO     | src.policies:train:123 - Epoch 247 / 800\n",
      "2021-09-07 17:18:19.441 | INFO     | src.policies:collect_trajectories:221 - Episode 991\n",
      "2021-09-07 17:18:19.473 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:19.474 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:19.474 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:19.476 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:19.511 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.19741958379745483, 'baseline_loss': 0.5069776773452759, 'total_loss': 0.056069254875183105}\n",
      "2021-09-07 17:18:19.536 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3402213454246521\n",
      "2021-09-07 17:18:19.538 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5718857049942017\n",
      "2021-09-07 17:18:19.540 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3402213454246521\n",
      "2021-09-07 17:18:19.541 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:19.542 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:19.544 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.11694006621837616, 'baseline_loss': 0.4348044693470001, 'total_loss': 0.1004621684551239}\n",
      "2021-09-07 17:18:19.546 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.26775485277175903\n",
      "2021-09-07 17:18:19.547 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3613027334213257\n",
      "2021-09-07 17:18:19.548 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.26775485277175903\n",
      "2021-09-07 17:18:19.549 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:19.550 | INFO     | src.policies:train:123 - Epoch 248 / 800\n",
      "2021-09-07 17:18:19.551 | INFO     | src.policies:collect_trajectories:221 - Episode 992\n",
      "2021-09-07 17:18:19.584 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:19.584 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:19.585 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:19.587 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:19.589 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29874497652053833, 'baseline_loss': 0.5742725133895874, 'total_loss': -0.011608719825744629}\n",
      "2021-09-07 17:18:19.590 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12471151351928711\n",
      "2021-09-07 17:18:19.591 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2036439180374146\n",
      "2021-09-07 17:18:19.593 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12471151351928711\n",
      "2021-09-07 17:18:19.594 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:19.596 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:19.597 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24940438568592072, 'baseline_loss': 0.49191486835479736, 'total_loss': -0.0034469515085220337}\n",
      "2021-09-07 17:18:19.598 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24084055423736572\n",
      "2021-09-07 17:18:19.599 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8537904620170593\n",
      "2021-09-07 17:18:19.600 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24084055423736572\n",
      "2021-09-07 17:18:19.602 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:19.603 | INFO     | src.policies:train:123 - Epoch 249 / 800\n",
      "2021-09-07 17:18:19.604 | INFO     | src.policies:collect_trajectories:221 - Episode 993\n",
      "2021-09-07 17:18:19.624 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:19.626 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 127.0\n",
      "2021-09-07 17:18:19.626 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 127.0\n",
      "2021-09-07 17:18:19.627 | INFO     | src.policies:collect_trajectories:221 - Episode 994\n",
      "2021-09-07 17:18:19.661 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:19.662 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:19.663 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 163.5\n",
      "2021-09-07 17:18:19.664 | WARNING  | src.policies:train:144 - The actual batch size is 327, instead of 200\n",
      "2021-09-07 17:18:19.667 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:19.669 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.162174791097641, 'baseline_loss': 0.4498845636844635, 'total_loss': 0.06276749074459076}\n",
      "2021-09-07 17:18:19.670 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1388910859823227\n",
      "2021-09-07 17:18:19.671 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3815793991088867\n",
      "2021-09-07 17:18:19.672 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1388910859823227\n",
      "2021-09-07 17:18:19.673 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:19.675 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:19.676 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.18696177005767822, 'baseline_loss': 0.4364406168460846, 'total_loss': 0.031258538365364075}\n",
      "2021-09-07 17:18:19.677 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11129377037286758\n",
      "2021-09-07 17:18:19.678 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7381033897399902\n",
      "2021-09-07 17:18:19.679 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11129377037286758\n",
      "2021-09-07 17:18:19.680 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:19.682 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:19.683 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.21291571855545044, 'baseline_loss': 0.3987831771373749, 'total_loss': -0.013524129986763}\n",
      "2021-09-07 17:18:19.685 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19116365909576416\n",
      "2021-09-07 17:18:19.686 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4504048824310303\n",
      "2021-09-07 17:18:19.687 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19116365909576416\n",
      "2021-09-07 17:18:19.688 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:19.690 | INFO     | src.policies:train:123 - Epoch 250 / 800\n",
      "2021-09-07 17:18:19.690 | INFO     | src.policies:collect_trajectories:221 - Episode 995\n",
      "2021-09-07 17:18:19.720 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:19.721 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:19.721 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:19.724 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:19.726 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4510670602321625, 'baseline_loss': 0.8277929425239563, 'total_loss': -0.037170588970184326}\n",
      "2021-09-07 17:18:19.727 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08536902815103531\n",
      "2021-09-07 17:18:19.728 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6421524882316589\n",
      "2021-09-07 17:18:19.729 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08536902815103531\n",
      "2021-09-07 17:18:19.730 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:19.732 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:19.733 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3755321502685547, 'baseline_loss': 0.5186054110527039, 'total_loss': -0.11622944474220276}\n",
      "2021-09-07 17:18:19.734 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13160927593708038\n",
      "2021-09-07 17:18:19.735 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5530135631561279\n",
      "2021-09-07 17:18:19.736 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13160927593708038\n",
      "2021-09-07 17:18:19.737 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:19.739 | INFO     | src.policies:train:123 - Epoch 251 / 800\n",
      "2021-09-07 17:18:19.739 | INFO     | src.policies:collect_trajectories:221 - Episode 996\n",
      "2021-09-07 17:18:19.771 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:19.772 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:19.772 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:19.774 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:19.776 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.11464405059814453, 'baseline_loss': 0.47302618622779846, 'total_loss': 0.1218690425157547}\n",
      "2021-09-07 17:18:19.777 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2815066874027252\n",
      "2021-09-07 17:18:19.778 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7518688440322876\n",
      "2021-09-07 17:18:19.780 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2815066874027252\n",
      "2021-09-07 17:18:19.781 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:19.783 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:19.784 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.18641327321529388, 'baseline_loss': 0.43980416655540466, 'total_loss': 0.03348881006240845}\n",
      "2021-09-07 17:18:19.786 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.545568585395813\n",
      "2021-09-07 17:18:19.787 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4684796333312988\n",
      "2021-09-07 17:18:19.788 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:19.789 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:19.791 | INFO     | src.policies:train:123 - Epoch 252 / 800\n",
      "2021-09-07 17:18:19.791 | INFO     | src.policies:collect_trajectories:221 - Episode 997\n",
      "2021-09-07 17:18:19.819 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:19.820 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 166.0\n",
      "2021-09-07 17:18:19.820 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 166.0\n",
      "2021-09-07 17:18:19.821 | INFO     | src.policies:collect_trajectories:221 - Episode 998\n",
      "2021-09-07 17:18:19.852 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:19.853 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 198.0\n",
      "2021-09-07 17:18:19.853 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 182.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:19.854 | WARNING  | src.policies:train:144 - The actual batch size is 364, instead of 200\n",
      "2021-09-07 17:18:19.857 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:19.859 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.21299688518047333, 'baseline_loss': 0.45348572731018066, 'total_loss': 0.013745978474617004}\n",
      "2021-09-07 17:18:19.860 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5140014886856079\n",
      "2021-09-07 17:18:19.862 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.414684772491455\n",
      "2021-09-07 17:18:19.863 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999989867210388\n",
      "2021-09-07 17:18:19.864 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:19.866 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:19.867 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2532769441604614, 'baseline_loss': 0.42940816283226013, 'total_loss': -0.03857286274433136}\n",
      "2021-09-07 17:18:19.868 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.8327209949493408\n",
      "2021-09-07 17:18:19.869 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0572649240493774\n",
      "2021-09-07 17:18:19.870 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:19.871 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:19.873 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:19.874 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.25368136167526245, 'baseline_loss': 0.4866708517074585, 'total_loss': -0.010345935821533203}\n",
      "2021-09-07 17:18:19.875 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20090252161026\n",
      "2021-09-07 17:18:19.876 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8976889252662659\n",
      "2021-09-07 17:18:19.877 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20090252161026\n",
      "2021-09-07 17:18:19.878 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:19.879 | INFO     | src.policies:train:123 - Epoch 253 / 800\n",
      "2021-09-07 17:18:19.880 | INFO     | src.policies:collect_trajectories:221 - Episode 999\n",
      "2021-09-07 17:18:19.913 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:19.914 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:19.914 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:19.917 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:19.918 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.469842791557312, 'baseline_loss': 0.7300570011138916, 'total_loss': -0.10481429100036621}\n",
      "2021-09-07 17:18:19.919 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.352140337228775\n",
      "2021-09-07 17:18:19.920 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.41162002086639404\n",
      "2021-09-07 17:18:19.922 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.352140337228775\n",
      "2021-09-07 17:18:19.923 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.41162002086639404\n",
      "2021-09-07 17:18:19.925 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:19.926 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45856064558029175, 'baseline_loss': 0.7601785063743591, 'total_loss': -0.07847139239311218}\n",
      "2021-09-07 17:18:19.927 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10808075964450836\n",
      "2021-09-07 17:18:19.928 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5190555453300476\n",
      "2021-09-07 17:18:19.929 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10808075964450836\n",
      "2021-09-07 17:18:19.930 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:19.932 | INFO     | src.policies:train:123 - Epoch 254 / 800\n",
      "2021-09-07 17:18:19.932 | INFO     | src.policies:collect_trajectories:221 - Episode 1000\n",
      "2021-09-07 17:18:19.963 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:19.964 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:19.965 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:19.967 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:19.969 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5919605493545532, 'baseline_loss': 1.2650245428085327, 'total_loss': 0.040551722049713135}\n",
      "2021-09-07 17:18:19.970 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.29238468408584595\n",
      "2021-09-07 17:18:19.972 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5077335834503174\n",
      "2021-09-07 17:18:19.974 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.29238468408584595\n",
      "2021-09-07 17:18:19.975 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:19.976 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:19.978 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5373289585113525, 'baseline_loss': 1.2751610279083252, 'total_loss': 0.10025155544281006}\n",
      "2021-09-07 17:18:19.979 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10682719945907593\n",
      "2021-09-07 17:18:19.980 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8567404747009277\n",
      "2021-09-07 17:18:19.982 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10682719945907593\n",
      "2021-09-07 17:18:19.983 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:19.984 | INFO     | src.policies:train:123 - Epoch 255 / 800\n",
      "2021-09-07 17:18:19.985 | INFO     | src.policies:collect_trajectories:221 - Episode 1001\n",
      "2021-09-07 17:18:20.021 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:20.022 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:20.022 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:20.024 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:20.026 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8704128265380859, 'baseline_loss': 3.020354986190796, 'total_loss': 0.639764666557312}\n",
      "2021-09-07 17:18:20.027 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.36785459518432617\n",
      "2021-09-07 17:18:20.028 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.971686840057373\n",
      "2021-09-07 17:18:20.030 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.36785459518432617\n",
      "2021-09-07 17:18:20.031 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999999403953552\n",
      "2021-09-07 17:18:20.033 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:20.034 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8988014459609985, 'baseline_loss': 2.9222607612609863, 'total_loss': 0.5623289346694946}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:20.035 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5013110041618347\n",
      "2021-09-07 17:18:20.036 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.68612003326416\n",
      "2021-09-07 17:18:20.037 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999895691871643\n",
      "2021-09-07 17:18:20.039 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:20.098 | INFO     | src.policies:train:123 - Epoch 256 / 800\n",
      "2021-09-07 17:18:20.099 | INFO     | src.policies:collect_trajectories:221 - Episode 1002\n",
      "2021-09-07 17:18:20.131 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:20.132 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:20.132 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:20.134 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:20.136 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3694264590740204, 'baseline_loss': 0.802237868309021, 'total_loss': 0.03169247508049011}\n",
      "2021-09-07 17:18:20.137 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24616926908493042\n",
      "2021-09-07 17:18:20.138 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.764504075050354\n",
      "2021-09-07 17:18:20.139 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24616926908493042\n",
      "2021-09-07 17:18:20.141 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:20.142 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:20.144 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.49076133966445923, 'baseline_loss': 0.8315239548683167, 'total_loss': -0.0749993622303009}\n",
      "2021-09-07 17:18:20.146 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30659255385398865\n",
      "2021-09-07 17:18:20.147 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7770888209342957\n",
      "2021-09-07 17:18:20.148 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30659255385398865\n",
      "2021-09-07 17:18:20.149 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:20.150 | INFO     | src.policies:train:123 - Epoch 257 / 800\n",
      "2021-09-07 17:18:20.151 | INFO     | src.policies:collect_trajectories:221 - Episode 1003\n",
      "2021-09-07 17:18:20.181 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:20.182 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 172.0\n",
      "2021-09-07 17:18:20.182 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 172.0\n",
      "2021-09-07 17:18:20.182 | INFO     | src.policies:collect_trajectories:221 - Episode 1004\n",
      "2021-09-07 17:18:20.319 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:20.320 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:20.320 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 186.0\n",
      "2021-09-07 17:18:20.321 | WARNING  | src.policies:train:144 - The actual batch size is 372, instead of 200\n",
      "2021-09-07 17:18:20.323 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:20.325 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6416948437690735, 'baseline_loss': 2.5215044021606445, 'total_loss': 0.6190573573112488}\n",
      "2021-09-07 17:18:20.326 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2327745258808136\n",
      "2021-09-07 17:18:20.328 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.713559150695801\n",
      "2021-09-07 17:18:20.330 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2327745258808136\n",
      "2021-09-07 17:18:20.331 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:20.332 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:20.333 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6059023141860962, 'baseline_loss': 2.9309310913085938, 'total_loss': 0.8595632314682007}\n",
      "2021-09-07 17:18:20.334 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6799643635749817\n",
      "2021-09-07 17:18:20.335 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.1774306297302246\n",
      "2021-09-07 17:18:20.336 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:20.337 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:20.338 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:20.340 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7153059244155884, 'baseline_loss': 2.787400007247925, 'total_loss': 0.678394079208374}\n",
      "2021-09-07 17:18:20.341 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4124557673931122\n",
      "2021-09-07 17:18:20.342 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.8301193714141846\n",
      "2021-09-07 17:18:20.343 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4124557673931122\n",
      "2021-09-07 17:18:20.345 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:20.347 | INFO     | src.policies:train:123 - Epoch 258 / 800\n",
      "2021-09-07 17:18:20.347 | INFO     | src.policies:collect_trajectories:221 - Episode 1005\n",
      "2021-09-07 17:18:20.379 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:20.379 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:20.380 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:20.382 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:20.384 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.836406946182251, 'baseline_loss': 3.4111971855163574, 'total_loss': 0.8691916465759277}\n",
      "2021-09-07 17:18:20.385 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7407113313674927\n",
      "2021-09-07 17:18:20.386 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 5.144136905670166\n",
      "2021-09-07 17:18:20.389 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:20.390 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:20.392 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:20.393 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -1.0518085956573486, 'baseline_loss': 3.2735304832458496, 'total_loss': 0.5849566459655762}\n",
      "2021-09-07 17:18:20.394 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6230688691139221\n",
      "2021-09-07 17:18:20.395 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.686319828033447\n",
      "2021-09-07 17:18:20.396 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:20.397 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:20.398 | INFO     | src.policies:train:123 - Epoch 259 / 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:20.399 | INFO     | src.policies:collect_trajectories:221 - Episode 1006\n",
      "2021-09-07 17:18:20.416 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:20.416 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 104.0\n",
      "2021-09-07 17:18:20.417 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 104.0\n",
      "2021-09-07 17:18:20.417 | INFO     | src.policies:collect_trajectories:221 - Episode 1007\n",
      "2021-09-07 17:18:20.448 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:20.449 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:20.449 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 152.0\n",
      "2021-09-07 17:18:20.450 | WARNING  | src.policies:train:144 - The actual batch size is 304, instead of 200\n",
      "2021-09-07 17:18:20.452 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:20.454 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6417651176452637, 'baseline_loss': 1.4630465507507324, 'total_loss': 0.08975815773010254}\n",
      "2021-09-07 17:18:20.455 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20016874372959137\n",
      "2021-09-07 17:18:20.455 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4612926244735718\n",
      "2021-09-07 17:18:20.457 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20016874372959137\n",
      "2021-09-07 17:18:20.458 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:20.460 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:20.462 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7504603862762451, 'baseline_loss': 1.8020521402359009, 'total_loss': 0.15056568384170532}\n",
      "2021-09-07 17:18:20.463 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.9559929370880127\n",
      "2021-09-07 17:18:20.464 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.3972842693328857\n",
      "2021-09-07 17:18:20.466 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:20.467 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:20.469 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:20.470 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4504399597644806, 'baseline_loss': 1.2081990242004395, 'total_loss': 0.15365955233573914}\n",
      "2021-09-07 17:18:20.471 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20484204590320587\n",
      "2021-09-07 17:18:20.472 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.779790997505188\n",
      "2021-09-07 17:18:20.473 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20484204590320587\n",
      "2021-09-07 17:18:20.474 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:20.476 | INFO     | src.policies:train:123 - Epoch 260 / 800\n",
      "2021-09-07 17:18:20.476 | INFO     | src.policies:collect_trajectories:221 - Episode 1008\n",
      "2021-09-07 17:18:20.508 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:20.508 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:20.509 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:20.511 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:20.514 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23099231719970703, 'baseline_loss': 0.6101868748664856, 'total_loss': 0.07410112023353577}\n",
      "2021-09-07 17:18:20.515 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2264796942472458\n",
      "2021-09-07 17:18:20.516 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.347333312034607\n",
      "2021-09-07 17:18:20.518 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2264796942472458\n",
      "2021-09-07 17:18:20.520 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:20.521 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:20.522 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29547402262687683, 'baseline_loss': 0.5964627861976624, 'total_loss': 0.0027573704719543457}\n",
      "2021-09-07 17:18:20.523 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5932558178901672\n",
      "2021-09-07 17:18:20.525 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1574041843414307\n",
      "2021-09-07 17:18:20.526 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:20.527 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:20.529 | INFO     | src.policies:train:123 - Epoch 261 / 800\n",
      "2021-09-07 17:18:20.529 | INFO     | src.policies:collect_trajectories:221 - Episode 1009\n",
      "2021-09-07 17:18:20.550 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:20.551 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 127.0\n",
      "2021-09-07 17:18:20.551 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 127.0\n",
      "2021-09-07 17:18:20.552 | INFO     | src.policies:collect_trajectories:221 - Episode 1010\n",
      "2021-09-07 17:18:20.558 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:20.558 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 21.0\n",
      "2021-09-07 17:18:20.559 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 74.0\n",
      "2021-09-07 17:18:20.559 | INFO     | src.policies:collect_trajectories:221 - Episode 1011\n",
      "2021-09-07 17:18:20.591 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:20.592 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:20.592 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 116.0\n",
      "2021-09-07 17:18:20.593 | WARNING  | src.policies:train:144 - The actual batch size is 348, instead of 200\n",
      "2021-09-07 17:18:20.595 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:20.597 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8079180717468262, 'baseline_loss': 2.1534149646759033, 'total_loss': 0.2687894105911255}\n",
      "2021-09-07 17:18:20.598 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5848367214202881\n",
      "2021-09-07 17:18:20.599 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.4472925662994385\n",
      "2021-09-07 17:18:20.601 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:20.645 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:20.647 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:20.648 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6789515018463135, 'baseline_loss': 1.806616187095642, 'total_loss': 0.22435659170150757}\n",
      "2021-09-07 17:18:20.649 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5364471673965454\n",
      "2021-09-07 17:18:20.650 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8102089166641235\n",
      "2021-09-07 17:18:20.651 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990463256836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:20.652 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:20.653 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:20.655 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6352637410163879, 'baseline_loss': 1.8485995531082153, 'total_loss': 0.2890360355377197}\n",
      "2021-09-07 17:18:20.656 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16021248698234558\n",
      "2021-09-07 17:18:20.658 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.1023333072662354\n",
      "2021-09-07 17:18:20.659 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16021248698234558\n",
      "2021-09-07 17:18:20.660 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:20.662 | INFO     | src.policies:train:123 - Epoch 262 / 800\n",
      "2021-09-07 17:18:20.663 | INFO     | src.policies:collect_trajectories:221 - Episode 1012\n",
      "2021-09-07 17:18:20.695 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:20.695 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:20.696 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:20.698 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:20.701 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.30342161655426025, 'baseline_loss': 0.4561418294906616, 'total_loss': -0.07535070180892944}\n",
      "2021-09-07 17:18:20.703 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18872486054897308\n",
      "2021-09-07 17:18:20.704 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.989406406879425\n",
      "2021-09-07 17:18:20.706 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18872486054897308\n",
      "2021-09-07 17:18:20.707 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:20.708 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:20.710 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3158626854419708, 'baseline_loss': 0.47591689229011536, 'total_loss': -0.07790423929691315}\n",
      "2021-09-07 17:18:20.711 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1980893760919571\n",
      "2021-09-07 17:18:20.712 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3711620569229126\n",
      "2021-09-07 17:18:20.713 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1980893760919571\n",
      "2021-09-07 17:18:20.714 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:20.715 | INFO     | src.policies:train:123 - Epoch 263 / 800\n",
      "2021-09-07 17:18:20.716 | INFO     | src.policies:collect_trajectories:221 - Episode 1013\n",
      "2021-09-07 17:18:20.734 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:20.735 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 118.0\n",
      "2021-09-07 17:18:20.736 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 118.0\n",
      "2021-09-07 17:18:20.736 | INFO     | src.policies:collect_trajectories:221 - Episode 1014\n",
      "2021-09-07 17:18:20.768 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:20.768 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:20.769 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 159.0\n",
      "2021-09-07 17:18:20.769 | WARNING  | src.policies:train:144 - The actual batch size is 318, instead of 200\n",
      "2021-09-07 17:18:20.772 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:20.774 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.34900036454200745, 'baseline_loss': 0.744516909122467, 'total_loss': 0.023258090019226074}\n",
      "2021-09-07 17:18:20.775 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21653971076011658\n",
      "2021-09-07 17:18:20.776 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6122897863388062\n",
      "2021-09-07 17:18:20.777 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21653971076011658\n",
      "2021-09-07 17:18:20.778 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:20.779 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:20.781 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.28029951453208923, 'baseline_loss': 0.7021604776382446, 'total_loss': 0.07078072428703308}\n",
      "2021-09-07 17:18:20.782 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20518314838409424\n",
      "2021-09-07 17:18:20.783 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8797228336334229\n",
      "2021-09-07 17:18:20.785 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20518314838409424\n",
      "2021-09-07 17:18:20.786 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:20.788 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:20.789 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.33379852771759033, 'baseline_loss': 0.6283552050590515, 'total_loss': -0.019620925188064575}\n",
      "2021-09-07 17:18:20.790 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3525888919830322\n",
      "2021-09-07 17:18:20.791 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8863416314125061\n",
      "2021-09-07 17:18:20.792 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3525888919830322\n",
      "2021-09-07 17:18:20.793 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:20.795 | INFO     | src.policies:train:123 - Epoch 264 / 800\n",
      "2021-09-07 17:18:20.796 | INFO     | src.policies:collect_trajectories:221 - Episode 1015\n",
      "2021-09-07 17:18:20.827 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:20.827 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:20.828 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:20.830 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:20.832 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8466413021087646, 'baseline_loss': 2.8413994312286377, 'total_loss': 0.5740584135055542}\n",
      "2021-09-07 17:18:20.833 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5337817668914795\n",
      "2021-09-07 17:18:20.834 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.90891170501709\n",
      "2021-09-07 17:18:20.836 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999989867210388\n",
      "2021-09-07 17:18:20.837 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:20.839 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:20.840 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7975217700004578, 'baseline_loss': 3.0166683197021484, 'total_loss': 0.7108123898506165}\n",
      "2021-09-07 17:18:20.841 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3881438076496124\n",
      "2021-09-07 17:18:20.842 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.22019100189209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:20.843 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3881438076496124\n",
      "2021-09-07 17:18:20.845 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:20.846 | INFO     | src.policies:train:123 - Epoch 265 / 800\n",
      "2021-09-07 17:18:20.847 | INFO     | src.policies:collect_trajectories:221 - Episode 1016\n",
      "2021-09-07 17:18:20.877 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:20.878 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:20.878 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:20.881 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:20.883 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.14878083765506744, 'baseline_loss': 0.5227115750312805, 'total_loss': 0.11257494986057281}\n",
      "2021-09-07 17:18:20.884 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16789522767066956\n",
      "2021-09-07 17:18:20.886 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5765053033828735\n",
      "2021-09-07 17:18:20.888 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16789522767066956\n",
      "2021-09-07 17:18:20.889 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:20.890 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:20.891 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.19089215993881226, 'baseline_loss': 0.503400981426239, 'total_loss': 0.06080833077430725}\n",
      "2021-09-07 17:18:20.892 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10721655935049057\n",
      "2021-09-07 17:18:20.893 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1998540163040161\n",
      "2021-09-07 17:18:20.894 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10721655935049057\n",
      "2021-09-07 17:18:20.895 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:20.896 | INFO     | src.policies:train:123 - Epoch 266 / 800\n",
      "2021-09-07 17:18:20.897 | INFO     | src.policies:collect_trajectories:221 - Episode 1017\n",
      "2021-09-07 17:18:20.933 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:20.933 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 194.0\n",
      "2021-09-07 17:18:20.934 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 194.0\n",
      "2021-09-07 17:18:20.934 | INFO     | src.policies:collect_trajectories:221 - Episode 1018\n",
      "2021-09-07 17:18:20.969 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:20.970 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:20.970 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 197.0\n",
      "2021-09-07 17:18:20.971 | WARNING  | src.policies:train:144 - The actual batch size is 394, instead of 200\n",
      "2021-09-07 17:18:20.974 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:20.975 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6420902013778687, 'baseline_loss': 1.408850908279419, 'total_loss': 0.06233525276184082}\n",
      "2021-09-07 17:18:20.976 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21040533483028412\n",
      "2021-09-07 17:18:20.977 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4936152696609497\n",
      "2021-09-07 17:18:20.978 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21040533483028412\n",
      "2021-09-07 17:18:20.980 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:20.981 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:20.982 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5936362743377686, 'baseline_loss': 1.4267688989639282, 'total_loss': 0.11974817514419556}\n",
      "2021-09-07 17:18:20.983 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2783079445362091\n",
      "2021-09-07 17:18:20.985 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4857134819030762\n",
      "2021-09-07 17:18:20.987 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2783079445362091\n",
      "2021-09-07 17:18:20.988 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:20.989 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:20.990 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5944861769676208, 'baseline_loss': 1.2665324211120605, 'total_loss': 0.038780033588409424}\n",
      "2021-09-07 17:18:20.991 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17470191419124603\n",
      "2021-09-07 17:18:20.992 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0907353162765503\n",
      "2021-09-07 17:18:20.993 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17470191419124603\n",
      "2021-09-07 17:18:20.994 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:20.995 | INFO     | src.policies:train:123 - Epoch 267 / 800\n",
      "2021-09-07 17:18:20.996 | INFO     | src.policies:collect_trajectories:221 - Episode 1019\n",
      "2021-09-07 17:18:21.027 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:21.027 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:21.028 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:21.030 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:21.031 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7545908093452454, 'baseline_loss': 2.101149559020996, 'total_loss': 0.2959839701652527}\n",
      "2021-09-07 17:18:21.032 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4747996926307678\n",
      "2021-09-07 17:18:21.033 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.9895992279052734\n",
      "2021-09-07 17:18:21.034 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4747996926307678\n",
      "2021-09-07 17:18:21.035 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:21.037 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:21.038 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7223538756370544, 'baseline_loss': 1.863649606704712, 'total_loss': 0.2094709277153015}\n",
      "2021-09-07 17:18:21.039 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5558373332023621\n",
      "2021-09-07 17:18:21.041 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.208563804626465\n",
      "2021-09-07 17:18:21.043 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:21.044 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:21.046 | INFO     | src.policies:train:123 - Epoch 268 / 800\n",
      "2021-09-07 17:18:21.046 | INFO     | src.policies:collect_trajectories:221 - Episode 1020\n",
      "2021-09-07 17:18:21.078 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:21.079 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:21.079 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:21.082 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:21.083 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7921368479728699, 'baseline_loss': 2.3416764736175537, 'total_loss': 0.378701388835907}\n",
      "2021-09-07 17:18:21.084 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1639082133769989\n",
      "2021-09-07 17:18:21.086 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.400983810424805\n",
      "2021-09-07 17:18:21.087 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1639082133769989\n",
      "2021-09-07 17:18:21.088 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:21.089 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:21.090 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6799702644348145, 'baseline_loss': 2.1638567447662354, 'total_loss': 0.4019581079483032}\n",
      "2021-09-07 17:18:21.092 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1853833645582199\n",
      "2021-09-07 17:18:21.092 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.993922710418701\n",
      "2021-09-07 17:18:21.093 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1853833645582199\n",
      "2021-09-07 17:18:21.094 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:21.096 | INFO     | src.policies:train:123 - Epoch 269 / 800\n",
      "2021-09-07 17:18:21.096 | INFO     | src.policies:collect_trajectories:221 - Episode 1021\n",
      "2021-09-07 17:18:21.117 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:21.117 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 135.0\n",
      "2021-09-07 17:18:21.118 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 135.0\n",
      "2021-09-07 17:18:21.118 | INFO     | src.policies:collect_trajectories:221 - Episode 1022\n",
      "2021-09-07 17:18:21.136 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:21.137 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 99.0\n",
      "2021-09-07 17:18:21.137 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 117.0\n",
      "2021-09-07 17:18:21.137 | WARNING  | src.policies:train:144 - The actual batch size is 234, instead of 200\n",
      "2021-09-07 17:18:21.140 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:21.142 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5006601214408875, 'baseline_loss': 0.7845633625984192, 'total_loss': -0.10837844014167786}\n",
      "2021-09-07 17:18:21.143 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25567153096199036\n",
      "2021-09-07 17:18:21.144 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6673210263252258\n",
      "2021-09-07 17:18:21.145 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25567153096199036\n",
      "2021-09-07 17:18:21.146 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:21.147 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:21.148 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3808802664279938, 'baseline_loss': 0.800269603729248, 'total_loss': 0.01925453543663025}\n",
      "2021-09-07 17:18:21.149 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1835598647594452\n",
      "2021-09-07 17:18:21.156 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0973620414733887\n",
      "2021-09-07 17:18:21.287 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1835598647594452\n",
      "2021-09-07 17:18:21.289 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:21.291 | INFO     | src.policies:train:123 - Epoch 270 / 800\n",
      "2021-09-07 17:18:21.292 | INFO     | src.policies:collect_trajectories:221 - Episode 1023\n",
      "2021-09-07 17:18:21.326 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:21.327 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:21.328 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:21.330 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:21.332 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20396588742733002, 'baseline_loss': 0.5177007913589478, 'total_loss': 0.05488450825214386}\n",
      "2021-09-07 17:18:21.333 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20623204112052917\n",
      "2021-09-07 17:18:21.334 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.915968418121338\n",
      "2021-09-07 17:18:21.335 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20623204112052917\n",
      "2021-09-07 17:18:21.336 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:21.337 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:21.338 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.22316737473011017, 'baseline_loss': 0.5258252024650574, 'total_loss': 0.03974522650241852}\n",
      "2021-09-07 17:18:21.339 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16425541043281555\n",
      "2021-09-07 17:18:21.340 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4964545965194702\n",
      "2021-09-07 17:18:21.341 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16425541043281555\n",
      "2021-09-07 17:18:21.342 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:21.344 | INFO     | src.policies:train:123 - Epoch 271 / 800\n",
      "2021-09-07 17:18:21.344 | INFO     | src.policies:collect_trajectories:221 - Episode 1024\n",
      "2021-09-07 17:18:21.374 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:21.374 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:21.375 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:21.376 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:21.378 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4886225163936615, 'baseline_loss': 1.5480103492736816, 'total_loss': 0.2853826582431793}\n",
      "2021-09-07 17:18:21.379 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24799610674381256\n",
      "2021-09-07 17:18:21.380 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.4720051288604736\n",
      "2021-09-07 17:18:21.382 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24799610674381256\n",
      "2021-09-07 17:18:21.383 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:21.384 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:21.385 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5024267435073853, 'baseline_loss': 1.3164067268371582, 'total_loss': 0.15577661991119385}\n",
      "2021-09-07 17:18:21.386 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23468299210071564\n",
      "2021-09-07 17:18:21.387 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6279867887496948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:21.388 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23468299210071564\n",
      "2021-09-07 17:18:21.389 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:21.391 | INFO     | src.policies:train:123 - Epoch 272 / 800\n",
      "2021-09-07 17:18:21.391 | INFO     | src.policies:collect_trajectories:221 - Episode 1025\n",
      "2021-09-07 17:18:21.410 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:21.411 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 124.0\n",
      "2021-09-07 17:18:21.411 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 124.0\n",
      "2021-09-07 17:18:21.412 | INFO     | src.policies:collect_trajectories:221 - Episode 1026\n",
      "2021-09-07 17:18:21.444 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:21.445 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:21.446 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 162.0\n",
      "2021-09-07 17:18:21.447 | WARNING  | src.policies:train:144 - The actual batch size is 324, instead of 200\n",
      "2021-09-07 17:18:21.450 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:21.451 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.451772540807724, 'baseline_loss': 0.930148184299469, 'total_loss': 0.013301551342010498}\n",
      "2021-09-07 17:18:21.453 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11878795921802521\n",
      "2021-09-07 17:18:21.454 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9163764119148254\n",
      "2021-09-07 17:18:21.455 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11878795921802521\n",
      "2021-09-07 17:18:21.456 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:21.457 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:21.459 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3932134509086609, 'baseline_loss': 0.9664432406425476, 'total_loss': 0.09000816941261292}\n",
      "2021-09-07 17:18:21.460 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22839519381523132\n",
      "2021-09-07 17:18:21.461 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7287396788597107\n",
      "2021-09-07 17:18:21.463 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22839519381523132\n",
      "2021-09-07 17:18:21.464 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:21.466 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:21.467 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4632584750652313, 'baseline_loss': 0.9576563835144043, 'total_loss': 0.015569716691970825}\n",
      "2021-09-07 17:18:21.468 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24577683210372925\n",
      "2021-09-07 17:18:21.469 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.843185544013977\n",
      "2021-09-07 17:18:21.471 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24577683210372925\n",
      "2021-09-07 17:18:21.472 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:21.473 | INFO     | src.policies:train:123 - Epoch 273 / 800\n",
      "2021-09-07 17:18:21.474 | INFO     | src.policies:collect_trajectories:221 - Episode 1027\n",
      "2021-09-07 17:18:21.508 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:21.509 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:21.509 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:21.512 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:21.513 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.626473605632782, 'baseline_loss': 2.019171714782715, 'total_loss': 0.38311225175857544}\n",
      "2021-09-07 17:18:21.514 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4292834997177124\n",
      "2021-09-07 17:18:21.515 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.93807315826416\n",
      "2021-09-07 17:18:21.516 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4292834997177124\n",
      "2021-09-07 17:18:21.517 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:21.519 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:21.520 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6575939655303955, 'baseline_loss': 1.7712550163269043, 'total_loss': 0.22803354263305664}\n",
      "2021-09-07 17:18:21.522 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1588403582572937\n",
      "2021-09-07 17:18:21.523 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.421743869781494\n",
      "2021-09-07 17:18:21.524 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1588403582572937\n",
      "2021-09-07 17:18:21.525 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:21.527 | INFO     | src.policies:train:123 - Epoch 274 / 800\n",
      "2021-09-07 17:18:21.528 | INFO     | src.policies:collect_trajectories:221 - Episode 1028\n",
      "2021-09-07 17:18:21.561 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:21.562 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:21.563 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:21.565 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:21.567 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.07423834502696991, 'baseline_loss': 0.7127189636230469, 'total_loss': 0.28212112188339233}\n",
      "2021-09-07 17:18:21.568 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14325068891048431\n",
      "2021-09-07 17:18:21.569 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2961915731430054\n",
      "2021-09-07 17:18:21.570 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14325068891048431\n",
      "2021-09-07 17:18:21.572 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:21.573 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:21.574 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.028720207512378693, 'baseline_loss': 0.7830581665039062, 'total_loss': 0.36280888319015503}\n",
      "2021-09-07 17:18:21.575 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4401734471321106\n",
      "2021-09-07 17:18:21.576 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1232928037643433\n",
      "2021-09-07 17:18:21.577 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4401734471321106\n",
      "2021-09-07 17:18:21.578 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:21.580 | INFO     | src.policies:train:123 - Epoch 275 / 800\n",
      "2021-09-07 17:18:21.581 | INFO     | src.policies:collect_trajectories:221 - Episode 1029\n",
      "2021-09-07 17:18:21.615 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:21.616 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:21.616 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:21.618 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:21.621 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.30286672711372375, 'baseline_loss': 0.7161865830421448, 'total_loss': 0.05522656440734863}\n",
      "2021-09-07 17:18:21.623 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21945995092391968\n",
      "2021-09-07 17:18:21.624 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.098760962486267\n",
      "2021-09-07 17:18:21.625 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21945995092391968\n",
      "2021-09-07 17:18:21.627 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:21.628 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:21.629 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.33910053968429565, 'baseline_loss': 0.7577939629554749, 'total_loss': 0.03979644179344177}\n",
      "2021-09-07 17:18:21.630 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.160777285695076\n",
      "2021-09-07 17:18:21.631 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9234669208526611\n",
      "2021-09-07 17:18:21.632 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.160777285695076\n",
      "2021-09-07 17:18:21.633 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:21.635 | INFO     | src.policies:train:123 - Epoch 276 / 800\n",
      "2021-09-07 17:18:21.635 | INFO     | src.policies:collect_trajectories:221 - Episode 1030\n",
      "2021-09-07 17:18:21.670 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:21.670 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:21.671 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:21.673 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:21.674 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.47476670145988464, 'baseline_loss': 1.109607458114624, 'total_loss': 0.08003702759742737}\n",
      "2021-09-07 17:18:21.675 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.41549670696258545\n",
      "2021-09-07 17:18:21.676 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6163183450698853\n",
      "2021-09-07 17:18:21.678 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.41549670696258545\n",
      "2021-09-07 17:18:21.679 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:21.680 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:21.682 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.39716851711273193, 'baseline_loss': 0.8588234186172485, 'total_loss': 0.032243192195892334}\n",
      "2021-09-07 17:18:21.683 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3466864824295044\n",
      "2021-09-07 17:18:21.684 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.33623185753822327\n",
      "2021-09-07 17:18:21.685 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3466864824295044\n",
      "2021-09-07 17:18:21.686 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.33623185753822327\n",
      "2021-09-07 17:18:21.689 | INFO     | src.policies:train:123 - Epoch 277 / 800\n",
      "2021-09-07 17:18:21.689 | INFO     | src.policies:collect_trajectories:221 - Episode 1031\n",
      "2021-09-07 17:18:21.711 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:21.712 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 125.0\n",
      "2021-09-07 17:18:21.713 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 125.0\n",
      "2021-09-07 17:18:21.713 | INFO     | src.policies:collect_trajectories:221 - Episode 1032\n",
      "2021-09-07 17:18:21.733 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:21.734 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 114.0\n",
      "2021-09-07 17:18:21.734 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 119.5\n",
      "2021-09-07 17:18:21.735 | WARNING  | src.policies:train:144 - The actual batch size is 239, instead of 200\n",
      "2021-09-07 17:18:21.737 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:21.739 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.16239337623119354, 'baseline_loss': 0.5189387798309326, 'total_loss': 0.09707601368427277}\n",
      "2021-09-07 17:18:21.740 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12556983530521393\n",
      "2021-09-07 17:18:21.740 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6132899522781372\n",
      "2021-09-07 17:18:21.741 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12556983530521393\n",
      "2021-09-07 17:18:21.742 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:21.744 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:21.745 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.17163003981113434, 'baseline_loss': 0.4933680593967438, 'total_loss': 0.07505398988723755}\n",
      "2021-09-07 17:18:21.746 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24340347945690155\n",
      "2021-09-07 17:18:21.747 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.738679051399231\n",
      "2021-09-07 17:18:21.748 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24340347945690155\n",
      "2021-09-07 17:18:21.749 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:21.751 | INFO     | src.policies:train:123 - Epoch 278 / 800\n",
      "2021-09-07 17:18:21.751 | INFO     | src.policies:collect_trajectories:221 - Episode 1033\n",
      "2021-09-07 17:18:21.782 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:21.783 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:21.783 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:21.785 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:21.787 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.30644577741622925, 'baseline_loss': 0.8721820116043091, 'total_loss': 0.1296452283859253}\n",
      "2021-09-07 17:18:21.788 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2526620030403137\n",
      "2021-09-07 17:18:21.790 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6294667720794678\n",
      "2021-09-07 17:18:21.843 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2526620030403137\n",
      "2021-09-07 17:18:21.844 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:21.847 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:21.848 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43267184495925903, 'baseline_loss': 0.8490698337554932, 'total_loss': -0.008136928081512451}\n",
      "2021-09-07 17:18:21.849 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.29798436164855957\n",
      "2021-09-07 17:18:21.850 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3574005365371704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:21.851 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.29798436164855957\n",
      "2021-09-07 17:18:21.852 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:21.854 | INFO     | src.policies:train:123 - Epoch 279 / 800\n",
      "2021-09-07 17:18:21.854 | INFO     | src.policies:collect_trajectories:221 - Episode 1034\n",
      "2021-09-07 17:18:21.874 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:21.875 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 131.0\n",
      "2021-09-07 17:18:21.875 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 131.0\n",
      "2021-09-07 17:18:21.876 | INFO     | src.policies:collect_trajectories:221 - Episode 1035\n",
      "2021-09-07 17:18:21.907 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:21.907 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:21.908 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 165.5\n",
      "2021-09-07 17:18:21.908 | WARNING  | src.policies:train:144 - The actual batch size is 331, instead of 200\n",
      "2021-09-07 17:18:21.911 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:21.913 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3375392258167267, 'baseline_loss': 0.8435689806938171, 'total_loss': 0.08424526453018188}\n",
      "2021-09-07 17:18:21.914 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25790026783943176\n",
      "2021-09-07 17:18:21.915 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8909124135971069\n",
      "2021-09-07 17:18:21.916 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25790026783943176\n",
      "2021-09-07 17:18:21.917 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:21.918 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:21.919 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3738356828689575, 'baseline_loss': 0.7105629444122314, 'total_loss': -0.018554210662841797}\n",
      "2021-09-07 17:18:21.920 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4176865816116333\n",
      "2021-09-07 17:18:21.921 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8654106855392456\n",
      "2021-09-07 17:18:21.922 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4176865816116333\n",
      "2021-09-07 17:18:21.923 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:21.924 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:21.925 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.35838907957077026, 'baseline_loss': 0.8983807563781738, 'total_loss': 0.09080129861831665}\n",
      "2021-09-07 17:18:21.926 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.39893850684165955\n",
      "2021-09-07 17:18:21.927 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6945633292198181\n",
      "2021-09-07 17:18:21.928 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.39893850684165955\n",
      "2021-09-07 17:18:21.929 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:21.930 | INFO     | src.policies:train:123 - Epoch 280 / 800\n",
      "2021-09-07 17:18:21.931 | INFO     | src.policies:collect_trajectories:221 - Episode 1036\n",
      "2021-09-07 17:18:21.962 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:21.962 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:21.963 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:21.965 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:21.966 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7699623107910156, 'baseline_loss': 2.8737664222717285, 'total_loss': 0.6669209003448486}\n",
      "2021-09-07 17:18:21.967 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6160939931869507\n",
      "2021-09-07 17:18:21.968 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.123566150665283\n",
      "2021-09-07 17:18:21.969 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:21.971 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:21.972 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:21.973 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7895989418029785, 'baseline_loss': 2.8596484661102295, 'total_loss': 0.6402252912521362}\n",
      "2021-09-07 17:18:21.974 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4748293161392212\n",
      "2021-09-07 17:18:21.975 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.207336902618408\n",
      "2021-09-07 17:18:21.976 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4748293161392212\n",
      "2021-09-07 17:18:21.977 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:21.979 | INFO     | src.policies:train:123 - Epoch 281 / 800\n",
      "2021-09-07 17:18:21.979 | INFO     | src.policies:collect_trajectories:221 - Episode 1037\n",
      "2021-09-07 17:18:22.002 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.003 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 159.0\n",
      "2021-09-07 17:18:22.003 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 159.0\n",
      "2021-09-07 17:18:22.004 | INFO     | src.policies:collect_trajectories:221 - Episode 1038\n",
      "2021-09-07 17:18:22.026 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.027 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 130.0\n",
      "2021-09-07 17:18:22.027 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 144.5\n",
      "2021-09-07 17:18:22.028 | WARNING  | src.policies:train:144 - The actual batch size is 289, instead of 200\n",
      "2021-09-07 17:18:22.030 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:22.032 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24947988986968994, 'baseline_loss': 0.5744612216949463, 'total_loss': 0.0377507209777832}\n",
      "2021-09-07 17:18:22.033 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2138926237821579\n",
      "2021-09-07 17:18:22.034 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1260627508163452\n",
      "2021-09-07 17:18:22.035 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2138926237821579\n",
      "2021-09-07 17:18:22.036 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:22.037 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:22.038 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2856598198413849, 'baseline_loss': 0.6124166250228882, 'total_loss': 0.020548492670059204}\n",
      "2021-09-07 17:18:22.039 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.37774816155433655\n",
      "2021-09-07 17:18:22.040 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1451387405395508\n",
      "2021-09-07 17:18:22.041 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.37774816155433655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:22.042 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:22.044 | INFO     | src.policies:train:123 - Epoch 282 / 800\n",
      "2021-09-07 17:18:22.044 | INFO     | src.policies:collect_trajectories:221 - Episode 1039\n",
      "2021-09-07 17:18:22.064 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.065 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 136.0\n",
      "2021-09-07 17:18:22.065 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 136.0\n",
      "2021-09-07 17:18:22.066 | INFO     | src.policies:collect_trajectories:221 - Episode 1040\n",
      "2021-09-07 17:18:22.083 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.084 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 108.0\n",
      "2021-09-07 17:18:22.084 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 122.0\n",
      "2021-09-07 17:18:22.085 | WARNING  | src.policies:train:144 - The actual batch size is 244, instead of 200\n",
      "2021-09-07 17:18:22.087 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:22.089 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.255312442779541, 'baseline_loss': 0.4703429341316223, 'total_loss': -0.02014097571372986}\n",
      "2021-09-07 17:18:22.090 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23054417967796326\n",
      "2021-09-07 17:18:22.091 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2512726783752441\n",
      "2021-09-07 17:18:22.092 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23054417967796326\n",
      "2021-09-07 17:18:22.093 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:22.094 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:22.096 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31520405411720276, 'baseline_loss': 0.48087185621261597, 'total_loss': -0.07476812601089478}\n",
      "2021-09-07 17:18:22.097 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3224407732486725\n",
      "2021-09-07 17:18:22.098 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3331660032272339\n",
      "2021-09-07 17:18:22.098 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3224407732486725\n",
      "2021-09-07 17:18:22.100 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:22.101 | INFO     | src.policies:train:123 - Epoch 283 / 800\n",
      "2021-09-07 17:18:22.102 | INFO     | src.policies:collect_trajectories:221 - Episode 1041\n",
      "2021-09-07 17:18:22.125 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.126 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 143.0\n",
      "2021-09-07 17:18:22.126 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 143.0\n",
      "2021-09-07 17:18:22.126 | INFO     | src.policies:collect_trajectories:221 - Episode 1042\n",
      "2021-09-07 17:18:22.148 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.148 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 118.0\n",
      "2021-09-07 17:18:22.149 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 130.5\n",
      "2021-09-07 17:18:22.149 | WARNING  | src.policies:train:144 - The actual batch size is 261, instead of 200\n",
      "2021-09-07 17:18:22.152 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:22.154 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5003395676612854, 'baseline_loss': 0.8917789459228516, 'total_loss': -0.05445009469985962}\n",
      "2021-09-07 17:18:22.155 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.36089882254600525\n",
      "2021-09-07 17:18:22.156 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8124228715896606\n",
      "2021-09-07 17:18:22.157 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.36089882254600525\n",
      "2021-09-07 17:18:22.158 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:22.160 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:22.161 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.551500141620636, 'baseline_loss': 1.042295217514038, 'total_loss': -0.030352532863616943}\n",
      "2021-09-07 17:18:22.162 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4069364070892334\n",
      "2021-09-07 17:18:22.163 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9961085319519043\n",
      "2021-09-07 17:18:22.164 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4069364070892334\n",
      "2021-09-07 17:18:22.165 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:22.166 | INFO     | src.policies:train:123 - Epoch 284 / 800\n",
      "2021-09-07 17:18:22.166 | INFO     | src.policies:collect_trajectories:221 - Episode 1043\n",
      "2021-09-07 17:18:22.196 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.196 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:22.197 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:22.199 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:22.201 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6375578045845032, 'baseline_loss': 2.4870901107788086, 'total_loss': 0.6059872508049011}\n",
      "2021-09-07 17:18:22.202 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30927732586860657\n",
      "2021-09-07 17:18:22.203 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.1553702354431152\n",
      "2021-09-07 17:18:22.204 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30927732586860657\n",
      "2021-09-07 17:18:22.205 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:22.206 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:22.207 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5815695524215698, 'baseline_loss': 2.254563093185425, 'total_loss': 0.5457119941711426}\n",
      "2021-09-07 17:18:22.208 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15795937180519104\n",
      "2021-09-07 17:18:22.209 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.2777881622314453\n",
      "2021-09-07 17:18:22.210 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15795937180519104\n",
      "2021-09-07 17:18:22.211 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:22.212 | INFO     | src.policies:train:123 - Epoch 285 / 800\n",
      "2021-09-07 17:18:22.213 | INFO     | src.policies:collect_trajectories:221 - Episode 1044\n",
      "2021-09-07 17:18:22.231 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.231 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 117.0\n",
      "2021-09-07 17:18:22.232 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 117.0\n",
      "2021-09-07 17:18:22.232 | INFO     | src.policies:collect_trajectories:221 - Episode 1045\n",
      "2021-09-07 17:18:22.260 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.260 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 175.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:22.261 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 146.0\n",
      "2021-09-07 17:18:22.261 | WARNING  | src.policies:train:144 - The actual batch size is 292, instead of 200\n",
      "2021-09-07 17:18:22.264 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:22.266 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4249342381954193, 'baseline_loss': 0.9055987596511841, 'total_loss': 0.02786514163017273}\n",
      "2021-09-07 17:18:22.266 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3636590540409088\n",
      "2021-09-07 17:18:22.267 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9418933987617493\n",
      "2021-09-07 17:18:22.268 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3636590540409088\n",
      "2021-09-07 17:18:22.269 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:22.270 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:22.272 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.30115988850593567, 'baseline_loss': 0.6939590573310852, 'total_loss': 0.045819640159606934}\n",
      "2021-09-07 17:18:22.272 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16531507670879364\n",
      "2021-09-07 17:18:22.273 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5106968283653259\n",
      "2021-09-07 17:18:22.274 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16531507670879364\n",
      "2021-09-07 17:18:22.275 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:22.277 | INFO     | src.policies:train:123 - Epoch 286 / 800\n",
      "2021-09-07 17:18:22.277 | INFO     | src.policies:collect_trajectories:221 - Episode 1046\n",
      "2021-09-07 17:18:22.433 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.434 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:22.435 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:22.437 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:22.439 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5552916526794434, 'baseline_loss': 1.2891747951507568, 'total_loss': 0.08929574489593506}\n",
      "2021-09-07 17:18:22.441 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23861314356327057\n",
      "2021-09-07 17:18:22.442 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4650518894195557\n",
      "2021-09-07 17:18:22.444 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23861314356327057\n",
      "2021-09-07 17:18:22.445 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:22.446 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:22.447 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5212677121162415, 'baseline_loss': 1.0638577938079834, 'total_loss': 0.010661184787750244}\n",
      "2021-09-07 17:18:22.448 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1902538537979126\n",
      "2021-09-07 17:18:22.449 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8121569156646729\n",
      "2021-09-07 17:18:22.450 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1902538537979126\n",
      "2021-09-07 17:18:22.450 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:22.452 | INFO     | src.policies:train:123 - Epoch 287 / 800\n",
      "2021-09-07 17:18:22.452 | INFO     | src.policies:collect_trajectories:221 - Episode 1047\n",
      "2021-09-07 17:18:22.481 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.481 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 182.0\n",
      "2021-09-07 17:18:22.482 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 182.0\n",
      "2021-09-07 17:18:22.482 | INFO     | src.policies:collect_trajectories:221 - Episode 1048\n",
      "2021-09-07 17:18:22.514 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.514 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:22.515 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 191.0\n",
      "2021-09-07 17:18:22.515 | WARNING  | src.policies:train:144 - The actual batch size is 382, instead of 200\n",
      "2021-09-07 17:18:22.518 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:22.521 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6697143316268921, 'baseline_loss': 1.331032395362854, 'total_loss': -0.004198133945465088}\n",
      "2021-09-07 17:18:22.522 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.27105048298835754\n",
      "2021-09-07 17:18:22.523 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.036660075187683\n",
      "2021-09-07 17:18:22.524 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.27105048298835754\n",
      "2021-09-07 17:18:22.525 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:22.526 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:22.528 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4594047963619232, 'baseline_loss': 1.4450877904891968, 'total_loss': 0.26313909888267517}\n",
      "2021-09-07 17:18:22.529 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31648990511894226\n",
      "2021-09-07 17:18:22.530 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4959721565246582\n",
      "2021-09-07 17:18:22.531 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31648990511894226\n",
      "2021-09-07 17:18:22.533 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:22.534 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:22.535 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5591714382171631, 'baseline_loss': 1.3747153282165527, 'total_loss': 0.12818622589111328}\n",
      "2021-09-07 17:18:22.536 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1706404983997345\n",
      "2021-09-07 17:18:22.536 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1762354373931885\n",
      "2021-09-07 17:18:22.537 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1706404983997345\n",
      "2021-09-07 17:18:22.539 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:22.540 | INFO     | src.policies:train:123 - Epoch 288 / 800\n",
      "2021-09-07 17:18:22.540 | INFO     | src.policies:collect_trajectories:221 - Episode 1049\n",
      "2021-09-07 17:18:22.562 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.563 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 145.0\n",
      "2021-09-07 17:18:22.563 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 145.0\n",
      "2021-09-07 17:18:22.564 | INFO     | src.policies:collect_trajectories:221 - Episode 1050\n",
      "2021-09-07 17:18:22.588 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.589 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 155.0\n",
      "2021-09-07 17:18:22.589 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 150.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:22.590 | WARNING  | src.policies:train:144 - The actual batch size is 300, instead of 200\n",
      "2021-09-07 17:18:22.592 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:22.594 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.19629985094070435, 'baseline_loss': 0.47057095170021057, 'total_loss': 0.03898562490940094}\n",
      "2021-09-07 17:18:22.595 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16877257823944092\n",
      "2021-09-07 17:18:22.596 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6679171323776245\n",
      "2021-09-07 17:18:22.597 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16877257823944092\n",
      "2021-09-07 17:18:22.598 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:22.599 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:22.601 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27599990367889404, 'baseline_loss': 0.4777691960334778, 'total_loss': -0.03711530566215515}\n",
      "2021-09-07 17:18:22.602 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3051939308643341\n",
      "2021-09-07 17:18:22.603 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0822733640670776\n",
      "2021-09-07 17:18:22.604 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3051939308643341\n",
      "2021-09-07 17:18:22.606 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:22.607 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:22.609 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2387586385011673, 'baseline_loss': 0.47985076904296875, 'total_loss': 0.0011667460203170776}\n",
      "2021-09-07 17:18:22.609 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6457012295722961\n",
      "2021-09-07 17:18:22.610 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2749009132385254\n",
      "2021-09-07 17:18:22.611 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:22.612 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:22.614 | INFO     | src.policies:train:123 - Epoch 289 / 800\n",
      "2021-09-07 17:18:22.614 | INFO     | src.policies:collect_trajectories:221 - Episode 1051\n",
      "2021-09-07 17:18:22.630 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.631 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 101.0\n",
      "2021-09-07 17:18:22.631 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 101.0\n",
      "2021-09-07 17:18:22.631 | INFO     | src.policies:collect_trajectories:221 - Episode 1052\n",
      "2021-09-07 17:18:22.668 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.669 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:22.669 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 150.5\n",
      "2021-09-07 17:18:22.670 | WARNING  | src.policies:train:144 - The actual batch size is 301, instead of 200\n",
      "2021-09-07 17:18:22.672 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:22.675 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3591110110282898, 'baseline_loss': 0.8634452819824219, 'total_loss': 0.07261162996292114}\n",
      "2021-09-07 17:18:22.676 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3273247182369232\n",
      "2021-09-07 17:18:22.677 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7333750128746033\n",
      "2021-09-07 17:18:22.678 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3273247182369232\n",
      "2021-09-07 17:18:22.679 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:22.681 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:22.683 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4748348593711853, 'baseline_loss': 0.9457860589027405, 'total_loss': -0.0019418299198150635}\n",
      "2021-09-07 17:18:22.684 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.37944573163986206\n",
      "2021-09-07 17:18:22.686 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7287243008613586\n",
      "2021-09-07 17:18:22.687 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.37944573163986206\n",
      "2021-09-07 17:18:22.688 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:22.689 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:22.691 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3441910445690155, 'baseline_loss': 0.8264798521995544, 'total_loss': 0.06904888153076172}\n",
      "2021-09-07 17:18:22.692 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11429088562726974\n",
      "2021-09-07 17:18:22.693 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5930827856063843\n",
      "2021-09-07 17:18:22.694 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11429088562726974\n",
      "2021-09-07 17:18:22.695 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:22.697 | INFO     | src.policies:train:123 - Epoch 290 / 800\n",
      "2021-09-07 17:18:22.697 | INFO     | src.policies:collect_trajectories:221 - Episode 1053\n",
      "2021-09-07 17:18:22.734 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.735 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:22.735 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:22.739 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:22.741 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40656742453575134, 'baseline_loss': 1.2278454303741455, 'total_loss': 0.2073552906513214}\n",
      "2021-09-07 17:18:22.742 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.0830002874135971\n",
      "2021-09-07 17:18:22.743 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.06790292263031\n",
      "2021-09-07 17:18:22.744 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.0830002874135971\n",
      "2021-09-07 17:18:22.746 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:22.747 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:22.748 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40319275856018066, 'baseline_loss': 1.245526671409607, 'total_loss': 0.2195705771446228}\n",
      "2021-09-07 17:18:22.750 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24030359089374542\n",
      "2021-09-07 17:18:22.751 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8794941902160645\n",
      "2021-09-07 17:18:22.752 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24030359089374542\n",
      "2021-09-07 17:18:22.753 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:22.755 | INFO     | src.policies:train:123 - Epoch 291 / 800\n",
      "2021-09-07 17:18:22.755 | INFO     | src.policies:collect_trajectories:221 - Episode 1054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:22.789 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.790 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:22.790 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:22.793 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:22.795 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.13593655824661255, 'baseline_loss': 0.4831361770629883, 'total_loss': 0.10563153028488159}\n",
      "2021-09-07 17:18:22.796 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23522652685642242\n",
      "2021-09-07 17:18:22.797 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7093541622161865\n",
      "2021-09-07 17:18:22.799 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23522652685642242\n",
      "2021-09-07 17:18:22.800 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:22.802 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:22.803 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.11341294646263123, 'baseline_loss': 0.5199022889137268, 'total_loss': 0.14653819799423218}\n",
      "2021-09-07 17:18:22.804 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18875044584274292\n",
      "2021-09-07 17:18:22.805 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.2823424339294434\n",
      "2021-09-07 17:18:22.806 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18875044584274292\n",
      "2021-09-07 17:18:22.808 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:22.809 | INFO     | src.policies:train:123 - Epoch 292 / 800\n",
      "2021-09-07 17:18:22.810 | INFO     | src.policies:collect_trajectories:221 - Episode 1055\n",
      "2021-09-07 17:18:22.847 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.848 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:22.848 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:22.851 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:22.852 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29003944993019104, 'baseline_loss': 0.6318231821060181, 'total_loss': 0.025872141122817993}\n",
      "2021-09-07 17:18:22.853 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11509928107261658\n",
      "2021-09-07 17:18:22.855 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7224150896072388\n",
      "2021-09-07 17:18:22.856 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11509928107261658\n",
      "2021-09-07 17:18:22.857 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:22.858 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:22.860 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.47013095021247864, 'baseline_loss': 0.8395522832870483, 'total_loss': -0.05035480856895447}\n",
      "2021-09-07 17:18:22.861 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4221152663230896\n",
      "2021-09-07 17:18:22.863 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6425461769104004\n",
      "2021-09-07 17:18:22.864 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4221152663230896\n",
      "2021-09-07 17:18:22.865 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:22.867 | INFO     | src.policies:train:123 - Epoch 293 / 800\n",
      "2021-09-07 17:18:22.867 | INFO     | src.policies:collect_trajectories:221 - Episode 1056\n",
      "2021-09-07 17:18:22.903 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:22.904 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:22.904 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:22.908 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:22.909 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1736501306295395, 'baseline_loss': 0.43862518668174744, 'total_loss': 0.04566246271133423}\n",
      "2021-09-07 17:18:22.911 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1527983695268631\n",
      "2021-09-07 17:18:22.912 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4884941577911377\n",
      "2021-09-07 17:18:22.913 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1527983695268631\n",
      "2021-09-07 17:18:22.915 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:22.916 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:22.917 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31729477643966675, 'baseline_loss': 0.44023972749710083, 'total_loss': -0.09717491269111633}\n",
      "2021-09-07 17:18:22.918 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1875121146440506\n",
      "2021-09-07 17:18:22.920 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1536309719085693\n",
      "2021-09-07 17:18:22.921 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1875121146440506\n",
      "2021-09-07 17:18:22.922 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:22.924 | INFO     | src.policies:train:123 - Epoch 294 / 800\n",
      "2021-09-07 17:18:22.925 | INFO     | src.policies:collect_trajectories:221 - Episode 1057\n",
      "2021-09-07 17:18:23.016 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:23.018 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:23.018 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:23.021 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:23.023 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4359794557094574, 'baseline_loss': 0.7980521321296692, 'total_loss': -0.0369533896446228}\n",
      "2021-09-07 17:18:23.024 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3112046718597412\n",
      "2021-09-07 17:18:23.025 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.46861734986305237\n",
      "2021-09-07 17:18:23.026 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3112046718597412\n",
      "2021-09-07 17:18:23.027 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.46861734986305237\n",
      "2021-09-07 17:18:23.029 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:23.030 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.402151495218277, 'baseline_loss': 0.6987348794937134, 'total_loss': -0.05278405547142029}\n",
      "2021-09-07 17:18:23.031 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2680743634700775\n",
      "2021-09-07 17:18:23.032 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9181492924690247\n",
      "2021-09-07 17:18:23.033 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2680743634700775\n",
      "2021-09-07 17:18:23.034 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:23.035 | INFO     | src.policies:train:123 - Epoch 295 / 800\n",
      "2021-09-07 17:18:23.036 | INFO     | src.policies:collect_trajectories:221 - Episode 1058\n",
      "2021-09-07 17:18:23.067 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:23.068 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:23.068 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:23.070 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:23.073 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31735873222351074, 'baseline_loss': 0.43542158603668213, 'total_loss': -0.09964793920516968}\n",
      "2021-09-07 17:18:23.073 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18790099024772644\n",
      "2021-09-07 17:18:23.074 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.111680507659912\n",
      "2021-09-07 17:18:23.075 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18790099024772644\n",
      "2021-09-07 17:18:23.076 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:23.077 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:23.079 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1863013058900833, 'baseline_loss': 0.4497649371623993, 'total_loss': 0.03858116269111633}\n",
      "2021-09-07 17:18:23.080 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10002580285072327\n",
      "2021-09-07 17:18:23.080 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1467928886413574\n",
      "2021-09-07 17:18:23.081 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10002580285072327\n",
      "2021-09-07 17:18:23.082 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:23.084 | INFO     | src.policies:train:123 - Epoch 296 / 800\n",
      "2021-09-07 17:18:23.084 | INFO     | src.policies:collect_trajectories:221 - Episode 1059\n",
      "2021-09-07 17:18:23.114 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:23.114 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:23.115 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:23.117 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:23.119 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24441419541835785, 'baseline_loss': 0.3983478844165802, 'total_loss': -0.04524025321006775}\n",
      "2021-09-07 17:18:23.120 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.27803245186805725\n",
      "2021-09-07 17:18:23.121 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.445330262184143\n",
      "2021-09-07 17:18:23.122 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.27803245186805725\n",
      "2021-09-07 17:18:23.123 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:23.124 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:23.126 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3189338445663452, 'baseline_loss': 0.4629038870334625, 'total_loss': -0.08748190104961395}\n",
      "2021-09-07 17:18:23.126 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6892750263214111\n",
      "2021-09-07 17:18:23.128 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6106988191604614\n",
      "2021-09-07 17:18:23.129 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:23.130 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:23.131 | INFO     | src.policies:train:123 - Epoch 297 / 800\n",
      "2021-09-07 17:18:23.131 | INFO     | src.policies:collect_trajectories:221 - Episode 1060\n",
      "2021-09-07 17:18:23.161 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:23.162 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:23.162 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:23.164 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:23.166 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40674296021461487, 'baseline_loss': 0.9949787855148315, 'total_loss': 0.0907464325428009}\n",
      "2021-09-07 17:18:23.167 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5101515650749207\n",
      "2021-09-07 17:18:23.168 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.279263973236084\n",
      "2021-09-07 17:18:23.170 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:23.171 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:23.172 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:23.173 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3671709895133972, 'baseline_loss': 0.9725606441497803, 'total_loss': 0.11910933256149292}\n",
      "2021-09-07 17:18:23.174 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.29340389370918274\n",
      "2021-09-07 17:18:23.175 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7268267869949341\n",
      "2021-09-07 17:18:23.176 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.29340389370918274\n",
      "2021-09-07 17:18:23.177 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:23.179 | INFO     | src.policies:train:123 - Epoch 298 / 800\n",
      "2021-09-07 17:18:23.180 | INFO     | src.policies:collect_trajectories:221 - Episode 1061\n",
      "2021-09-07 17:18:23.211 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:23.212 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:23.213 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:23.215 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:23.217 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.662451982498169, 'baseline_loss': 1.6956405639648438, 'total_loss': 0.18536829948425293}\n",
      "2021-09-07 17:18:23.218 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17678585648536682\n",
      "2021-09-07 17:18:23.219 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.396328926086426\n",
      "2021-09-07 17:18:23.220 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17678585648536682\n",
      "2021-09-07 17:18:23.222 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:23.223 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:23.224 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5440781712532043, 'baseline_loss': 1.1198838949203491, 'total_loss': 0.015863776206970215}\n",
      "2021-09-07 17:18:23.225 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2872466444969177\n",
      "2021-09-07 17:18:23.226 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4427262544631958\n",
      "2021-09-07 17:18:23.227 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2872466444969177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:23.228 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:23.229 | INFO     | src.policies:train:123 - Epoch 299 / 800\n",
      "2021-09-07 17:18:23.230 | INFO     | src.policies:collect_trajectories:221 - Episode 1062\n",
      "2021-09-07 17:18:23.259 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:23.259 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:23.260 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:23.262 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:23.264 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4234309494495392, 'baseline_loss': 1.2459063529968262, 'total_loss': 0.1995222270488739}\n",
      "2021-09-07 17:18:23.264 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4644346833229065\n",
      "2021-09-07 17:18:23.265 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2047885656356812\n",
      "2021-09-07 17:18:23.266 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4644346833229065\n",
      "2021-09-07 17:18:23.267 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:23.269 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:23.270 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4358096718788147, 'baseline_loss': 1.1433662176132202, 'total_loss': 0.1358734369277954}\n",
      "2021-09-07 17:18:23.270 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24198567867279053\n",
      "2021-09-07 17:18:23.271 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.212769627571106\n",
      "2021-09-07 17:18:23.272 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24198567867279053\n",
      "2021-09-07 17:18:23.273 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:23.275 | INFO     | src.policies:train:123 - Epoch 300 / 800\n",
      "2021-09-07 17:18:23.275 | INFO     | src.policies:collect_trajectories:221 - Episode 1063\n",
      "2021-09-07 17:18:23.305 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:23.306 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:23.306 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:23.308 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:23.310 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8978262543678284, 'baseline_loss': 3.415950298309326, 'total_loss': 0.8101488947868347}\n",
      "2021-09-07 17:18:23.311 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4634680449962616\n",
      "2021-09-07 17:18:23.312 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.752222061157227\n",
      "2021-09-07 17:18:23.313 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4634680449962616\n",
      "2021-09-07 17:18:23.314 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999999403953552\n",
      "2021-09-07 17:18:23.316 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:23.317 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7617610692977905, 'baseline_loss': 3.3464725017547607, 'total_loss': 0.9114751815795898}\n",
      "2021-09-07 17:18:23.317 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.43205350637435913\n",
      "2021-09-07 17:18:23.318 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.844629764556885\n",
      "2021-09-07 17:18:23.319 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.43205350637435913\n",
      "2021-09-07 17:18:23.320 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:23.322 | INFO     | src.policies:train:123 - Epoch 301 / 800\n",
      "2021-09-07 17:18:23.322 | INFO     | src.policies:collect_trajectories:221 - Episode 1064\n",
      "2021-09-07 17:18:23.339 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:23.339 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 100.0\n",
      "2021-09-07 17:18:23.340 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 100.0\n",
      "2021-09-07 17:18:23.340 | INFO     | src.policies:collect_trajectories:221 - Episode 1065\n",
      "2021-09-07 17:18:23.372 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:23.373 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:23.373 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 150.0\n",
      "2021-09-07 17:18:23.374 | WARNING  | src.policies:train:144 - The actual batch size is 300, instead of 200\n",
      "2021-09-07 17:18:23.376 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:23.378 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.390047162771225, 'baseline_loss': 0.8024761080741882, 'total_loss': 0.01119089126586914}\n",
      "2021-09-07 17:18:23.379 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5999379754066467\n",
      "2021-09-07 17:18:23.380 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8253486752510071\n",
      "2021-09-07 17:18:23.381 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:23.382 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:23.383 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:23.384 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.35823875665664673, 'baseline_loss': 0.8636388182640076, 'total_loss': 0.07358065247535706}\n",
      "2021-09-07 17:18:23.385 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13519243896007538\n",
      "2021-09-07 17:18:23.386 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9059783220291138\n",
      "2021-09-07 17:18:23.387 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13519243896007538\n",
      "2021-09-07 17:18:23.388 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:23.389 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:23.390 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29871153831481934, 'baseline_loss': 0.8365028500556946, 'total_loss': 0.11953988671302795}\n",
      "2021-09-07 17:18:23.391 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15953600406646729\n",
      "2021-09-07 17:18:23.392 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8254818320274353\n",
      "2021-09-07 17:18:23.393 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15953600406646729\n",
      "2021-09-07 17:18:23.394 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:23.395 | INFO     | src.policies:train:123 - Epoch 302 / 800\n",
      "2021-09-07 17:18:23.396 | INFO     | src.policies:collect_trajectories:221 - Episode 1066\n",
      "2021-09-07 17:18:23.427 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:23.428 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:23.429 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:23.431 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:23.434 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3927168548107147, 'baseline_loss': 1.092500925064087, 'total_loss': 0.15353360772132874}\n",
      "2021-09-07 17:18:23.435 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14398762583732605\n",
      "2021-09-07 17:18:23.436 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.656749427318573\n",
      "2021-09-07 17:18:23.437 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14398762583732605\n",
      "2021-09-07 17:18:23.438 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:23.439 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:23.441 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.389823317527771, 'baseline_loss': 0.8721413612365723, 'total_loss': 0.04624736309051514}\n",
      "2021-09-07 17:18:23.442 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.45199882984161377\n",
      "2021-09-07 17:18:23.444 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7807308435440063\n",
      "2021-09-07 17:18:23.445 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.45199882984161377\n",
      "2021-09-07 17:18:23.446 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:23.447 | INFO     | src.policies:train:123 - Epoch 303 / 800\n",
      "2021-09-07 17:18:23.448 | INFO     | src.policies:collect_trajectories:221 - Episode 1067\n",
      "2021-09-07 17:18:23.480 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:23.481 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:23.481 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:23.484 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:23.486 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5775233507156372, 'baseline_loss': 1.4114527702331543, 'total_loss': 0.12820303440093994}\n",
      "2021-09-07 17:18:23.487 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17563925683498383\n",
      "2021-09-07 17:18:23.488 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.673114538192749\n",
      "2021-09-07 17:18:23.489 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17563925683498383\n",
      "2021-09-07 17:18:23.490 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:23.492 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:23.631 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6100768446922302, 'baseline_loss': 1.5184191465377808, 'total_loss': 0.14913272857666016}\n",
      "2021-09-07 17:18:23.632 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10237835347652435\n",
      "2021-09-07 17:18:23.634 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.890596866607666\n",
      "2021-09-07 17:18:23.636 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10237835347652435\n",
      "2021-09-07 17:18:23.637 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:23.639 | INFO     | src.policies:train:123 - Epoch 304 / 800\n",
      "2021-09-07 17:18:23.640 | INFO     | src.policies:collect_trajectories:221 - Episode 1068\n",
      "2021-09-07 17:18:23.675 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:23.676 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:23.676 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:23.678 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:23.681 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.682155430316925, 'baseline_loss': 2.068781852722168, 'total_loss': 0.35223549604415894}\n",
      "2021-09-07 17:18:23.682 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3371129333972931\n",
      "2021-09-07 17:18:23.683 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.393500566482544\n",
      "2021-09-07 17:18:23.684 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3371129333972931\n",
      "2021-09-07 17:18:23.685 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:23.687 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:23.688 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5790164470672607, 'baseline_loss': 1.8351582288742065, 'total_loss': 0.33856266736984253}\n",
      "2021-09-07 17:18:23.689 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1428024172782898\n",
      "2021-09-07 17:18:23.690 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.1554551124572754\n",
      "2021-09-07 17:18:23.692 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1428024172782898\n",
      "2021-09-07 17:18:23.693 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:23.694 | INFO     | src.policies:train:123 - Epoch 305 / 800\n",
      "2021-09-07 17:18:23.695 | INFO     | src.policies:collect_trajectories:221 - Episode 1069\n",
      "2021-09-07 17:18:23.729 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:23.730 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:23.731 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:23.733 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:23.736 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3410956561565399, 'baseline_loss': 0.7872409820556641, 'total_loss': 0.052524834871292114}\n",
      "2021-09-07 17:18:23.737 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23197871446609497\n",
      "2021-09-07 17:18:23.738 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2220652103424072\n",
      "2021-09-07 17:18:23.739 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23197871446609497\n",
      "2021-09-07 17:18:23.741 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:23.743 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:23.745 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.28200894594192505, 'baseline_loss': 0.6742257475852966, 'total_loss': 0.05510392785072327}\n",
      "2021-09-07 17:18:23.746 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11465691030025482\n",
      "2021-09-07 17:18:23.747 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9461302757263184\n",
      "2021-09-07 17:18:23.748 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11465691030025482\n",
      "2021-09-07 17:18:23.750 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:23.751 | INFO     | src.policies:train:123 - Epoch 306 / 800\n",
      "2021-09-07 17:18:23.752 | INFO     | src.policies:collect_trajectories:221 - Episode 1070\n",
      "2021-09-07 17:18:23.787 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:23.788 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:23.788 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:23.792 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:23.794 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2663658857345581, 'baseline_loss': 0.46792933344841003, 'total_loss': -0.03240121901035309}\n",
      "2021-09-07 17:18:23.795 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1195637583732605\n",
      "2021-09-07 17:18:23.797 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9078469276428223\n",
      "2021-09-07 17:18:23.798 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1195637583732605\n",
      "2021-09-07 17:18:23.799 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:23.801 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:23.802 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2517980635166168, 'baseline_loss': 0.4170590937137604, 'total_loss': -0.04326851665973663}\n",
      "2021-09-07 17:18:23.803 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.0988023653626442\n",
      "2021-09-07 17:18:23.805 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1911615133285522\n",
      "2021-09-07 17:18:23.807 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.0988023653626442\n",
      "2021-09-07 17:18:23.808 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:23.810 | INFO     | src.policies:train:123 - Epoch 307 / 800\n",
      "2021-09-07 17:18:23.811 | INFO     | src.policies:collect_trajectories:221 - Episode 1071\n",
      "2021-09-07 17:18:23.845 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:23.845 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:23.846 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:23.848 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:23.849 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6335655450820923, 'baseline_loss': 1.9378976821899414, 'total_loss': 0.3353832960128784}\n",
      "2021-09-07 17:18:23.850 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4933760166168213\n",
      "2021-09-07 17:18:23.852 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.972266435623169\n",
      "2021-09-07 17:18:23.853 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4933760166168213\n",
      "2021-09-07 17:18:23.854 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:23.856 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:23.857 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.604776918888092, 'baseline_loss': 1.745124340057373, 'total_loss': 0.2677852511405945}\n",
      "2021-09-07 17:18:23.858 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09366528689861298\n",
      "2021-09-07 17:18:23.860 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.5519630908966064\n",
      "2021-09-07 17:18:23.861 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09366528689861298\n",
      "2021-09-07 17:18:23.862 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:23.864 | INFO     | src.policies:train:123 - Epoch 308 / 800\n",
      "2021-09-07 17:18:23.865 | INFO     | src.policies:collect_trajectories:221 - Episode 1072\n",
      "2021-09-07 17:18:23.900 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:23.901 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:23.901 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:23.906 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:23.908 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3510552942752838, 'baseline_loss': 0.6287643909454346, 'total_loss': -0.03667309880256653}\n",
      "2021-09-07 17:18:23.909 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2507348358631134\n",
      "2021-09-07 17:18:23.910 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7369950413703918\n",
      "2021-09-07 17:18:23.912 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2507348358631134\n",
      "2021-09-07 17:18:23.914 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:23.915 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:23.917 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.44948315620422363, 'baseline_loss': 0.7912243008613586, 'total_loss': -0.05387100577354431}\n",
      "2021-09-07 17:18:23.918 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21624213457107544\n",
      "2021-09-07 17:18:23.919 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8148373961448669\n",
      "2021-09-07 17:18:23.921 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21624213457107544\n",
      "2021-09-07 17:18:23.923 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:23.925 | INFO     | src.policies:train:123 - Epoch 309 / 800\n",
      "2021-09-07 17:18:23.925 | INFO     | src.policies:collect_trajectories:221 - Episode 1073\n",
      "2021-09-07 17:18:23.962 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:23.963 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:23.964 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:23.969 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:23.971 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.19886352121829987, 'baseline_loss': 0.4208522140979767, 'total_loss': 0.011562585830688477}\n",
      "2021-09-07 17:18:23.973 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11640579998493195\n",
      "2021-09-07 17:18:23.975 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0652469396591187\n",
      "2021-09-07 17:18:23.976 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11640579998493195\n",
      "2021-09-07 17:18:23.978 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:23.979 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:23.981 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20907196402549744, 'baseline_loss': 0.4304247200489044, 'total_loss': 0.006140395998954773}\n",
      "2021-09-07 17:18:23.982 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.153722882270813\n",
      "2021-09-07 17:18:23.983 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3263856172561646\n",
      "2021-09-07 17:18:23.985 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.153722882270813\n",
      "2021-09-07 17:18:23.986 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:23.989 | INFO     | src.policies:train:123 - Epoch 310 / 800\n",
      "2021-09-07 17:18:23.990 | INFO     | src.policies:collect_trajectories:221 - Episode 1074\n",
      "2021-09-07 17:18:24.023 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:24.023 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:24.024 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:24.025 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:24.027 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7233911156654358, 'baseline_loss': 2.3884923458099365, 'total_loss': 0.47085505723953247}\n",
      "2021-09-07 17:18:24.028 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4977518320083618\n",
      "2021-09-07 17:18:24.029 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.5169384479522705\n",
      "2021-09-07 17:18:24.030 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4977518320083618\n",
      "2021-09-07 17:18:24.031 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:24.033 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:24.034 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6454650163650513, 'baseline_loss': 2.4929823875427246, 'total_loss': 0.601026177406311}\n",
      "2021-09-07 17:18:24.035 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32952818274497986\n",
      "2021-09-07 17:18:24.036 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.7959952354431152\n",
      "2021-09-07 17:18:24.037 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32952818274497986\n",
      "2021-09-07 17:18:24.038 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999999403953552\n",
      "2021-09-07 17:18:24.039 | INFO     | src.policies:train:123 - Epoch 311 / 800\n",
      "2021-09-07 17:18:24.040 | INFO     | src.policies:collect_trajectories:221 - Episode 1075\n",
      "2021-09-07 17:18:24.072 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:24.073 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:24.073 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:24.075 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:24.077 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40065425634384155, 'baseline_loss': 0.9613913893699646, 'total_loss': 0.08004143834114075}\n",
      "2021-09-07 17:18:24.078 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3944782614707947\n",
      "2021-09-07 17:18:24.079 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9103822112083435\n",
      "2021-09-07 17:18:24.080 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3944782614707947\n",
      "2021-09-07 17:18:24.081 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:24.082 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:24.083 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.46518024802207947, 'baseline_loss': 1.1500163078308105, 'total_loss': 0.1098279058933258}\n",
      "2021-09-07 17:18:24.084 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1287434995174408\n",
      "2021-09-07 17:18:24.085 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.166163444519043\n",
      "2021-09-07 17:18:24.086 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1287434995174408\n",
      "2021-09-07 17:18:24.087 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:24.089 | INFO     | src.policies:train:123 - Epoch 312 / 800\n",
      "2021-09-07 17:18:24.089 | INFO     | src.policies:collect_trajectories:221 - Episode 1076\n",
      "2021-09-07 17:18:24.121 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:24.121 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:24.122 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:24.124 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:24.126 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.06894668191671371, 'baseline_loss': 0.5754284858703613, 'total_loss': 0.21876755356788635}\n",
      "2021-09-07 17:18:24.127 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09487798064947128\n",
      "2021-09-07 17:18:24.128 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8069204092025757\n",
      "2021-09-07 17:18:24.129 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09487798064947128\n",
      "2021-09-07 17:18:24.130 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:24.131 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:24.132 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.07104114443063736, 'baseline_loss': 0.5801855325698853, 'total_loss': 0.21905162930488586}\n",
      "2021-09-07 17:18:24.133 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3180096745491028\n",
      "2021-09-07 17:18:24.134 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1524137258529663\n",
      "2021-09-07 17:18:24.183 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3180096745491028\n",
      "2021-09-07 17:18:24.184 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:24.186 | INFO     | src.policies:train:123 - Epoch 313 / 800\n",
      "2021-09-07 17:18:24.187 | INFO     | src.policies:collect_trajectories:221 - Episode 1077\n",
      "2021-09-07 17:18:24.215 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:24.216 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:24.216 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:24.218 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:24.220 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.48638302087783813, 'baseline_loss': 1.2824821472167969, 'total_loss': 0.1548580527305603}\n",
      "2021-09-07 17:18:24.221 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.244277223944664\n",
      "2021-09-07 17:18:24.222 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8642786741256714\n",
      "2021-09-07 17:18:24.223 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.244277223944664\n",
      "2021-09-07 17:18:24.225 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:24.226 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:24.227 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4527776837348938, 'baseline_loss': 1.3235576152801514, 'total_loss': 0.20900112390518188}\n",
      "2021-09-07 17:18:24.228 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4121783375740051\n",
      "2021-09-07 17:18:24.228 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.454279899597168\n",
      "2021-09-07 17:18:24.229 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4121783375740051\n",
      "2021-09-07 17:18:24.230 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:24.232 | INFO     | src.policies:train:123 - Epoch 314 / 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:24.232 | INFO     | src.policies:collect_trajectories:221 - Episode 1078\n",
      "2021-09-07 17:18:24.257 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:24.258 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 177.0\n",
      "2021-09-07 17:18:24.259 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 177.0\n",
      "2021-09-07 17:18:24.259 | INFO     | src.policies:collect_trajectories:221 - Episode 1079\n",
      "2021-09-07 17:18:24.281 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:24.282 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 137.0\n",
      "2021-09-07 17:18:24.282 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 157.0\n",
      "2021-09-07 17:18:24.283 | WARNING  | src.policies:train:144 - The actual batch size is 314, instead of 200\n",
      "2021-09-07 17:18:24.286 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:24.288 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31429457664489746, 'baseline_loss': 0.6330415606498718, 'total_loss': 0.002226203680038452}\n",
      "2021-09-07 17:18:24.288 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2430192232131958\n",
      "2021-09-07 17:18:24.289 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8642958998680115\n",
      "2021-09-07 17:18:24.290 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2430192232131958\n",
      "2021-09-07 17:18:24.291 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:24.292 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:24.294 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.28706926107406616, 'baseline_loss': 0.6260530948638916, 'total_loss': 0.02595728635787964}\n",
      "2021-09-07 17:18:24.294 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20602016150951385\n",
      "2021-09-07 17:18:24.295 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8431933522224426\n",
      "2021-09-07 17:18:24.296 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20602016150951385\n",
      "2021-09-07 17:18:24.297 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:24.299 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:24.301 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2901875972747803, 'baseline_loss': 0.5929223895072937, 'total_loss': 0.006273597478866577}\n",
      "2021-09-07 17:18:24.302 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24369065463542938\n",
      "2021-09-07 17:18:24.302 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8976309895515442\n",
      "2021-09-07 17:18:24.303 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24369065463542938\n",
      "2021-09-07 17:18:24.305 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:24.306 | INFO     | src.policies:train:123 - Epoch 315 / 800\n",
      "2021-09-07 17:18:24.307 | INFO     | src.policies:collect_trajectories:221 - Episode 1080\n",
      "2021-09-07 17:18:24.337 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:24.337 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:24.338 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:24.340 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:24.344 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29310107231140137, 'baseline_loss': 0.5299919247627258, 'total_loss': -0.028105109930038452}\n",
      "2021-09-07 17:18:24.345 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16309280693531036\n",
      "2021-09-07 17:18:24.346 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9506430625915527\n",
      "2021-09-07 17:18:24.347 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16309280693531036\n",
      "2021-09-07 17:18:24.349 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:24.350 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:24.351 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32209134101867676, 'baseline_loss': 0.5364450812339783, 'total_loss': -0.05386880040168762}\n",
      "2021-09-07 17:18:24.352 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.37178704142570496\n",
      "2021-09-07 17:18:24.353 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0100467205047607\n",
      "2021-09-07 17:18:24.354 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.37178704142570496\n",
      "2021-09-07 17:18:24.355 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:24.357 | INFO     | src.policies:train:123 - Epoch 316 / 800\n",
      "2021-09-07 17:18:24.357 | INFO     | src.policies:collect_trajectories:221 - Episode 1081\n",
      "2021-09-07 17:18:24.385 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:24.386 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:24.386 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:24.388 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:24.390 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20658011734485626, 'baseline_loss': 0.3756512701511383, 'total_loss': -0.01875448226928711}\n",
      "2021-09-07 17:18:24.391 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1643921285867691\n",
      "2021-09-07 17:18:24.504 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4774665832519531\n",
      "2021-09-07 17:18:24.505 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1643921285867691\n",
      "2021-09-07 17:18:24.507 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:24.508 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:24.510 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23261205852031708, 'baseline_loss': 0.39579036831855774, 'total_loss': -0.03471687436103821}\n",
      "2021-09-07 17:18:24.511 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11666018515825272\n",
      "2021-09-07 17:18:24.512 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.288848876953125\n",
      "2021-09-07 17:18:24.513 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11666018515825272\n",
      "2021-09-07 17:18:24.514 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:24.515 | INFO     | src.policies:train:123 - Epoch 317 / 800\n",
      "2021-09-07 17:18:24.516 | INFO     | src.policies:collect_trajectories:221 - Episode 1082\n",
      "2021-09-07 17:18:24.544 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:24.545 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 177.0\n",
      "2021-09-07 17:18:24.546 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 177.0\n",
      "2021-09-07 17:18:24.546 | INFO     | src.policies:collect_trajectories:221 - Episode 1083\n",
      "2021-09-07 17:18:24.579 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:24.580 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:24.580 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 188.5\n",
      "2021-09-07 17:18:24.581 | WARNING  | src.policies:train:144 - The actual batch size is 377, instead of 200\n",
      "2021-09-07 17:18:24.583 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:24.586 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.44219815731048584, 'baseline_loss': 1.0291526317596436, 'total_loss': 0.07237815856933594}\n",
      "2021-09-07 17:18:24.588 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2264396995306015\n",
      "2021-09-07 17:18:24.589 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5679299831390381\n",
      "2021-09-07 17:18:24.590 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2264396995306015\n",
      "2021-09-07 17:18:24.591 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:24.592 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:24.593 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32151326537132263, 'baseline_loss': 0.8093010783195496, 'total_loss': 0.08313727378845215}\n",
      "2021-09-07 17:18:24.594 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2190307080745697\n",
      "2021-09-07 17:18:24.595 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4758681058883667\n",
      "2021-09-07 17:18:24.596 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2190307080745697\n",
      "2021-09-07 17:18:24.597 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4758681058883667\n",
      "2021-09-07 17:18:24.598 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:24.599 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36385563015937805, 'baseline_loss': 0.8784273266792297, 'total_loss': 0.07535803318023682}\n",
      "2021-09-07 17:18:24.600 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19819381833076477\n",
      "2021-09-07 17:18:24.601 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4004977345466614\n",
      "2021-09-07 17:18:24.602 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19819381833076477\n",
      "2021-09-07 17:18:24.603 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4004977345466614\n",
      "2021-09-07 17:18:24.605 | INFO     | src.policies:train:123 - Epoch 318 / 800\n",
      "2021-09-07 17:18:24.606 | INFO     | src.policies:collect_trajectories:221 - Episode 1084\n",
      "2021-09-07 17:18:24.637 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:24.638 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:24.639 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:24.642 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:24.644 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.39342808723449707, 'baseline_loss': 0.7003939747810364, 'total_loss': -0.04323109984397888}\n",
      "2021-09-07 17:18:24.645 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2611534893512726\n",
      "2021-09-07 17:18:24.646 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2969729006290436\n",
      "2021-09-07 17:18:24.647 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2611534893512726\n",
      "2021-09-07 17:18:24.648 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2969729006290436\n",
      "2021-09-07 17:18:24.649 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:24.650 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3393299877643585, 'baseline_loss': 0.7056611776351929, 'total_loss': 0.013500601053237915}\n",
      "2021-09-07 17:18:24.651 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16583681106567383\n",
      "2021-09-07 17:18:24.652 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.44088461995124817\n",
      "2021-09-07 17:18:24.653 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16583681106567383\n",
      "2021-09-07 17:18:24.654 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.44088461995124817\n",
      "2021-09-07 17:18:24.655 | INFO     | src.policies:train:123 - Epoch 319 / 800\n",
      "2021-09-07 17:18:24.656 | INFO     | src.policies:collect_trajectories:221 - Episode 1085\n",
      "2021-09-07 17:18:24.729 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:24.730 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:24.730 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:24.732 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:24.734 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.21453090012073517, 'baseline_loss': 0.5901285409927368, 'total_loss': 0.08053337037563324}\n",
      "2021-09-07 17:18:24.735 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10118564963340759\n",
      "2021-09-07 17:18:24.736 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.515844464302063\n",
      "2021-09-07 17:18:24.738 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10118564963340759\n",
      "2021-09-07 17:18:24.739 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:24.740 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:24.742 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23252151906490326, 'baseline_loss': 0.6627654433250427, 'total_loss': 0.0988612025976181}\n",
      "2021-09-07 17:18:24.742 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25378620624542236\n",
      "2021-09-07 17:18:24.743 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.356423258781433\n",
      "2021-09-07 17:18:24.745 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25378620624542236\n",
      "2021-09-07 17:18:24.746 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:24.748 | INFO     | src.policies:train:123 - Epoch 320 / 800\n",
      "2021-09-07 17:18:24.748 | INFO     | src.policies:collect_trajectories:221 - Episode 1086\n",
      "2021-09-07 17:18:24.779 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:24.780 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 178.0\n",
      "2021-09-07 17:18:24.780 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 178.0\n",
      "2021-09-07 17:18:24.781 | INFO     | src.policies:collect_trajectories:221 - Episode 1087\n",
      "2021-09-07 17:18:24.816 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:24.817 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:24.817 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 189.0\n",
      "2021-09-07 17:18:24.818 | WARNING  | src.policies:train:144 - The actual batch size is 378, instead of 200\n",
      "2021-09-07 17:18:24.822 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:24.824 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4885545074939728, 'baseline_loss': 1.2864598035812378, 'total_loss': 0.15467539429664612}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:24.825 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5714660286903381\n",
      "2021-09-07 17:18:24.826 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6887465119361877\n",
      "2021-09-07 17:18:24.827 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:24.828 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:24.830 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:24.831 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5792365670204163, 'baseline_loss': 1.3950262069702148, 'total_loss': 0.11827653646469116}\n",
      "2021-09-07 17:18:24.832 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7248097062110901\n",
      "2021-09-07 17:18:24.833 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1426780223846436\n",
      "2021-09-07 17:18:24.834 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:24.835 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:24.836 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:24.838 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4796142876148224, 'baseline_loss': 1.3566293716430664, 'total_loss': 0.19870039820671082}\n",
      "2021-09-07 17:18:24.839 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2500993311405182\n",
      "2021-09-07 17:18:24.840 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.442979097366333\n",
      "2021-09-07 17:18:24.841 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2500993311405182\n",
      "2021-09-07 17:18:24.842 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:24.844 | INFO     | src.policies:train:123 - Epoch 321 / 800\n",
      "2021-09-07 17:18:24.845 | INFO     | src.policies:collect_trajectories:221 - Episode 1088\n",
      "2021-09-07 17:18:24.868 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:24.868 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 150.0\n",
      "2021-09-07 17:18:24.869 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 150.0\n",
      "2021-09-07 17:18:24.869 | INFO     | src.policies:collect_trajectories:221 - Episode 1089\n",
      "2021-09-07 17:18:24.900 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:24.901 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:24.901 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 175.0\n",
      "2021-09-07 17:18:24.902 | WARNING  | src.policies:train:144 - The actual batch size is 350, instead of 200\n",
      "2021-09-07 17:18:24.905 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:24.907 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4587477743625641, 'baseline_loss': 0.8868615031242371, 'total_loss': -0.015317022800445557}\n",
      "2021-09-07 17:18:24.909 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32474958896636963\n",
      "2021-09-07 17:18:24.910 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8442499041557312\n",
      "2021-09-07 17:18:24.911 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32474958896636963\n",
      "2021-09-07 17:18:24.912 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:24.913 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:24.914 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4302254021167755, 'baseline_loss': 0.9430669546127319, 'total_loss': 0.041308075189590454}\n",
      "2021-09-07 17:18:24.915 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21309144794940948\n",
      "2021-09-07 17:18:24.916 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1989108324050903\n",
      "2021-09-07 17:18:24.917 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21309144794940948\n",
      "2021-09-07 17:18:24.918 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:24.919 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:24.920 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.49911215901374817, 'baseline_loss': 0.826718270778656, 'total_loss': -0.08575302362442017}\n",
      "2021-09-07 17:18:24.921 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2290688455104828\n",
      "2021-09-07 17:18:24.922 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5894071459770203\n",
      "2021-09-07 17:18:24.923 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2290688455104828\n",
      "2021-09-07 17:18:24.924 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:24.926 | INFO     | src.policies:train:123 - Epoch 322 / 800\n",
      "2021-09-07 17:18:24.926 | INFO     | src.policies:collect_trajectories:221 - Episode 1090\n",
      "2021-09-07 17:18:24.950 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:24.950 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 143.0\n",
      "2021-09-07 17:18:24.951 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 143.0\n",
      "2021-09-07 17:18:24.952 | INFO     | src.policies:collect_trajectories:221 - Episode 1091\n",
      "2021-09-07 17:18:24.983 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:24.984 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:24.984 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 171.5\n",
      "2021-09-07 17:18:24.985 | WARNING  | src.policies:train:144 - The actual batch size is 343, instead of 200\n",
      "2021-09-07 17:18:24.988 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:24.990 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4824938476085663, 'baseline_loss': 1.5147480964660645, 'total_loss': 0.27488020062446594}\n",
      "2021-09-07 17:18:24.991 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.373468816280365\n",
      "2021-09-07 17:18:24.991 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5172176361083984\n",
      "2021-09-07 17:18:24.992 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.373468816280365\n",
      "2021-09-07 17:18:24.994 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:24.995 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:24.996 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.44699668884277344, 'baseline_loss': 1.7295713424682617, 'total_loss': 0.4177889823913574}\n",
      "2021-09-07 17:18:24.997 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.28814995288848877\n",
      "2021-09-07 17:18:24.997 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.1731491088867188\n",
      "2021-09-07 17:18:24.998 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.28814995288848877\n",
      "2021-09-07 17:18:24.999 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:25.001 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:25.002 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5137607455253601, 'baseline_loss': 1.5708383321762085, 'total_loss': 0.27165842056274414}\n",
      "2021-09-07 17:18:25.003 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2789570689201355\n",
      "2021-09-07 17:18:25.004 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9556273221969604\n",
      "2021-09-07 17:18:25.005 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2789570689201355\n",
      "2021-09-07 17:18:25.006 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:25.008 | INFO     | src.policies:train:123 - Epoch 323 / 800\n",
      "2021-09-07 17:18:25.009 | INFO     | src.policies:collect_trajectories:221 - Episode 1092\n",
      "2021-09-07 17:18:25.016 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:25.017 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 45.0\n",
      "2021-09-07 17:18:25.017 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 45.0\n",
      "2021-09-07 17:18:25.018 | INFO     | src.policies:collect_trajectories:221 - Episode 1093\n",
      "2021-09-07 17:18:25.051 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:25.052 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:25.052 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 122.5\n",
      "2021-09-07 17:18:25.053 | WARNING  | src.policies:train:144 - The actual batch size is 245, instead of 200\n",
      "2021-09-07 17:18:25.055 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:25.057 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5784713625907898, 'baseline_loss': 1.6709709167480469, 'total_loss': 0.25701409578323364}\n",
      "2021-09-07 17:18:25.058 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19598104059696198\n",
      "2021-09-07 17:18:25.059 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.8115227222442627\n",
      "2021-09-07 17:18:25.060 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19598104059696198\n",
      "2021-09-07 17:18:25.061 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:25.062 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:25.063 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6615772843360901, 'baseline_loss': 1.7762216329574585, 'total_loss': 0.22653353214263916}\n",
      "2021-09-07 17:18:25.064 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1887262463569641\n",
      "2021-09-07 17:18:25.065 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.442380666732788\n",
      "2021-09-07 17:18:25.066 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1887262463569641\n",
      "2021-09-07 17:18:25.067 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:25.069 | INFO     | src.policies:train:123 - Epoch 324 / 800\n",
      "2021-09-07 17:18:25.069 | INFO     | src.policies:collect_trajectories:221 - Episode 1094\n",
      "2021-09-07 17:18:25.098 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:25.099 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:25.099 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:25.100 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:25.103 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7384678721427917, 'baseline_loss': 2.3864004611968994, 'total_loss': 0.45473235845565796}\n",
      "2021-09-07 17:18:25.104 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1871671825647354\n",
      "2021-09-07 17:18:25.105 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.722867012023926\n",
      "2021-09-07 17:18:25.106 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1871671825647354\n",
      "2021-09-07 17:18:25.108 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:25.109 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:25.110 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7408036589622498, 'baseline_loss': 2.284144639968872, 'total_loss': 0.4012686610221863}\n",
      "2021-09-07 17:18:25.111 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7089974880218506\n",
      "2021-09-07 17:18:25.112 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.605818510055542\n",
      "2021-09-07 17:18:25.113 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:25.114 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:25.116 | INFO     | src.policies:train:123 - Epoch 325 / 800\n",
      "2021-09-07 17:18:25.116 | INFO     | src.policies:collect_trajectories:221 - Episode 1095\n",
      "2021-09-07 17:18:25.146 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:25.147 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:25.147 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:25.149 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:25.151 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2228155881166458, 'baseline_loss': 0.5092572569847107, 'total_loss': 0.031813040375709534}\n",
      "2021-09-07 17:18:25.152 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2639547288417816\n",
      "2021-09-07 17:18:25.153 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.741033911705017\n",
      "2021-09-07 17:18:25.154 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2639547288417816\n",
      "2021-09-07 17:18:25.156 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:25.157 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:25.158 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2181577831506729, 'baseline_loss': 0.4459354281425476, 'total_loss': 0.004809930920600891}\n",
      "2021-09-07 17:18:25.159 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2124481201171875\n",
      "2021-09-07 17:18:25.159 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6409200429916382\n",
      "2021-09-07 17:18:25.160 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2124481201171875\n",
      "2021-09-07 17:18:25.161 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:25.163 | INFO     | src.policies:train:123 - Epoch 326 / 800\n",
      "2021-09-07 17:18:25.163 | INFO     | src.policies:collect_trajectories:221 - Episode 1096\n",
      "2021-09-07 17:18:25.192 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:25.192 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:25.193 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:25.195 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:25.197 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.34992530941963196, 'baseline_loss': 0.5070223808288574, 'total_loss': -0.09641411900520325}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:25.198 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12148456275463104\n",
      "2021-09-07 17:18:25.199 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6569643020629883\n",
      "2021-09-07 17:18:25.201 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12148456275463104\n",
      "2021-09-07 17:18:25.203 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:25.204 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:25.205 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2938474714756012, 'baseline_loss': 0.44085174798965454, 'total_loss': -0.07342159748077393}\n",
      "2021-09-07 17:18:25.206 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1278126835823059\n",
      "2021-09-07 17:18:25.207 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1939016580581665\n",
      "2021-09-07 17:18:25.208 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1278126835823059\n",
      "2021-09-07 17:18:25.209 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:25.211 | INFO     | src.policies:train:123 - Epoch 327 / 800\n",
      "2021-09-07 17:18:25.211 | INFO     | src.policies:collect_trajectories:221 - Episode 1097\n",
      "2021-09-07 17:18:25.300 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:25.302 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:25.302 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:25.304 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:25.306 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6213257312774658, 'baseline_loss': 2.415149450302124, 'total_loss': 0.5862489938735962}\n",
      "2021-09-07 17:18:25.307 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20215298235416412\n",
      "2021-09-07 17:18:25.307 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.8009212017059326\n",
      "2021-09-07 17:18:25.309 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20215298235416412\n",
      "2021-09-07 17:18:25.309 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:25.311 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:25.312 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6660458445549011, 'baseline_loss': 2.4943816661834717, 'total_loss': 0.5811449885368347}\n",
      "2021-09-07 17:18:25.313 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32043829560279846\n",
      "2021-09-07 17:18:25.314 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.5483920574188232\n",
      "2021-09-07 17:18:25.315 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32043829560279846\n",
      "2021-09-07 17:18:25.316 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:25.317 | INFO     | src.policies:train:123 - Epoch 328 / 800\n",
      "2021-09-07 17:18:25.318 | INFO     | src.policies:collect_trajectories:221 - Episode 1098\n",
      "2021-09-07 17:18:25.348 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:25.349 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:25.349 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:25.351 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:25.353 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4742904603481293, 'baseline_loss': 1.353700876235962, 'total_loss': 0.20255997776985168}\n",
      "2021-09-07 17:18:25.354 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1745067983865738\n",
      "2021-09-07 17:18:25.355 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6384267807006836\n",
      "2021-09-07 17:18:25.356 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1745067983865738\n",
      "2021-09-07 17:18:25.357 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:25.358 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:25.360 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42960166931152344, 'baseline_loss': 1.17107355594635, 'total_loss': 0.1559351086616516}\n",
      "2021-09-07 17:18:25.361 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1946764886379242\n",
      "2021-09-07 17:18:25.362 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1945539712905884\n",
      "2021-09-07 17:18:25.363 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1946764886379242\n",
      "2021-09-07 17:18:25.364 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:25.366 | INFO     | src.policies:train:123 - Epoch 329 / 800\n",
      "2021-09-07 17:18:25.366 | INFO     | src.policies:collect_trajectories:221 - Episode 1099\n",
      "2021-09-07 17:18:25.403 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:25.404 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:25.404 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:25.406 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:25.408 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5564154386520386, 'baseline_loss': 1.1590046882629395, 'total_loss': 0.023086905479431152}\n",
      "2021-09-07 17:18:25.409 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4564380347728729\n",
      "2021-09-07 17:18:25.410 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.55182945728302\n",
      "2021-09-07 17:18:25.411 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4564380347728729\n",
      "2021-09-07 17:18:25.412 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:25.414 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:25.415 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.39691162109375, 'baseline_loss': 0.8943805694580078, 'total_loss': 0.050278663635253906}\n",
      "2021-09-07 17:18:25.416 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19365832209587097\n",
      "2021-09-07 17:18:25.417 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.329417109489441\n",
      "2021-09-07 17:18:25.418 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19365832209587097\n",
      "2021-09-07 17:18:25.419 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:25.421 | INFO     | src.policies:train:123 - Epoch 330 / 800\n",
      "2021-09-07 17:18:25.421 | INFO     | src.policies:collect_trajectories:221 - Episode 1100\n",
      "2021-09-07 17:18:25.452 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:25.453 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:25.453 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:25.455 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:25.457 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.13168972730636597, 'baseline_loss': 0.4728235900402069, 'total_loss': 0.10472206771373749}\n",
      "2021-09-07 17:18:25.458 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07956526428461075\n",
      "2021-09-07 17:18:25.459 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5186136960983276\n",
      "2021-09-07 17:18:25.460 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07956526428461075\n",
      "2021-09-07 17:18:25.461 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:25.463 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:25.464 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1535518616437912, 'baseline_loss': 0.4465338885784149, 'total_loss': 0.06971508264541626}\n",
      "2021-09-07 17:18:25.465 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.194022998213768\n",
      "2021-09-07 17:18:25.465 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6196261644363403\n",
      "2021-09-07 17:18:25.466 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.194022998213768\n",
      "2021-09-07 17:18:25.468 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:25.469 | INFO     | src.policies:train:123 - Epoch 331 / 800\n",
      "2021-09-07 17:18:25.469 | INFO     | src.policies:collect_trajectories:221 - Episode 1101\n",
      "2021-09-07 17:18:25.499 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:25.499 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:25.500 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:25.502 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:25.504 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1845085173845291, 'baseline_loss': 0.6382524371147156, 'total_loss': 0.13461770117282867}\n",
      "2021-09-07 17:18:25.505 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20682266354560852\n",
      "2021-09-07 17:18:25.505 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9363462924957275\n",
      "2021-09-07 17:18:25.507 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20682266354560852\n",
      "2021-09-07 17:18:25.508 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:25.509 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:25.510 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1876145899295807, 'baseline_loss': 0.6265637874603271, 'total_loss': 0.12566730380058289}\n",
      "2021-09-07 17:18:25.511 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3181074857711792\n",
      "2021-09-07 17:18:25.512 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7658478021621704\n",
      "2021-09-07 17:18:25.513 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3181074857711792\n",
      "2021-09-07 17:18:25.514 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:25.516 | INFO     | src.policies:train:123 - Epoch 332 / 800\n",
      "2021-09-07 17:18:25.516 | INFO     | src.policies:collect_trajectories:221 - Episode 1102\n",
      "2021-09-07 17:18:25.546 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:25.547 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:25.547 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:25.549 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:25.552 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2796776294708252, 'baseline_loss': 0.42891964316368103, 'total_loss': -0.06521780788898468}\n",
      "2021-09-07 17:18:25.553 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.35887497663497925\n",
      "2021-09-07 17:18:25.555 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0365478992462158\n",
      "2021-09-07 17:18:25.556 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.35887497663497925\n",
      "2021-09-07 17:18:25.557 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:25.559 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:25.560 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29848289489746094, 'baseline_loss': 0.49480733275413513, 'total_loss': -0.05107922852039337}\n",
      "2021-09-07 17:18:25.561 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.40803205966949463\n",
      "2021-09-07 17:18:25.561 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7944287657737732\n",
      "2021-09-07 17:18:25.562 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.40803205966949463\n",
      "2021-09-07 17:18:25.563 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:25.565 | INFO     | src.policies:train:123 - Epoch 333 / 800\n",
      "2021-09-07 17:18:25.565 | INFO     | src.policies:collect_trajectories:221 - Episode 1103\n",
      "2021-09-07 17:18:25.594 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:25.595 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:25.595 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:25.597 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:25.601 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6342708468437195, 'baseline_loss': 2.105288028717041, 'total_loss': 0.418373167514801}\n",
      "2021-09-07 17:18:25.602 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25318896770477295\n",
      "2021-09-07 17:18:25.603 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.005751132965088\n",
      "2021-09-07 17:18:25.604 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25318896770477295\n",
      "2021-09-07 17:18:25.605 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:25.607 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:25.608 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6365853548049927, 'baseline_loss': 2.112398147583008, 'total_loss': 0.41961371898651123}\n",
      "2021-09-07 17:18:25.609 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1927177906036377\n",
      "2021-09-07 17:18:25.610 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.082563877105713\n",
      "2021-09-07 17:18:25.612 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1927177906036377\n",
      "2021-09-07 17:18:25.613 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:25.614 | INFO     | src.policies:train:123 - Epoch 334 / 800\n",
      "2021-09-07 17:18:25.615 | INFO     | src.policies:collect_trajectories:221 - Episode 1104\n",
      "2021-09-07 17:18:25.644 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:25.644 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:25.645 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:25.646 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:25.649 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.13594159483909607, 'baseline_loss': 0.45421794056892395, 'total_loss': 0.0911673754453659}\n",
      "2021-09-07 17:18:25.650 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14391183853149414\n",
      "2021-09-07 17:18:25.651 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.518497347831726\n",
      "2021-09-07 17:18:25.652 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14391183853149414\n",
      "2021-09-07 17:18:25.653 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:25.655 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:25.656 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.19401811063289642, 'baseline_loss': 0.4120081663131714, 'total_loss': 0.01198597252368927}\n",
      "2021-09-07 17:18:25.656 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.26173195242881775\n",
      "2021-09-07 17:18:25.657 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6749078035354614\n",
      "2021-09-07 17:18:25.658 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.26173195242881775\n",
      "2021-09-07 17:18:25.659 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:25.661 | INFO     | src.policies:train:123 - Epoch 335 / 800\n",
      "2021-09-07 17:18:25.661 | INFO     | src.policies:collect_trajectories:221 - Episode 1105\n",
      "2021-09-07 17:18:25.690 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:25.691 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:25.691 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:25.694 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:25.696 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.38198551535606384, 'baseline_loss': 0.988044261932373, 'total_loss': 0.11203661561012268}\n",
      "2021-09-07 17:18:25.697 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.34876617789268494\n",
      "2021-09-07 17:18:25.698 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1621495485305786\n",
      "2021-09-07 17:18:25.699 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.34876617789268494\n",
      "2021-09-07 17:18:25.700 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:25.702 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:25.703 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3675730228424072, 'baseline_loss': 0.9729107022285461, 'total_loss': 0.11888232827186584}\n",
      "2021-09-07 17:18:25.704 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5513741970062256\n",
      "2021-09-07 17:18:25.705 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9487849473953247\n",
      "2021-09-07 17:18:25.706 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:25.707 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:25.708 | INFO     | src.policies:train:123 - Epoch 336 / 800\n",
      "2021-09-07 17:18:25.709 | INFO     | src.policies:collect_trajectories:221 - Episode 1106\n",
      "2021-09-07 17:18:25.738 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:25.738 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:25.739 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:25.741 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:25.744 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5206907391548157, 'baseline_loss': 1.0270522832870483, 'total_loss': -0.007164597511291504}\n",
      "2021-09-07 17:18:25.745 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21630419790744781\n",
      "2021-09-07 17:18:25.746 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7472565174102783\n",
      "2021-09-07 17:18:25.747 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21630419790744781\n",
      "2021-09-07 17:18:25.748 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:25.749 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:25.750 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.48344624042510986, 'baseline_loss': 0.8649017214775085, 'total_loss': -0.05099537968635559}\n",
      "2021-09-07 17:18:25.751 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24532099068164825\n",
      "2021-09-07 17:18:25.752 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.29526886343955994\n",
      "2021-09-07 17:18:25.753 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24532099068164825\n",
      "2021-09-07 17:18:25.754 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.29526886343955994\n",
      "2021-09-07 17:18:25.756 | INFO     | src.policies:train:123 - Epoch 337 / 800\n",
      "2021-09-07 17:18:25.756 | INFO     | src.policies:collect_trajectories:221 - Episode 1107\n",
      "2021-09-07 17:18:25.785 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:25.786 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:25.786 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:25.799 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:25.846 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5847374200820923, 'baseline_loss': 1.517815113067627, 'total_loss': 0.1741701364517212}\n",
      "2021-09-07 17:18:25.847 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1650252789258957\n",
      "2021-09-07 17:18:25.848 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.7872142791748047\n",
      "2021-09-07 17:18:25.849 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1650252789258957\n",
      "2021-09-07 17:18:25.851 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:25.852 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:25.854 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4840056300163269, 'baseline_loss': 1.3318215608596802, 'total_loss': 0.18190515041351318}\n",
      "2021-09-07 17:18:25.855 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2530791163444519\n",
      "2021-09-07 17:18:25.856 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.4193167686462402\n",
      "2021-09-07 17:18:25.857 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2530791163444519\n",
      "2021-09-07 17:18:25.858 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:25.859 | INFO     | src.policies:train:123 - Epoch 338 / 800\n",
      "2021-09-07 17:18:25.860 | INFO     | src.policies:collect_trajectories:221 - Episode 1108\n",
      "2021-09-07 17:18:25.888 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:25.888 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:25.889 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:25.890 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:25.893 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6721581816673279, 'baseline_loss': 2.8475711345672607, 'total_loss': 0.7516273856163025}\n",
      "2021-09-07 17:18:25.894 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.8537241220474243\n",
      "2021-09-07 17:18:25.895 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.9244863986968994\n",
      "2021-09-07 17:18:25.896 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:25.897 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:25.899 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:25.900 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.710704505443573, 'baseline_loss': 2.9177911281585693, 'total_loss': 0.7481910586357117}\n",
      "2021-09-07 17:18:25.901 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5649276375770569\n",
      "2021-09-07 17:18:25.901 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.150746822357178\n",
      "2021-09-07 17:18:25.902 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:25.903 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:25.905 | INFO     | src.policies:train:123 - Epoch 339 / 800\n",
      "2021-09-07 17:18:25.905 | INFO     | src.policies:collect_trajectories:221 - Episode 1109\n",
      "2021-09-07 17:18:25.934 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:25.935 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:25.935 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:25.937 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:25.941 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5243536233901978, 'baseline_loss': 1.6025623083114624, 'total_loss': 0.27692753076553345}\n",
      "2021-09-07 17:18:25.942 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1719033420085907\n",
      "2021-09-07 17:18:25.943 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.9465439319610596\n",
      "2021-09-07 17:18:25.945 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1719033420085907\n",
      "2021-09-07 17:18:25.946 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:25.947 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:25.948 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6034046411514282, 'baseline_loss': 1.757008671760559, 'total_loss': 0.2750996947288513}\n",
      "2021-09-07 17:18:25.949 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.26149991154670715\n",
      "2021-09-07 17:18:25.950 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.908034324645996\n",
      "2021-09-07 17:18:25.951 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.26149991154670715\n",
      "2021-09-07 17:18:25.952 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:25.954 | INFO     | src.policies:train:123 - Epoch 340 / 800\n",
      "2021-09-07 17:18:25.954 | INFO     | src.policies:collect_trajectories:221 - Episode 1110\n",
      "2021-09-07 17:18:25.983 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:25.984 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:25.984 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:25.987 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:25.989 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4892863929271698, 'baseline_loss': 1.4970823526382446, 'total_loss': 0.2592547833919525}\n",
      "2021-09-07 17:18:25.990 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18289075791835785\n",
      "2021-09-07 17:18:25.991 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.6955671310424805\n",
      "2021-09-07 17:18:25.992 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18289075791835785\n",
      "2021-09-07 17:18:25.994 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:25.995 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:25.996 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6159024834632874, 'baseline_loss': 1.4511713981628418, 'total_loss': 0.10968321561813354}\n",
      "2021-09-07 17:18:25.997 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2997385263442993\n",
      "2021-09-07 17:18:25.998 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8797718286514282\n",
      "2021-09-07 17:18:26.000 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2997385263442993\n",
      "2021-09-07 17:18:26.001 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:26.003 | INFO     | src.policies:train:123 - Epoch 341 / 800\n",
      "2021-09-07 17:18:26.004 | INFO     | src.policies:collect_trajectories:221 - Episode 1111\n",
      "2021-09-07 17:18:26.034 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:26.034 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:26.035 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:26.036 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:26.039 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.14148658514022827, 'baseline_loss': 0.45329806208610535, 'total_loss': 0.0851624459028244}\n",
      "2021-09-07 17:18:26.040 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1344284564256668\n",
      "2021-09-07 17:18:26.041 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4376206398010254\n",
      "2021-09-07 17:18:26.042 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1344284564256668\n",
      "2021-09-07 17:18:26.043 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:26.044 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:26.046 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1489148586988449, 'baseline_loss': 0.48401889204978943, 'total_loss': 0.0930945873260498}\n",
      "2021-09-07 17:18:26.046 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16852068901062012\n",
      "2021-09-07 17:18:26.047 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.678479790687561\n",
      "2021-09-07 17:18:26.048 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16852068901062012\n",
      "2021-09-07 17:18:26.049 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:26.050 | INFO     | src.policies:train:123 - Epoch 342 / 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:26.051 | INFO     | src.policies:collect_trajectories:221 - Episode 1112\n",
      "2021-09-07 17:18:26.080 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:26.081 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:26.081 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:26.083 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:26.085 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.30326366424560547, 'baseline_loss': 0.49540820717811584, 'total_loss': -0.055559560656547546}\n",
      "2021-09-07 17:18:26.086 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3937210440635681\n",
      "2021-09-07 17:18:26.087 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.304807424545288\n",
      "2021-09-07 17:18:26.089 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3937210440635681\n",
      "2021-09-07 17:18:26.090 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:26.091 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:26.092 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31739291548728943, 'baseline_loss': 0.5890910625457764, 'total_loss': -0.022847384214401245}\n",
      "2021-09-07 17:18:26.093 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19193637371063232\n",
      "2021-09-07 17:18:26.093 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8907710313796997\n",
      "2021-09-07 17:18:26.094 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19193637371063232\n",
      "2021-09-07 17:18:26.095 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:26.097 | INFO     | src.policies:train:123 - Epoch 343 / 800\n",
      "2021-09-07 17:18:26.097 | INFO     | src.policies:collect_trajectories:221 - Episode 1113\n",
      "2021-09-07 17:18:26.128 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:26.129 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:26.129 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:26.131 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:26.133 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1192525327205658, 'baseline_loss': 0.49194735288619995, 'total_loss': 0.12672114372253418}\n",
      "2021-09-07 17:18:26.134 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2916432321071625\n",
      "2021-09-07 17:18:26.135 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.1513617038726807\n",
      "2021-09-07 17:18:26.136 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2916432321071625\n",
      "2021-09-07 17:18:26.137 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:26.139 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:26.140 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1316639631986618, 'baseline_loss': 0.4942454993724823, 'total_loss': 0.11545878648757935}\n",
      "2021-09-07 17:18:26.141 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15840640664100647\n",
      "2021-09-07 17:18:26.142 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.958566427230835\n",
      "2021-09-07 17:18:26.143 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15840640664100647\n",
      "2021-09-07 17:18:26.144 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:26.145 | INFO     | src.policies:train:123 - Epoch 344 / 800\n",
      "2021-09-07 17:18:26.146 | INFO     | src.policies:collect_trajectories:221 - Episode 1114\n",
      "2021-09-07 17:18:26.174 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:26.175 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:26.176 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:26.177 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:26.179 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27229586243629456, 'baseline_loss': 0.8763440847396851, 'total_loss': 0.16587617993354797}\n",
      "2021-09-07 17:18:26.180 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16474661231040955\n",
      "2021-09-07 17:18:26.181 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.740847110748291\n",
      "2021-09-07 17:18:26.182 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16474661231040955\n",
      "2021-09-07 17:18:26.183 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:26.185 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:26.186 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.34165385365486145, 'baseline_loss': 0.8702403903007507, 'total_loss': 0.09346634149551392}\n",
      "2021-09-07 17:18:26.187 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20553985238075256\n",
      "2021-09-07 17:18:26.188 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9297595024108887\n",
      "2021-09-07 17:18:26.189 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20553985238075256\n",
      "2021-09-07 17:18:26.190 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:26.191 | INFO     | src.policies:train:123 - Epoch 345 / 800\n",
      "2021-09-07 17:18:26.192 | INFO     | src.policies:collect_trajectories:221 - Episode 1115\n",
      "2021-09-07 17:18:26.220 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:26.221 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:26.222 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:26.223 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:26.225 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5621022582054138, 'baseline_loss': 1.9509221315383911, 'total_loss': 0.41335880756378174}\n",
      "2021-09-07 17:18:26.226 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16211450099945068\n",
      "2021-09-07 17:18:26.227 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.983107805252075\n",
      "2021-09-07 17:18:26.228 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16211450099945068\n",
      "2021-09-07 17:18:26.229 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:26.231 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:26.232 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5697769522666931, 'baseline_loss': 1.9919394254684448, 'total_loss': 0.4261927604675293}\n",
      "2021-09-07 17:18:26.233 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2387935072183609\n",
      "2021-09-07 17:18:26.233 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.2777624130249023\n",
      "2021-09-07 17:18:26.234 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2387935072183609\n",
      "2021-09-07 17:18:26.236 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:26.237 | INFO     | src.policies:train:123 - Epoch 346 / 800\n",
      "2021-09-07 17:18:26.237 | INFO     | src.policies:collect_trajectories:221 - Episode 1116\n",
      "2021-09-07 17:18:26.266 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:26.266 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:26.267 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:26.269 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:26.272 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.38264986872673035, 'baseline_loss': 1.3723766803741455, 'total_loss': 0.3035384714603424}\n",
      "2021-09-07 17:18:26.273 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.47700411081314087\n",
      "2021-09-07 17:18:26.274 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5305384397506714\n",
      "2021-09-07 17:18:26.275 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.47700411081314087\n",
      "2021-09-07 17:18:26.276 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:26.277 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:26.278 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5126497149467468, 'baseline_loss': 1.473762035369873, 'total_loss': 0.2242313027381897}\n",
      "2021-09-07 17:18:26.279 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.330151230096817\n",
      "2021-09-07 17:18:26.280 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2324893474578857\n",
      "2021-09-07 17:18:26.281 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.330151230096817\n",
      "2021-09-07 17:18:26.282 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:26.283 | INFO     | src.policies:train:123 - Epoch 347 / 800\n",
      "2021-09-07 17:18:26.284 | INFO     | src.policies:collect_trajectories:221 - Episode 1117\n",
      "2021-09-07 17:18:26.314 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:26.315 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:26.316 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:26.318 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:26.321 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.11037706583738327, 'baseline_loss': 0.6492377519607544, 'total_loss': 0.21424180269241333}\n",
      "2021-09-07 17:18:26.322 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.542506992816925\n",
      "2021-09-07 17:18:26.322 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4639270305633545\n",
      "2021-09-07 17:18:26.323 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:26.324 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:26.326 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:26.327 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': 0.004579770844429731, 'baseline_loss': 0.6412294507026672, 'total_loss': 0.32519450783729553}\n",
      "2021-09-07 17:18:26.328 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3490578234195709\n",
      "2021-09-07 17:18:26.329 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5640820264816284\n",
      "2021-09-07 17:18:26.330 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3490578234195709\n",
      "2021-09-07 17:18:26.331 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:26.332 | INFO     | src.policies:train:123 - Epoch 348 / 800\n",
      "2021-09-07 17:18:26.333 | INFO     | src.policies:collect_trajectories:221 - Episode 1118\n",
      "2021-09-07 17:18:26.624 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:26.624 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:26.625 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:26.626 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:26.630 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4916219413280487, 'baseline_loss': 1.2927895784378052, 'total_loss': 0.15477284789085388}\n",
      "2021-09-07 17:18:26.631 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16556453704833984\n",
      "2021-09-07 17:18:26.632 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.011962652206421\n",
      "2021-09-07 17:18:26.633 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16556453704833984\n",
      "2021-09-07 17:18:26.635 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:26.636 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:26.637 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43809032440185547, 'baseline_loss': 1.2483724355697632, 'total_loss': 0.18609589338302612}\n",
      "2021-09-07 17:18:26.638 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.37904641032218933\n",
      "2021-09-07 17:18:26.639 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.065675973892212\n",
      "2021-09-07 17:18:26.640 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.37904641032218933\n",
      "2021-09-07 17:18:26.641 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:26.642 | INFO     | src.policies:train:123 - Epoch 349 / 800\n",
      "2021-09-07 17:18:26.643 | INFO     | src.policies:collect_trajectories:221 - Episode 1119\n",
      "2021-09-07 17:18:26.671 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:26.672 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:26.672 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:26.674 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:26.677 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41373908519744873, 'baseline_loss': 1.054951548576355, 'total_loss': 0.11373668909072876}\n",
      "2021-09-07 17:18:26.678 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3900679051876068\n",
      "2021-09-07 17:18:26.679 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9540849924087524\n",
      "2021-09-07 17:18:26.680 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3900679051876068\n",
      "2021-09-07 17:18:26.681 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:26.682 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:26.683 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5063839554786682, 'baseline_loss': 1.0057463645935059, 'total_loss': -0.003510773181915283}\n",
      "2021-09-07 17:18:26.684 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3547063171863556\n",
      "2021-09-07 17:18:26.685 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3813906908035278\n",
      "2021-09-07 17:18:26.686 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3547063171863556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:26.687 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:26.688 | INFO     | src.policies:train:123 - Epoch 350 / 800\n",
      "2021-09-07 17:18:26.688 | INFO     | src.policies:collect_trajectories:221 - Episode 1120\n",
      "2021-09-07 17:18:26.716 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:26.717 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:26.717 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:26.719 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:26.722 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6201171278953552, 'baseline_loss': 1.2991151809692383, 'total_loss': 0.029440462589263916}\n",
      "2021-09-07 17:18:26.723 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.34069305658340454\n",
      "2021-09-07 17:18:26.724 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2674345970153809\n",
      "2021-09-07 17:18:26.725 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.34069305658340454\n",
      "2021-09-07 17:18:26.726 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:26.728 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:26.729 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4926421344280243, 'baseline_loss': 1.2335437536239624, 'total_loss': 0.12412974238395691}\n",
      "2021-09-07 17:18:26.730 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19194358587265015\n",
      "2021-09-07 17:18:26.731 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.122590184211731\n",
      "2021-09-07 17:18:26.732 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19194358587265015\n",
      "2021-09-07 17:18:26.733 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:26.734 | INFO     | src.policies:train:123 - Epoch 351 / 800\n",
      "2021-09-07 17:18:26.735 | INFO     | src.policies:collect_trajectories:221 - Episode 1121\n",
      "2021-09-07 17:18:26.763 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:26.764 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:26.764 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:26.767 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:26.769 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3594537377357483, 'baseline_loss': 0.9466318488121033, 'total_loss': 0.11386218667030334}\n",
      "2021-09-07 17:18:26.769 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21344096958637238\n",
      "2021-09-07 17:18:26.771 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.02960205078125\n",
      "2021-09-07 17:18:26.772 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21344096958637238\n",
      "2021-09-07 17:18:26.773 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:26.774 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:26.775 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2733853757381439, 'baseline_loss': 0.8423712849617004, 'total_loss': 0.1478002667427063}\n",
      "2021-09-07 17:18:26.776 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19022808969020844\n",
      "2021-09-07 17:18:26.777 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.558994174003601\n",
      "2021-09-07 17:18:26.778 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19022808969020844\n",
      "2021-09-07 17:18:26.779 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:26.781 | INFO     | src.policies:train:123 - Epoch 352 / 800\n",
      "2021-09-07 17:18:26.781 | INFO     | src.policies:collect_trajectories:221 - Episode 1122\n",
      "2021-09-07 17:18:26.811 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:26.811 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:26.812 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:26.813 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:26.816 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24951303005218506, 'baseline_loss': 0.4733034074306488, 'total_loss': -0.012861326336860657}\n",
      "2021-09-07 17:18:26.817 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10393629968166351\n",
      "2021-09-07 17:18:26.818 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5588291883468628\n",
      "2021-09-07 17:18:26.819 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10393629968166351\n",
      "2021-09-07 17:18:26.820 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:26.822 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:26.823 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.22624929249286652, 'baseline_loss': 0.4456731081008911, 'total_loss': -0.0034127384424209595}\n",
      "2021-09-07 17:18:26.824 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1683662384748459\n",
      "2021-09-07 17:18:26.825 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8351380825042725\n",
      "2021-09-07 17:18:26.827 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1683662384748459\n",
      "2021-09-07 17:18:26.828 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:26.829 | INFO     | src.policies:train:123 - Epoch 353 / 800\n",
      "2021-09-07 17:18:26.830 | INFO     | src.policies:collect_trajectories:221 - Episode 1123\n",
      "2021-09-07 17:18:26.856 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:26.857 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 172.0\n",
      "2021-09-07 17:18:26.858 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 172.0\n",
      "2021-09-07 17:18:26.858 | INFO     | src.policies:collect_trajectories:221 - Episode 1124\n",
      "2021-09-07 17:18:26.882 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:26.883 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 128.0\n",
      "2021-09-07 17:18:26.883 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 150.0\n",
      "2021-09-07 17:18:26.884 | WARNING  | src.policies:train:144 - The actual batch size is 300, instead of 200\n",
      "2021-09-07 17:18:26.887 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:26.889 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1620059609413147, 'baseline_loss': 0.46761393547058105, 'total_loss': 0.07180100679397583}\n",
      "2021-09-07 17:18:26.890 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.34551966190338135\n",
      "2021-09-07 17:18:26.891 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3017655611038208\n",
      "2021-09-07 17:18:26.892 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.34551966190338135\n",
      "2021-09-07 17:18:26.893 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:26.894 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:26.895 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.12072617560625076, 'baseline_loss': 0.5078584551811218, 'total_loss': 0.13320305943489075}\n",
      "2021-09-07 17:18:26.896 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2404179871082306\n",
      "2021-09-07 17:18:26.897 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4141533374786377\n",
      "2021-09-07 17:18:26.898 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2404179871082306\n",
      "2021-09-07 17:18:26.899 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:26.900 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:26.901 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.09656009823083878, 'baseline_loss': 0.5358593463897705, 'total_loss': 0.17136958241462708}\n",
      "2021-09-07 17:18:26.902 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16631993651390076\n",
      "2021-09-07 17:18:26.904 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9360864162445068\n",
      "2021-09-07 17:18:26.905 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16631993651390076\n",
      "2021-09-07 17:18:26.907 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:26.909 | INFO     | src.policies:train:123 - Epoch 354 / 800\n",
      "2021-09-07 17:18:26.909 | INFO     | src.policies:collect_trajectories:221 - Episode 1125\n",
      "2021-09-07 17:18:26.941 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:26.942 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 173.0\n",
      "2021-09-07 17:18:26.942 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 173.0\n",
      "2021-09-07 17:18:26.943 | INFO     | src.policies:collect_trajectories:221 - Episode 1126\n",
      "2021-09-07 17:18:26.973 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:26.974 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:26.974 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 186.5\n",
      "2021-09-07 17:18:26.975 | WARNING  | src.policies:train:144 - The actual batch size is 373, instead of 200\n",
      "2021-09-07 17:18:26.978 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:26.980 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.21146160364151, 'baseline_loss': 0.8366397023200989, 'total_loss': 0.20685824751853943}\n",
      "2021-09-07 17:18:26.981 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.29565855860710144\n",
      "2021-09-07 17:18:26.982 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.036694884300232\n",
      "2021-09-07 17:18:26.983 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.29565855860710144\n",
      "2021-09-07 17:18:26.984 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:26.985 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:26.986 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.22626367211341858, 'baseline_loss': 0.6852651238441467, 'total_loss': 0.11636888980865479}\n",
      "2021-09-07 17:18:26.987 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31699487566947937\n",
      "2021-09-07 17:18:26.988 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1580188274383545\n",
      "2021-09-07 17:18:26.989 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31699487566947937\n",
      "2021-09-07 17:18:26.991 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:26.992 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:26.993 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.11586029082536697, 'baseline_loss': 0.6739300489425659, 'total_loss': 0.22110474109649658}\n",
      "2021-09-07 17:18:26.994 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1685253232717514\n",
      "2021-09-07 17:18:26.995 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6176055669784546\n",
      "2021-09-07 17:18:26.996 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1685253232717514\n",
      "2021-09-07 17:18:26.997 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:26.998 | INFO     | src.policies:train:123 - Epoch 355 / 800\n",
      "2021-09-07 17:18:26.998 | INFO     | src.policies:collect_trajectories:221 - Episode 1127\n",
      "2021-09-07 17:18:27.072 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:27.073 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:27.074 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:27.076 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:27.078 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2080633044242859, 'baseline_loss': 0.39859911799430847, 'total_loss': -0.008763745427131653}\n",
      "2021-09-07 17:18:27.079 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.44147592782974243\n",
      "2021-09-07 17:18:27.080 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1959439516067505\n",
      "2021-09-07 17:18:27.081 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.44147592782974243\n",
      "2021-09-07 17:18:27.082 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:27.083 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:27.084 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29128050804138184, 'baseline_loss': 0.43699783086776733, 'total_loss': -0.07278159260749817}\n",
      "2021-09-07 17:18:27.085 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08413869887590408\n",
      "2021-09-07 17:18:27.086 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9230296015739441\n",
      "2021-09-07 17:18:27.087 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08413869887590408\n",
      "2021-09-07 17:18:27.088 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:27.089 | INFO     | src.policies:train:123 - Epoch 356 / 800\n",
      "2021-09-07 17:18:27.090 | INFO     | src.policies:collect_trajectories:221 - Episode 1128\n",
      "2021-09-07 17:18:27.111 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:27.111 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 138.0\n",
      "2021-09-07 17:18:27.111 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 138.0\n",
      "2021-09-07 17:18:27.112 | INFO     | src.policies:collect_trajectories:221 - Episode 1129\n",
      "2021-09-07 17:18:27.141 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:27.141 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:27.142 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 169.0\n",
      "2021-09-07 17:18:27.142 | WARNING  | src.policies:train:144 - The actual batch size is 338, instead of 200\n",
      "2021-09-07 17:18:27.145 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:27.147 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36891815066337585, 'baseline_loss': 0.6763364672660828, 'total_loss': -0.030749917030334473}\n",
      "2021-09-07 17:18:27.148 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2511575222015381\n",
      "2021-09-07 17:18:27.149 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6517954468727112\n",
      "2021-09-07 17:18:27.150 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2511575222015381\n",
      "2021-09-07 17:18:27.151 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:27.152 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:27.153 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3629641830921173, 'baseline_loss': 0.6805068850517273, 'total_loss': -0.022710740566253662}\n",
      "2021-09-07 17:18:27.154 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5329572558403015\n",
      "2021-09-07 17:18:27.155 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7629435062408447\n",
      "2021-09-07 17:18:27.156 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:27.157 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:27.158 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:27.160 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.38001060485839844, 'baseline_loss': 0.801642894744873, 'total_loss': 0.020810842514038086}\n",
      "2021-09-07 17:18:27.160 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15350311994552612\n",
      "2021-09-07 17:18:27.161 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.000685453414917\n",
      "2021-09-07 17:18:27.162 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15350311994552612\n",
      "2021-09-07 17:18:27.164 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:27.165 | INFO     | src.policies:train:123 - Epoch 357 / 800\n",
      "2021-09-07 17:18:27.165 | INFO     | src.policies:collect_trajectories:221 - Episode 1130\n",
      "2021-09-07 17:18:27.202 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:27.203 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:27.203 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:27.205 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:27.207 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5991482138633728, 'baseline_loss': 1.712916612625122, 'total_loss': 0.25731009244918823}\n",
      "2021-09-07 17:18:27.208 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16509303450584412\n",
      "2021-09-07 17:18:27.209 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.936607837677002\n",
      "2021-09-07 17:18:27.210 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16509303450584412\n",
      "2021-09-07 17:18:27.211 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:27.212 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:27.214 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6464781761169434, 'baseline_loss': 1.7668706178665161, 'total_loss': 0.2369571328163147}\n",
      "2021-09-07 17:18:27.215 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4271727502346039\n",
      "2021-09-07 17:18:27.215 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.8969058990478516\n",
      "2021-09-07 17:18:27.216 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4271727502346039\n",
      "2021-09-07 17:18:27.218 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:27.219 | INFO     | src.policies:train:123 - Epoch 358 / 800\n",
      "2021-09-07 17:18:27.220 | INFO     | src.policies:collect_trajectories:221 - Episode 1131\n",
      "2021-09-07 17:18:27.248 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:27.248 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:27.248 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:27.251 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:27.253 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.13480818271636963, 'baseline_loss': 0.509504497051239, 'total_loss': 0.11994406580924988}\n",
      "2021-09-07 17:18:27.253 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21056799590587616\n",
      "2021-09-07 17:18:27.255 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5570789575576782\n",
      "2021-09-07 17:18:27.256 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21056799590587616\n",
      "2021-09-07 17:18:27.257 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:27.258 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:27.259 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2034263014793396, 'baseline_loss': 0.47922366857528687, 'total_loss': 0.03618553280830383}\n",
      "2021-09-07 17:18:27.260 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.37105032801628113\n",
      "2021-09-07 17:18:27.260 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.617449164390564\n",
      "2021-09-07 17:18:27.261 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.37105032801628113\n",
      "2021-09-07 17:18:27.262 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:27.264 | INFO     | src.policies:train:123 - Epoch 359 / 800\n",
      "2021-09-07 17:18:27.264 | INFO     | src.policies:collect_trajectories:221 - Episode 1132\n",
      "2021-09-07 17:18:27.295 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:27.296 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:27.296 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:27.298 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:27.302 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2620644271373749, 'baseline_loss': 0.43454664945602417, 'total_loss': -0.04479110240936279}\n",
      "2021-09-07 17:18:27.303 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11543359607458115\n",
      "2021-09-07 17:18:27.304 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9713634252548218\n",
      "2021-09-07 17:18:27.305 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11543359607458115\n",
      "2021-09-07 17:18:27.306 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:27.308 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:27.309 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.181287944316864, 'baseline_loss': 0.3853549063205719, 'total_loss': 0.011389508843421936}\n",
      "2021-09-07 17:18:27.311 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10676635801792145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:27.312 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6505684852600098\n",
      "2021-09-07 17:18:27.313 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10676635801792145\n",
      "2021-09-07 17:18:27.314 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:27.315 | INFO     | src.policies:train:123 - Epoch 360 / 800\n",
      "2021-09-07 17:18:27.316 | INFO     | src.policies:collect_trajectories:221 - Episode 1133\n",
      "2021-09-07 17:18:27.344 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:27.344 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:27.345 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:27.347 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:27.350 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.11357978731393814, 'baseline_loss': 0.4616577625274658, 'total_loss': 0.11724909394979477}\n",
      "2021-09-07 17:18:27.351 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20302791893482208\n",
      "2021-09-07 17:18:27.352 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2351151704788208\n",
      "2021-09-07 17:18:27.355 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20302791893482208\n",
      "2021-09-07 17:18:27.357 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:27.358 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:27.360 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23012997210025787, 'baseline_loss': 0.41999512910842896, 'total_loss': -0.020132407546043396}\n",
      "2021-09-07 17:18:27.361 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2409777045249939\n",
      "2021-09-07 17:18:27.362 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4066393375396729\n",
      "2021-09-07 17:18:27.362 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2409777045249939\n",
      "2021-09-07 17:18:27.363 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:27.365 | INFO     | src.policies:train:123 - Epoch 361 / 800\n",
      "2021-09-07 17:18:27.365 | INFO     | src.policies:collect_trajectories:221 - Episode 1134\n",
      "2021-09-07 17:18:27.393 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:27.393 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:27.394 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:27.396 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:27.399 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.22484417259693146, 'baseline_loss': 0.40599822998046875, 'total_loss': -0.021845057606697083}\n",
      "2021-09-07 17:18:27.401 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7317721247673035\n",
      "2021-09-07 17:18:27.401 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7638448476791382\n",
      "2021-09-07 17:18:27.402 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:27.403 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:27.405 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:27.406 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2082868218421936, 'baseline_loss': 0.3471147119998932, 'total_loss': -0.03472946584224701}\n",
      "2021-09-07 17:18:27.406 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3551606833934784\n",
      "2021-09-07 17:18:27.407 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.162524938583374\n",
      "2021-09-07 17:18:27.408 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3551606833934784\n",
      "2021-09-07 17:18:27.410 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:27.411 | INFO     | src.policies:train:123 - Epoch 362 / 800\n",
      "2021-09-07 17:18:27.412 | INFO     | src.policies:collect_trajectories:221 - Episode 1135\n",
      "2021-09-07 17:18:27.440 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:27.441 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:27.441 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:27.444 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:27.447 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.37127941846847534, 'baseline_loss': 0.5826674699783325, 'total_loss': -0.07994568347930908}\n",
      "2021-09-07 17:18:27.448 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20385055243968964\n",
      "2021-09-07 17:18:27.449 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.25377362966537476\n",
      "2021-09-07 17:18:27.450 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20385055243968964\n",
      "2021-09-07 17:18:27.451 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.25377362966537476\n",
      "2021-09-07 17:18:27.452 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:27.453 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.35639679431915283, 'baseline_loss': 0.5232293009757996, 'total_loss': -0.09478214383125305}\n",
      "2021-09-07 17:18:27.454 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5162752866744995\n",
      "2021-09-07 17:18:27.455 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.26318785548210144\n",
      "2021-09-07 17:18:27.457 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:27.458 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.26318785548210144\n",
      "2021-09-07 17:18:27.460 | INFO     | src.policies:train:123 - Epoch 363 / 800\n",
      "2021-09-07 17:18:27.461 | INFO     | src.policies:collect_trajectories:221 - Episode 1136\n",
      "2021-09-07 17:18:27.492 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:27.493 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:27.493 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:27.495 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:27.497 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.35061827301979065, 'baseline_loss': 0.5067476630210876, 'total_loss': -0.09724444150924683}\n",
      "2021-09-07 17:18:27.498 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.06924979388713837\n",
      "2021-09-07 17:18:27.499 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6912677884101868\n",
      "2021-09-07 17:18:27.500 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.06924979388713837\n",
      "2021-09-07 17:18:27.501 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:27.503 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:27.504 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2496061772108078, 'baseline_loss': 0.4280160665512085, 'total_loss': -0.03559814393520355}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:27.505 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30805283784866333\n",
      "2021-09-07 17:18:27.505 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5495187044143677\n",
      "2021-09-07 17:18:27.507 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30805283784866333\n",
      "2021-09-07 17:18:27.507 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:27.509 | INFO     | src.policies:train:123 - Epoch 364 / 800\n",
      "2021-09-07 17:18:27.509 | INFO     | src.policies:collect_trajectories:221 - Episode 1137\n",
      "2021-09-07 17:18:27.540 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:27.540 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:27.541 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:27.543 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:27.545 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.26261967420578003, 'baseline_loss': 0.37331604957580566, 'total_loss': -0.0759616494178772}\n",
      "2021-09-07 17:18:27.545 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2982715666294098\n",
      "2021-09-07 17:18:27.600 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0663741827011108\n",
      "2021-09-07 17:18:27.602 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2982715666294098\n",
      "2021-09-07 17:18:27.603 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:27.604 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:27.605 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32395467162132263, 'baseline_loss': 0.42399442195892334, 'total_loss': -0.11195746064186096}\n",
      "2021-09-07 17:18:27.606 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.44983863830566406\n",
      "2021-09-07 17:18:27.607 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6626332998275757\n",
      "2021-09-07 17:18:27.608 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.44983863830566406\n",
      "2021-09-07 17:18:27.609 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:27.610 | INFO     | src.policies:train:123 - Epoch 365 / 800\n",
      "2021-09-07 17:18:27.611 | INFO     | src.policies:collect_trajectories:221 - Episode 1138\n",
      "2021-09-07 17:18:27.639 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:27.639 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:27.640 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:27.642 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:27.645 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5725613832473755, 'baseline_loss': 1.0586371421813965, 'total_loss': -0.043242812156677246}\n",
      "2021-09-07 17:18:27.646 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.571785569190979\n",
      "2021-09-07 17:18:27.647 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9482278823852539\n",
      "2021-09-07 17:18:27.648 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:27.649 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:27.650 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:27.652 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4730122685432434, 'baseline_loss': 1.0123732089996338, 'total_loss': 0.033174335956573486}\n",
      "2021-09-07 17:18:27.652 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.28286832571029663\n",
      "2021-09-07 17:18:27.654 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0129923820495605\n",
      "2021-09-07 17:18:27.655 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.28286832571029663\n",
      "2021-09-07 17:18:27.656 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:27.658 | INFO     | src.policies:train:123 - Epoch 366 / 800\n",
      "2021-09-07 17:18:27.658 | INFO     | src.policies:collect_trajectories:221 - Episode 1139\n",
      "2021-09-07 17:18:27.687 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:27.687 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:27.688 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:27.689 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:27.692 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.9305643439292908, 'baseline_loss': 3.4496538639068604, 'total_loss': 0.7942625880241394}\n",
      "2021-09-07 17:18:27.693 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24930992722511292\n",
      "2021-09-07 17:18:27.694 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 5.826797962188721\n",
      "2021-09-07 17:18:27.696 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24930992722511292\n",
      "2021-09-07 17:18:27.697 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:27.699 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:27.700 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.9597833156585693, 'baseline_loss': 3.681453227996826, 'total_loss': 0.8809432983398438}\n",
      "2021-09-07 17:18:27.701 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5685502290725708\n",
      "2021-09-07 17:18:27.702 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 5.887775897979736\n",
      "2021-09-07 17:18:27.704 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:27.706 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:27.707 | INFO     | src.policies:train:123 - Epoch 367 / 800\n",
      "2021-09-07 17:18:27.708 | INFO     | src.policies:collect_trajectories:221 - Episode 1140\n",
      "2021-09-07 17:18:27.738 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:27.739 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:27.739 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:27.742 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:27.744 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31462207436561584, 'baseline_loss': 0.3775743544101715, 'total_loss': -0.1258348971605301}\n",
      "2021-09-07 17:18:27.746 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1678837090730667\n",
      "2021-09-07 17:18:27.747 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0013585090637207\n",
      "2021-09-07 17:18:27.748 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1678837090730667\n",
      "2021-09-07 17:18:27.749 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:27.750 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:27.751 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.375771164894104, 'baseline_loss': 0.5763034820556641, 'total_loss': -0.08761942386627197}\n",
      "2021-09-07 17:18:27.752 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1350293755531311\n",
      "2021-09-07 17:18:27.753 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.222791850566864\n",
      "2021-09-07 17:18:27.754 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1350293755531311\n",
      "2021-09-07 17:18:27.755 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.222791850566864\n",
      "2021-09-07 17:18:27.757 | INFO     | src.policies:train:123 - Epoch 368 / 800\n",
      "2021-09-07 17:18:27.757 | INFO     | src.policies:collect_trajectories:221 - Episode 1141\n",
      "2021-09-07 17:18:27.786 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:27.786 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:27.786 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:27.788 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:27.790 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4485401213169098, 'baseline_loss': 0.7334531545639038, 'total_loss': -0.08181354403495789}\n",
      "2021-09-07 17:18:27.791 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11797469109296799\n",
      "2021-09-07 17:18:27.792 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0870682001113892\n",
      "2021-09-07 17:18:27.793 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11797469109296799\n",
      "2021-09-07 17:18:27.794 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:27.795 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:27.796 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.49467891454696655, 'baseline_loss': 0.7807840704917908, 'total_loss': -0.10428687930107117}\n",
      "2021-09-07 17:18:27.797 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22669820487499237\n",
      "2021-09-07 17:18:27.799 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7326419949531555\n",
      "2021-09-07 17:18:27.800 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22669820487499237\n",
      "2021-09-07 17:18:27.801 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:27.803 | INFO     | src.policies:train:123 - Epoch 369 / 800\n",
      "2021-09-07 17:18:27.803 | INFO     | src.policies:collect_trajectories:221 - Episode 1142\n",
      "2021-09-07 17:18:27.834 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:27.835 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:27.835 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:27.837 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:27.840 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8122133612632751, 'baseline_loss': 2.9101991653442383, 'total_loss': 0.642886221408844}\n",
      "2021-09-07 17:18:27.841 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5584059357643127\n",
      "2021-09-07 17:18:27.841 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 5.970242977142334\n",
      "2021-09-07 17:18:27.843 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:27.843 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:27.845 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:27.846 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.841349720954895, 'baseline_loss': 2.8960628509521484, 'total_loss': 0.6066817045211792}\n",
      "2021-09-07 17:18:27.847 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4058094322681427\n",
      "2021-09-07 17:18:27.848 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 6.125588417053223\n",
      "2021-09-07 17:18:27.849 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4058094322681427\n",
      "2021-09-07 17:18:27.850 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999999701976776\n",
      "2021-09-07 17:18:27.852 | INFO     | src.policies:train:123 - Epoch 370 / 800\n",
      "2021-09-07 17:18:27.852 | INFO     | src.policies:collect_trajectories:221 - Episode 1143\n",
      "2021-09-07 17:18:27.882 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:27.882 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:27.883 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:27.884 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:27.887 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.37661659717559814, 'baseline_loss': 0.5240640044212341, 'total_loss': -0.11458459496498108}\n",
      "2021-09-07 17:18:27.888 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4418798089027405\n",
      "2021-09-07 17:18:27.889 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3036026060581207\n",
      "2021-09-07 17:18:27.891 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4418798089027405\n",
      "2021-09-07 17:18:27.892 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3036026060581207\n",
      "2021-09-07 17:18:27.893 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:27.894 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.376585990190506, 'baseline_loss': 0.5220676064491272, 'total_loss': -0.11555218696594238}\n",
      "2021-09-07 17:18:27.895 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20563232898712158\n",
      "2021-09-07 17:18:27.896 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6435892581939697\n",
      "2021-09-07 17:18:27.897 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20563232898712158\n",
      "2021-09-07 17:18:27.898 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:27.899 | INFO     | src.policies:train:123 - Epoch 371 / 800\n",
      "2021-09-07 17:18:27.900 | INFO     | src.policies:collect_trajectories:221 - Episode 1144\n",
      "2021-09-07 17:18:27.928 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:27.928 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:27.929 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:27.930 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:27.933 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4323637783527374, 'baseline_loss': 0.6589742302894592, 'total_loss': -0.10287666320800781}\n",
      "2021-09-07 17:18:27.933 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13758639991283417\n",
      "2021-09-07 17:18:27.934 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5011129379272461\n",
      "2021-09-07 17:18:27.935 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13758639991283417\n",
      "2021-09-07 17:18:27.937 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999892711639404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:27.938 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:27.939 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.501645028591156, 'baseline_loss': 0.8406635522842407, 'total_loss': -0.08131325244903564}\n",
      "2021-09-07 17:18:27.940 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21512524783611298\n",
      "2021-09-07 17:18:27.941 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9607008099555969\n",
      "2021-09-07 17:18:27.943 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21512524783611298\n",
      "2021-09-07 17:18:27.945 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:27.946 | INFO     | src.policies:train:123 - Epoch 372 / 800\n",
      "2021-09-07 17:18:27.947 | INFO     | src.policies:collect_trajectories:221 - Episode 1145\n",
      "2021-09-07 17:18:27.977 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:27.977 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:27.978 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:27.980 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:27.982 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7920887470245361, 'baseline_loss': 2.0821421146392822, 'total_loss': 0.24898231029510498}\n",
      "2021-09-07 17:18:27.983 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5434820652008057\n",
      "2021-09-07 17:18:27.984 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.8014349937438965\n",
      "2021-09-07 17:18:27.985 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:27.987 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999999403953552\n",
      "2021-09-07 17:18:27.988 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:27.989 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7971422076225281, 'baseline_loss': 2.2757978439331055, 'total_loss': 0.34075671434402466}\n",
      "2021-09-07 17:18:27.990 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7850861549377441\n",
      "2021-09-07 17:18:27.991 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.997331142425537\n",
      "2021-09-07 17:18:27.993 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:27.994 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:27.996 | INFO     | src.policies:train:123 - Epoch 373 / 800\n",
      "2021-09-07 17:18:27.996 | INFO     | src.policies:collect_trajectories:221 - Episode 1146\n",
      "2021-09-07 17:18:28.026 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:28.026 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:28.027 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:28.029 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:28.031 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3510114252567291, 'baseline_loss': 0.46426746249198914, 'total_loss': -0.11887769401073456}\n",
      "2021-09-07 17:18:28.032 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.080895334482193\n",
      "2021-09-07 17:18:28.033 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4038669168949127\n",
      "2021-09-07 17:18:28.034 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.080895334482193\n",
      "2021-09-07 17:18:28.035 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4038669168949127\n",
      "2021-09-07 17:18:28.036 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:28.037 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2543005645275116, 'baseline_loss': 0.36792197823524475, 'total_loss': -0.07033957540988922}\n",
      "2021-09-07 17:18:28.038 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09232348948717117\n",
      "2021-09-07 17:18:28.039 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5060322284698486\n",
      "2021-09-07 17:18:28.040 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09232348948717117\n",
      "2021-09-07 17:18:28.041 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:28.042 | INFO     | src.policies:train:123 - Epoch 374 / 800\n",
      "2021-09-07 17:18:28.043 | INFO     | src.policies:collect_trajectories:221 - Episode 1147\n",
      "2021-09-07 17:18:28.065 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:28.066 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 156.0\n",
      "2021-09-07 17:18:28.066 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 156.0\n",
      "2021-09-07 17:18:28.067 | INFO     | src.policies:collect_trajectories:221 - Episode 1148\n",
      "2021-09-07 17:18:28.098 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:28.099 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:28.099 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 178.0\n",
      "2021-09-07 17:18:28.100 | WARNING  | src.policies:train:144 - The actual batch size is 356, instead of 200\n",
      "2021-09-07 17:18:28.140 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:28.165 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.48254939913749695, 'baseline_loss': 0.8110053539276123, 'total_loss': -0.0770467221736908}\n",
      "2021-09-07 17:18:28.166 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5746285319328308\n",
      "2021-09-07 17:18:28.167 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.31908074021339417\n",
      "2021-09-07 17:18:28.168 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:28.169 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.31908074021339417\n",
      "2021-09-07 17:18:28.171 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:28.172 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4409053325653076, 'baseline_loss': 0.7255028486251831, 'total_loss': -0.07815390825271606}\n",
      "2021-09-07 17:18:28.173 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.49727872014045715\n",
      "2021-09-07 17:18:28.174 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.637265145778656\n",
      "2021-09-07 17:18:28.174 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49727872014045715\n",
      "2021-09-07 17:18:28.176 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:28.177 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:28.178 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4325130879878998, 'baseline_loss': 0.7355598211288452, 'total_loss': -0.06473317742347717}\n",
      "2021-09-07 17:18:28.179 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5393913388252258\n",
      "2021-09-07 17:18:28.180 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5484311580657959\n",
      "2021-09-07 17:18:28.181 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990463256836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:28.182 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:28.183 | INFO     | src.policies:train:123 - Epoch 375 / 800\n",
      "2021-09-07 17:18:28.183 | INFO     | src.policies:collect_trajectories:221 - Episode 1149\n",
      "2021-09-07 17:18:28.213 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:28.213 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:28.214 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:28.215 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:28.218 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5076902508735657, 'baseline_loss': 1.1023855209350586, 'total_loss': 0.04350250959396362}\n",
      "2021-09-07 17:18:28.219 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3469427824020386\n",
      "2021-09-07 17:18:28.220 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.2609822750091553\n",
      "2021-09-07 17:18:28.221 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3469427824020386\n",
      "2021-09-07 17:18:28.222 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:28.223 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:28.224 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.528649091720581, 'baseline_loss': 1.1603809595108032, 'total_loss': 0.05154138803482056}\n",
      "2021-09-07 17:18:28.225 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4361022114753723\n",
      "2021-09-07 17:18:28.226 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.4052698612213135\n",
      "2021-09-07 17:18:28.227 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4361022114753723\n",
      "2021-09-07 17:18:28.228 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:28.229 | INFO     | src.policies:train:123 - Epoch 376 / 800\n",
      "2021-09-07 17:18:28.230 | INFO     | src.policies:collect_trajectories:221 - Episode 1150\n",
      "2021-09-07 17:18:28.259 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:28.260 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:28.260 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:28.262 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:28.264 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7819091081619263, 'baseline_loss': 3.45790958404541, 'total_loss': 0.9470456838607788}\n",
      "2021-09-07 17:18:28.265 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7686876654624939\n",
      "2021-09-07 17:18:28.266 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.964453220367432\n",
      "2021-09-07 17:18:28.267 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:28.268 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:28.269 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:28.270 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8812025189399719, 'baseline_loss': 3.408984661102295, 'total_loss': 0.8232898116111755}\n",
      "2021-09-07 17:18:28.271 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 1.0644302368164062\n",
      "2021-09-07 17:18:28.272 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.913812160491943\n",
      "2021-09-07 17:18:28.273 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:28.274 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:28.275 | INFO     | src.policies:train:123 - Epoch 377 / 800\n",
      "2021-09-07 17:18:28.276 | INFO     | src.policies:collect_trajectories:221 - Episode 1151\n",
      "2021-09-07 17:18:28.305 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:28.306 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:28.306 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:28.308 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:28.311 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6987372040748596, 'baseline_loss': 1.5659246444702148, 'total_loss': 0.0842251181602478}\n",
      "2021-09-07 17:18:28.312 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6547460556030273\n",
      "2021-09-07 17:18:28.313 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.7790749073028564\n",
      "2021-09-07 17:18:28.315 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:28.316 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:28.317 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:28.318 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7704899311065674, 'baseline_loss': 1.7076114416122437, 'total_loss': 0.08331578969955444}\n",
      "2021-09-07 17:18:28.319 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4734184443950653\n",
      "2021-09-07 17:18:28.320 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.3654563426971436\n",
      "2021-09-07 17:18:28.321 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4734184443950653\n",
      "2021-09-07 17:18:28.322 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:28.323 | INFO     | src.policies:train:123 - Epoch 378 / 800\n",
      "2021-09-07 17:18:28.324 | INFO     | src.policies:collect_trajectories:221 - Episode 1152\n",
      "2021-09-07 17:18:28.353 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:28.353 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:28.354 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:28.356 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:28.358 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2518770098686218, 'baseline_loss': 0.3860875964164734, 'total_loss': -0.05883321166038513}\n",
      "2021-09-07 17:18:28.359 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31538644433021545\n",
      "2021-09-07 17:18:28.360 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9215096831321716\n",
      "2021-09-07 17:18:28.361 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31538644433021545\n",
      "2021-09-07 17:18:28.362 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:28.363 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:28.364 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2483465075492859, 'baseline_loss': 0.3962380588054657, 'total_loss': -0.05022747814655304}\n",
      "2021-09-07 17:18:28.365 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1362721472978592\n",
      "2021-09-07 17:18:28.366 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2313532829284668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:28.367 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1362721472978592\n",
      "2021-09-07 17:18:28.368 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:28.370 | INFO     | src.policies:train:123 - Epoch 379 / 800\n",
      "2021-09-07 17:18:28.370 | INFO     | src.policies:collect_trajectories:221 - Episode 1153\n",
      "2021-09-07 17:18:28.399 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:28.400 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:28.400 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:28.402 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:28.404 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2899593710899353, 'baseline_loss': 0.5451498627662659, 'total_loss': -0.017384439706802368}\n",
      "2021-09-07 17:18:28.405 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3967665433883667\n",
      "2021-09-07 17:18:28.406 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8761543035507202\n",
      "2021-09-07 17:18:28.407 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3967665433883667\n",
      "2021-09-07 17:18:28.408 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:28.409 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:28.410 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.33019179105758667, 'baseline_loss': 0.539334774017334, 'total_loss': -0.06052440404891968}\n",
      "2021-09-07 17:18:28.411 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.233144149184227\n",
      "2021-09-07 17:18:28.412 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.533854365348816\n",
      "2021-09-07 17:18:28.413 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.233144149184227\n",
      "2021-09-07 17:18:28.415 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:28.416 | INFO     | src.policies:train:123 - Epoch 380 / 800\n",
      "2021-09-07 17:18:28.416 | INFO     | src.policies:collect_trajectories:221 - Episode 1154\n",
      "2021-09-07 17:18:28.445 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:28.446 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:28.447 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:28.449 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:28.451 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5177900791168213, 'baseline_loss': 1.311813473701477, 'total_loss': 0.13811665773391724}\n",
      "2021-09-07 17:18:28.452 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14004255831241608\n",
      "2021-09-07 17:18:28.453 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8690584897994995\n",
      "2021-09-07 17:18:28.454 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14004255831241608\n",
      "2021-09-07 17:18:28.455 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:28.456 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:28.457 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5255137085914612, 'baseline_loss': 0.9776060581207275, 'total_loss': -0.03671067953109741}\n",
      "2021-09-07 17:18:28.458 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.33969342708587646\n",
      "2021-09-07 17:18:28.459 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6366560459136963\n",
      "2021-09-07 17:18:28.461 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.33969342708587646\n",
      "2021-09-07 17:18:28.462 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:28.463 | INFO     | src.policies:train:123 - Epoch 381 / 800\n",
      "2021-09-07 17:18:28.464 | INFO     | src.policies:collect_trajectories:221 - Episode 1155\n",
      "2021-09-07 17:18:28.493 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:28.493 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:28.494 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:28.496 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:28.498 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1595415323972702, 'baseline_loss': 0.8562360405921936, 'total_loss': 0.2685765027999878}\n",
      "2021-09-07 17:18:28.500 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2757341265678406\n",
      "2021-09-07 17:18:28.501 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7174936532974243\n",
      "2021-09-07 17:18:28.502 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2757341265678406\n",
      "2021-09-07 17:18:28.503 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:28.504 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:28.505 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.15785302221775055, 'baseline_loss': 0.8701397180557251, 'total_loss': 0.2772168517112732}\n",
      "2021-09-07 17:18:28.506 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.40101882815361023\n",
      "2021-09-07 17:18:28.507 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6246708631515503\n",
      "2021-09-07 17:18:28.508 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.40101882815361023\n",
      "2021-09-07 17:18:28.509 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:28.510 | INFO     | src.policies:train:123 - Epoch 382 / 800\n",
      "2021-09-07 17:18:28.511 | INFO     | src.policies:collect_trajectories:221 - Episode 1156\n",
      "2021-09-07 17:18:28.541 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:28.541 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:28.542 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:28.544 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:28.547 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.34265026450157166, 'baseline_loss': 0.5916610360145569, 'total_loss': -0.04681974649429321}\n",
      "2021-09-07 17:18:28.548 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32463595271110535\n",
      "2021-09-07 17:18:28.549 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2754382789134979\n",
      "2021-09-07 17:18:28.550 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32463595271110535\n",
      "2021-09-07 17:18:28.551 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2754382789134979\n",
      "2021-09-07 17:18:28.552 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:28.554 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45797809958457947, 'baseline_loss': 0.7898663282394409, 'total_loss': -0.06304493546485901}\n",
      "2021-09-07 17:18:28.555 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09445828199386597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:28.555 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6260919570922852\n",
      "2021-09-07 17:18:28.557 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09445828199386597\n",
      "2021-09-07 17:18:28.558 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:28.559 | INFO     | src.policies:train:123 - Epoch 383 / 800\n",
      "2021-09-07 17:18:28.559 | INFO     | src.policies:collect_trajectories:221 - Episode 1157\n",
      "2021-09-07 17:18:28.588 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:28.589 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:28.589 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:28.592 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:28.594 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.16644881665706635, 'baseline_loss': 0.4769623875617981, 'total_loss': 0.0720323771238327}\n",
      "2021-09-07 17:18:28.595 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14674867689609528\n",
      "2021-09-07 17:18:28.596 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.916244387626648\n",
      "2021-09-07 17:18:28.597 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14674867689609528\n",
      "2021-09-07 17:18:28.598 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:28.599 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:28.600 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.25305262207984924, 'baseline_loss': 0.4880269765853882, 'total_loss': -0.009039133787155151}\n",
      "2021-09-07 17:18:28.602 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09987281262874603\n",
      "2021-09-07 17:18:28.603 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7092595100402832\n",
      "2021-09-07 17:18:28.604 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09987281262874603\n",
      "2021-09-07 17:18:28.606 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:28.608 | INFO     | src.policies:train:123 - Epoch 384 / 800\n",
      "2021-09-07 17:18:28.609 | INFO     | src.policies:collect_trajectories:221 - Episode 1158\n",
      "2021-09-07 17:18:28.858 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:28.858 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:28.859 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:28.861 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:28.864 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5598195791244507, 'baseline_loss': 0.9860442876815796, 'total_loss': -0.06679743528366089}\n",
      "2021-09-07 17:18:28.865 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20311570167541504\n",
      "2021-09-07 17:18:28.866 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9908899664878845\n",
      "2021-09-07 17:18:28.867 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20311570167541504\n",
      "2021-09-07 17:18:28.868 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:28.869 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:28.871 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.47089266777038574, 'baseline_loss': 0.8093534708023071, 'total_loss': -0.06621593236923218}\n",
      "2021-09-07 17:18:28.872 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24732045829296112\n",
      "2021-09-07 17:18:28.873 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6140217781066895\n",
      "2021-09-07 17:18:28.874 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24732045829296112\n",
      "2021-09-07 17:18:28.875 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:28.877 | INFO     | src.policies:train:123 - Epoch 385 / 800\n",
      "2021-09-07 17:18:28.877 | INFO     | src.policies:collect_trajectories:221 - Episode 1159\n",
      "2021-09-07 17:18:28.908 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:28.908 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:28.909 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:28.912 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:28.915 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4935379922389984, 'baseline_loss': 1.7293249368667603, 'total_loss': 0.3711244761943817}\n",
      "2021-09-07 17:18:28.916 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22624239325523376\n",
      "2021-09-07 17:18:28.918 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.593963146209717\n",
      "2021-09-07 17:18:28.920 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22624239325523376\n",
      "2021-09-07 17:18:28.922 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:28.923 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:28.924 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5790627598762512, 'baseline_loss': 1.6811517477035522, 'total_loss': 0.2615131139755249}\n",
      "2021-09-07 17:18:28.925 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6069917678833008\n",
      "2021-09-07 17:18:28.926 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.1238391399383545\n",
      "2021-09-07 17:18:28.927 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:28.928 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:28.929 | INFO     | src.policies:train:123 - Epoch 386 / 800\n",
      "2021-09-07 17:18:28.929 | INFO     | src.policies:collect_trajectories:221 - Episode 1160\n",
      "2021-09-07 17:18:28.958 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:28.959 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 183.0\n",
      "2021-09-07 17:18:28.960 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 183.0\n",
      "2021-09-07 17:18:28.961 | INFO     | src.policies:collect_trajectories:221 - Episode 1161\n",
      "2021-09-07 17:18:28.992 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:28.993 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:28.993 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 191.5\n",
      "2021-09-07 17:18:28.994 | WARNING  | src.policies:train:144 - The actual batch size is 383, instead of 200\n",
      "2021-09-07 17:18:28.996 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:28.998 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2777271270751953, 'baseline_loss': 0.4470882713794708, 'total_loss': -0.0541829913854599}\n",
      "2021-09-07 17:18:28.999 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16373836994171143\n",
      "2021-09-07 17:18:29.000 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9678686261177063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:29.002 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16373836994171143\n",
      "2021-09-07 17:18:29.003 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:29.005 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:29.006 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2896394729614258, 'baseline_loss': 0.5093467831611633, 'total_loss': -0.034966081380844116}\n",
      "2021-09-07 17:18:29.007 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15536366403102875\n",
      "2021-09-07 17:18:29.008 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7232069373130798\n",
      "2021-09-07 17:18:29.009 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15536366403102875\n",
      "2021-09-07 17:18:29.010 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:29.011 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:29.013 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.34224021434783936, 'baseline_loss': 0.5154541730880737, 'total_loss': -0.08451312780380249}\n",
      "2021-09-07 17:18:29.014 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10814549773931503\n",
      "2021-09-07 17:18:29.015 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9399194717407227\n",
      "2021-09-07 17:18:29.016 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10814549773931503\n",
      "2021-09-07 17:18:29.017 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:29.018 | INFO     | src.policies:train:123 - Epoch 387 / 800\n",
      "2021-09-07 17:18:29.019 | INFO     | src.policies:collect_trajectories:221 - Episode 1162\n",
      "2021-09-07 17:18:29.048 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:29.049 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:29.050 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:29.051 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:29.054 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7097008228302002, 'baseline_loss': 1.6615149974822998, 'total_loss': 0.12105667591094971}\n",
      "2021-09-07 17:18:29.055 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4446583092212677\n",
      "2021-09-07 17:18:29.055 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.3362879753112793\n",
      "2021-09-07 17:18:29.057 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4446583092212677\n",
      "2021-09-07 17:18:29.059 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:29.060 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:29.061 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5863814949989319, 'baseline_loss': 1.5174647569656372, 'total_loss': 0.17235088348388672}\n",
      "2021-09-07 17:18:29.062 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.492190957069397\n",
      "2021-09-07 17:18:29.063 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9722093343734741\n",
      "2021-09-07 17:18:29.064 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.492190957069397\n",
      "2021-09-07 17:18:29.066 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:29.067 | INFO     | src.policies:train:123 - Epoch 388 / 800\n",
      "2021-09-07 17:18:29.068 | INFO     | src.policies:collect_trajectories:221 - Episode 1163\n",
      "2021-09-07 17:18:29.097 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:29.098 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:29.098 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:29.100 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:29.102 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24355852603912354, 'baseline_loss': 0.3779163360595703, 'total_loss': -0.05460035800933838}\n",
      "2021-09-07 17:18:29.103 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08711478114128113\n",
      "2021-09-07 17:18:29.104 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1958202123641968\n",
      "2021-09-07 17:18:29.105 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08711478114128113\n",
      "2021-09-07 17:18:29.106 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:29.107 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:29.108 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23856723308563232, 'baseline_loss': 0.3935154974460602, 'total_loss': -0.041809484362602234}\n",
      "2021-09-07 17:18:29.109 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1943271905183792\n",
      "2021-09-07 17:18:29.110 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1870101690292358\n",
      "2021-09-07 17:18:29.111 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1943271905183792\n",
      "2021-09-07 17:18:29.112 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:29.113 | INFO     | src.policies:train:123 - Epoch 389 / 800\n",
      "2021-09-07 17:18:29.114 | INFO     | src.policies:collect_trajectories:221 - Episode 1164\n",
      "2021-09-07 17:18:29.144 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:29.145 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:29.145 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:29.147 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:29.150 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4162749946117401, 'baseline_loss': 0.762857973575592, 'total_loss': -0.03484600782394409}\n",
      "2021-09-07 17:18:29.152 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2787507176399231\n",
      "2021-09-07 17:18:29.152 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6189296841621399\n",
      "2021-09-07 17:18:29.154 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2787507176399231\n",
      "2021-09-07 17:18:29.155 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:29.156 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:29.157 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4240586757659912, 'baseline_loss': 0.7339761257171631, 'total_loss': -0.05707061290740967}\n",
      "2021-09-07 17:18:29.158 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3856520354747772\n",
      "2021-09-07 17:18:29.159 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6575706005096436\n",
      "2021-09-07 17:18:29.160 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3856520354747772\n",
      "2021-09-07 17:18:29.161 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:29.162 | INFO     | src.policies:train:123 - Epoch 390 / 800\n",
      "2021-09-07 17:18:29.163 | INFO     | src.policies:collect_trajectories:221 - Episode 1165\n",
      "2021-09-07 17:18:29.191 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:29.192 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:29.192 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:29.194 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:29.196 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40575259923934937, 'baseline_loss': 0.5981131792068481, 'total_loss': -0.10669600963592529}\n",
      "2021-09-07 17:18:29.197 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2700880765914917\n",
      "2021-09-07 17:18:29.198 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.44086799025535583\n",
      "2021-09-07 17:18:29.199 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2700880765914917\n",
      "2021-09-07 17:18:29.200 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.44086799025535583\n",
      "2021-09-07 17:18:29.201 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:29.202 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3530338406562805, 'baseline_loss': 0.5575686097145081, 'total_loss': -0.07424953579902649}\n",
      "2021-09-07 17:18:29.203 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2944960594177246\n",
      "2021-09-07 17:18:29.204 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4370761215686798\n",
      "2021-09-07 17:18:29.205 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2944960594177246\n",
      "2021-09-07 17:18:29.206 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4370761215686798\n",
      "2021-09-07 17:18:29.207 | INFO     | src.policies:train:123 - Epoch 391 / 800\n",
      "2021-09-07 17:18:29.208 | INFO     | src.policies:collect_trajectories:221 - Episode 1166\n",
      "2021-09-07 17:18:29.237 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:29.237 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:29.238 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:29.240 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:29.242 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24099290370941162, 'baseline_loss': 0.42753446102142334, 'total_loss': -0.02722567319869995}\n",
      "2021-09-07 17:18:29.243 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1804571896791458\n",
      "2021-09-07 17:18:29.244 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7630338072776794\n",
      "2021-09-07 17:18:29.245 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1804571896791458\n",
      "2021-09-07 17:18:29.246 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:29.247 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:29.248 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2882058620452881, 'baseline_loss': 0.4070361852645874, 'total_loss': -0.08468776941299438}\n",
      "2021-09-07 17:18:29.249 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08639416843652725\n",
      "2021-09-07 17:18:29.250 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9275358319282532\n",
      "2021-09-07 17:18:29.251 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08639416843652725\n",
      "2021-09-07 17:18:29.252 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:29.254 | INFO     | src.policies:train:123 - Epoch 392 / 800\n",
      "2021-09-07 17:18:29.254 | INFO     | src.policies:collect_trajectories:221 - Episode 1167\n",
      "2021-09-07 17:18:29.283 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:29.284 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:29.284 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:29.286 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:29.288 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5225725173950195, 'baseline_loss': 1.8315484523773193, 'total_loss': 0.39320170879364014}\n",
      "2021-09-07 17:18:29.289 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5212972164154053\n",
      "2021-09-07 17:18:29.290 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.5573747158050537\n",
      "2021-09-07 17:18:29.292 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999989867210388\n",
      "2021-09-07 17:18:29.293 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:29.295 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:29.296 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3579040467739105, 'baseline_loss': 1.6275668144226074, 'total_loss': 0.4558793604373932}\n",
      "2021-09-07 17:18:29.296 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5067858099937439\n",
      "2021-09-07 17:18:29.297 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.6886558532714844\n",
      "2021-09-07 17:18:29.299 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:29.300 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:29.302 | INFO     | src.policies:train:123 - Epoch 393 / 800\n",
      "2021-09-07 17:18:29.302 | INFO     | src.policies:collect_trajectories:221 - Episode 1168\n",
      "2021-09-07 17:18:29.318 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:29.319 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 104.0\n",
      "2021-09-07 17:18:29.319 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 104.0\n",
      "2021-09-07 17:18:29.320 | INFO     | src.policies:collect_trajectories:221 - Episode 1169\n",
      "2021-09-07 17:18:29.403 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:29.403 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:29.404 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 152.0\n",
      "2021-09-07 17:18:29.404 | WARNING  | src.policies:train:144 - The actual batch size is 304, instead of 200\n",
      "2021-09-07 17:18:29.407 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:29.409 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.37531906366348267, 'baseline_loss': 0.433106929063797, 'total_loss': -0.15876559913158417}\n",
      "2021-09-07 17:18:29.410 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.27772417664527893\n",
      "2021-09-07 17:18:29.411 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7386010885238647\n",
      "2021-09-07 17:18:29.412 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.27772417664527893\n",
      "2021-09-07 17:18:29.413 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:29.414 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:29.415 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2526404857635498, 'baseline_loss': 0.42742615938186646, 'total_loss': -0.03892740607261658}\n",
      "2021-09-07 17:18:29.416 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1337449550628662\n",
      "2021-09-07 17:18:29.417 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3824814558029175\n",
      "2021-09-07 17:18:29.418 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1337449550628662\n",
      "2021-09-07 17:18:29.419 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:29.420 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:29.422 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2914535403251648, 'baseline_loss': 0.41025570034980774, 'total_loss': -0.08632569015026093}\n",
      "2021-09-07 17:18:29.422 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14272764325141907\n",
      "2021-09-07 17:18:29.423 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3152202367782593\n",
      "2021-09-07 17:18:29.424 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14272764325141907\n",
      "2021-09-07 17:18:29.425 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:29.427 | INFO     | src.policies:train:123 - Epoch 394 / 800\n",
      "2021-09-07 17:18:29.427 | INFO     | src.policies:collect_trajectories:221 - Episode 1170\n",
      "2021-09-07 17:18:29.459 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:29.459 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:29.460 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:29.464 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:29.466 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.34236598014831543, 'baseline_loss': 0.6030181646347046, 'total_loss': -0.040856897830963135}\n",
      "2021-09-07 17:18:29.467 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2512257695198059\n",
      "2021-09-07 17:18:29.467 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5646602511405945\n",
      "2021-09-07 17:18:29.468 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2512257695198059\n",
      "2021-09-07 17:18:29.469 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:29.471 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:29.472 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2626916766166687, 'baseline_loss': 0.5520530939102173, 'total_loss': 0.013334870338439941}\n",
      "2021-09-07 17:18:29.473 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21442778408527374\n",
      "2021-09-07 17:18:29.474 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3878594636917114\n",
      "2021-09-07 17:18:29.475 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21442778408527374\n",
      "2021-09-07 17:18:29.476 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:29.477 | INFO     | src.policies:train:123 - Epoch 395 / 800\n",
      "2021-09-07 17:18:29.478 | INFO     | src.policies:collect_trajectories:221 - Episode 1171\n",
      "2021-09-07 17:18:29.508 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:29.509 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:29.509 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:29.511 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:29.514 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4749324321746826, 'baseline_loss': 2.0784900188446045, 'total_loss': 0.5643125772476196}\n",
      "2021-09-07 17:18:29.515 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3440706729888916\n",
      "2021-09-07 17:18:29.516 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.066082000732422\n",
      "2021-09-07 17:18:29.517 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3440706729888916\n",
      "2021-09-07 17:18:29.518 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:29.520 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:29.521 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5531604290008545, 'baseline_loss': 2.0925989151000977, 'total_loss': 0.49313902854919434}\n",
      "2021-09-07 17:18:29.522 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6108083724975586\n",
      "2021-09-07 17:18:29.523 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.459468364715576\n",
      "2021-09-07 17:18:29.524 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:29.525 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:29.526 | INFO     | src.policies:train:123 - Epoch 396 / 800\n",
      "2021-09-07 17:18:29.526 | INFO     | src.policies:collect_trajectories:221 - Episode 1172\n",
      "2021-09-07 17:18:29.555 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:29.556 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:29.556 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:29.558 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:29.560 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2376224398612976, 'baseline_loss': 0.42300713062286377, 'total_loss': -0.026118874549865723}\n",
      "2021-09-07 17:18:29.561 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20342528820037842\n",
      "2021-09-07 17:18:29.562 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.300467610359192\n",
      "2021-09-07 17:18:29.563 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20342528820037842\n",
      "2021-09-07 17:18:29.564 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:29.565 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:29.567 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.06470999866724014, 'baseline_loss': 0.49232712388038635, 'total_loss': 0.18145355582237244}\n",
      "2021-09-07 17:18:29.568 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2359749972820282\n",
      "2021-09-07 17:18:29.568 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5780174732208252\n",
      "2021-09-07 17:18:29.569 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2359749972820282\n",
      "2021-09-07 17:18:29.570 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:29.572 | INFO     | src.policies:train:123 - Epoch 397 / 800\n",
      "2021-09-07 17:18:29.572 | INFO     | src.policies:collect_trajectories:221 - Episode 1173\n",
      "2021-09-07 17:18:29.600 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:29.601 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:29.601 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:29.603 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:29.605 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4642680883407593, 'baseline_loss': 1.2792465686798096, 'total_loss': 0.1753551959991455}\n",
      "2021-09-07 17:18:29.606 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2607796788215637\n",
      "2021-09-07 17:18:29.607 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.913916826248169\n",
      "2021-09-07 17:18:29.608 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2607796788215637\n",
      "2021-09-07 17:18:29.609 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:29.610 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:29.611 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4613685607910156, 'baseline_loss': 1.3349589109420776, 'total_loss': 0.2061108946800232}\n",
      "2021-09-07 17:18:29.612 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.46280547976493835\n",
      "2021-09-07 17:18:29.613 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.566264033317566\n",
      "2021-09-07 17:18:29.614 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.46280547976493835\n",
      "2021-09-07 17:18:29.615 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:29.617 | INFO     | src.policies:train:123 - Epoch 398 / 800\n",
      "2021-09-07 17:18:29.617 | INFO     | src.policies:collect_trajectories:221 - Episode 1174\n",
      "2021-09-07 17:18:29.647 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:29.647 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:29.648 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:29.651 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:29.655 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.22866734862327576, 'baseline_loss': 0.919334888458252, 'total_loss': 0.23100009560585022}\n",
      "2021-09-07 17:18:29.656 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.8034483790397644\n",
      "2021-09-07 17:18:29.658 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5299009084701538\n",
      "2021-09-07 17:18:29.659 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:29.660 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:29.662 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:29.663 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.21211302280426025, 'baseline_loss': 0.9637871384620667, 'total_loss': 0.26978054642677307}\n",
      "2021-09-07 17:18:29.664 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.38280194997787476\n",
      "2021-09-07 17:18:29.665 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.441260576248169\n",
      "2021-09-07 17:18:29.666 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.38280194997787476\n",
      "2021-09-07 17:18:29.667 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:29.669 | INFO     | src.policies:train:123 - Epoch 399 / 800\n",
      "2021-09-07 17:18:29.670 | INFO     | src.policies:collect_trajectories:221 - Episode 1175\n",
      "2021-09-07 17:18:29.685 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:29.685 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 98.0\n",
      "2021-09-07 17:18:29.686 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 98.0\n",
      "2021-09-07 17:18:29.686 | INFO     | src.policies:collect_trajectories:221 - Episode 1176\n",
      "2021-09-07 17:18:29.716 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:29.716 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:29.717 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 149.0\n",
      "2021-09-07 17:18:29.718 | WARNING  | src.policies:train:144 - The actual batch size is 298, instead of 200\n",
      "2021-09-07 17:18:29.720 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:29.722 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5623131394386292, 'baseline_loss': 1.417995572090149, 'total_loss': 0.1466846466064453}\n",
      "2021-09-07 17:18:29.723 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5815889835357666\n",
      "2021-09-07 17:18:29.724 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.1254422664642334\n",
      "2021-09-07 17:18:29.725 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:29.726 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:29.727 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:29.729 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5456230044364929, 'baseline_loss': 1.2477375268936157, 'total_loss': 0.07824575901031494}\n",
      "2021-09-07 17:18:29.730 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1965097039937973\n",
      "2021-09-07 17:18:29.731 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4258899688720703\n",
      "2021-09-07 17:18:29.732 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1965097039937973\n",
      "2021-09-07 17:18:29.733 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:29.734 | INFO     | src.policies:train:123 - Epoch 400 / 800\n",
      "2021-09-07 17:18:29.735 | INFO     | src.policies:collect_trajectories:221 - Episode 1177\n",
      "2021-09-07 17:18:29.766 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:29.766 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:29.767 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:29.769 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:29.771 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5299819707870483, 'baseline_loss': 1.8205580711364746, 'total_loss': 0.38029706478118896}\n",
      "2021-09-07 17:18:29.772 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.63658607006073\n",
      "2021-09-07 17:18:29.773 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.250600814819336\n",
      "2021-09-07 17:18:29.774 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:29.775 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:29.776 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:29.777 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5039018392562866, 'baseline_loss': 1.5178489685058594, 'total_loss': 0.25502264499664307}\n",
      "2021-09-07 17:18:29.778 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2548757493495941\n",
      "2021-09-07 17:18:29.779 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.3643853664398193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:29.780 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2548757493495941\n",
      "2021-09-07 17:18:29.781 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:29.783 | INFO     | src.policies:train:123 - Epoch 401 / 800\n",
      "2021-09-07 17:18:29.783 | INFO     | src.policies:collect_trajectories:221 - Episode 1178\n",
      "2021-09-07 17:18:29.813 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:29.814 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:29.814 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:29.816 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:29.818 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4290468096733093, 'baseline_loss': 1.1078122854232788, 'total_loss': 0.12485933303833008}\n",
      "2021-09-07 17:18:29.819 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.351188987493515\n",
      "2021-09-07 17:18:29.820 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9815362691879272\n",
      "2021-09-07 17:18:29.821 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.351188987493515\n",
      "2021-09-07 17:18:29.823 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:29.824 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:29.825 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40755295753479004, 'baseline_loss': 0.9629097580909729, 'total_loss': 0.07390192151069641}\n",
      "2021-09-07 17:18:29.826 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20360355079174042\n",
      "2021-09-07 17:18:29.827 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9453206658363342\n",
      "2021-09-07 17:18:29.828 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20360355079174042\n",
      "2021-09-07 17:18:29.829 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:29.830 | INFO     | src.policies:train:123 - Epoch 402 / 800\n",
      "2021-09-07 17:18:29.831 | INFO     | src.policies:collect_trajectories:221 - Episode 1179\n",
      "2021-09-07 17:18:29.861 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:29.861 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:29.862 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:29.864 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:29.866 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4273604452610016, 'baseline_loss': 0.8678300380706787, 'total_loss': 0.0065545737743377686}\n",
      "2021-09-07 17:18:29.867 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.34952667355537415\n",
      "2021-09-07 17:18:29.868 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.034193754196167\n",
      "2021-09-07 17:18:29.870 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.34952667355537415\n",
      "2021-09-07 17:18:29.871 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:29.872 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:29.874 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4063537120819092, 'baseline_loss': 0.8527255058288574, 'total_loss': 0.02000904083251953}\n",
      "2021-09-07 17:18:29.874 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.41131043434143066\n",
      "2021-09-07 17:18:29.875 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9342426061630249\n",
      "2021-09-07 17:18:29.876 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.41131043434143066\n",
      "2021-09-07 17:18:29.877 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:29.879 | INFO     | src.policies:train:123 - Epoch 403 / 800\n",
      "2021-09-07 17:18:29.879 | INFO     | src.policies:collect_trajectories:221 - Episode 1180\n",
      "2021-09-07 17:18:29.970 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:29.971 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:29.972 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:29.973 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:29.976 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.33446988463401794, 'baseline_loss': 0.5400623679161072, 'total_loss': -0.06443870067596436}\n",
      "2021-09-07 17:18:29.977 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13238659501075745\n",
      "2021-09-07 17:18:29.978 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5278305411338806\n",
      "2021-09-07 17:18:29.979 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13238659501075745\n",
      "2021-09-07 17:18:29.980 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:29.981 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:29.982 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4017481207847595, 'baseline_loss': 0.7155548334121704, 'total_loss': -0.043970704078674316}\n",
      "2021-09-07 17:18:29.983 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5672630667686462\n",
      "2021-09-07 17:18:29.984 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5751990675926208\n",
      "2021-09-07 17:18:29.985 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:29.986 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:29.987 | INFO     | src.policies:train:123 - Epoch 404 / 800\n",
      "2021-09-07 17:18:29.988 | INFO     | src.policies:collect_trajectories:221 - Episode 1181\n",
      "2021-09-07 17:18:30.017 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:30.018 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:30.018 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:30.022 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:30.023 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1838737428188324, 'baseline_loss': 0.37216758728027344, 'total_loss': 0.0022100508213043213}\n",
      "2021-09-07 17:18:30.024 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12946267426013947\n",
      "2021-09-07 17:18:30.025 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.589682698249817\n",
      "2021-09-07 17:18:30.027 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12946267426013947\n",
      "2021-09-07 17:18:30.028 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:30.029 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:30.030 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.217171311378479, 'baseline_loss': 0.44164150953292847, 'total_loss': 0.0036494433879852295}\n",
      "2021-09-07 17:18:30.031 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1197444275021553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:30.031 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6868997812271118\n",
      "2021-09-07 17:18:30.032 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1197444275021553\n",
      "2021-09-07 17:18:30.033 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:30.035 | INFO     | src.policies:train:123 - Epoch 405 / 800\n",
      "2021-09-07 17:18:30.035 | INFO     | src.policies:collect_trajectories:221 - Episode 1182\n",
      "2021-09-07 17:18:30.064 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:30.064 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:30.065 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:30.067 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:30.069 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3082510828971863, 'baseline_loss': 0.4433789849281311, 'total_loss': -0.08656159043312073}\n",
      "2021-09-07 17:18:30.070 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5005633234977722\n",
      "2021-09-07 17:18:30.071 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2692174911499023\n",
      "2021-09-07 17:18:30.072 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999895691871643\n",
      "2021-09-07 17:18:30.073 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:30.074 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:30.075 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.25586622953414917, 'baseline_loss': 0.40107154846191406, 'total_loss': -0.05533045530319214}\n",
      "2021-09-07 17:18:30.076 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.36896416544914246\n",
      "2021-09-07 17:18:30.077 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1140813827514648\n",
      "2021-09-07 17:18:30.078 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.36896416544914246\n",
      "2021-09-07 17:18:30.079 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:30.080 | INFO     | src.policies:train:123 - Epoch 406 / 800\n",
      "2021-09-07 17:18:30.081 | INFO     | src.policies:collect_trajectories:221 - Episode 1183\n",
      "2021-09-07 17:18:30.110 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:30.110 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:30.110 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:30.112 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:30.114 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.08827231079339981, 'baseline_loss': 0.5757485032081604, 'total_loss': 0.199601948261261}\n",
      "2021-09-07 17:18:30.115 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15668687224388123\n",
      "2021-09-07 17:18:30.116 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5300878286361694\n",
      "2021-09-07 17:18:30.117 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15668687224388123\n",
      "2021-09-07 17:18:30.118 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:30.120 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:30.121 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.10926178842782974, 'baseline_loss': 0.6343909502029419, 'total_loss': 0.2079336941242218}\n",
      "2021-09-07 17:18:30.122 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3014290928840637\n",
      "2021-09-07 17:18:30.123 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1449854373931885\n",
      "2021-09-07 17:18:30.124 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3014290928840637\n",
      "2021-09-07 17:18:30.125 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:30.126 | INFO     | src.policies:train:123 - Epoch 407 / 800\n",
      "2021-09-07 17:18:30.127 | INFO     | src.policies:collect_trajectories:221 - Episode 1184\n",
      "2021-09-07 17:18:30.154 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:30.155 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:30.155 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:30.158 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:30.160 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2968718707561493, 'baseline_loss': 0.9339713454246521, 'total_loss': 0.17011380195617676}\n",
      "2021-09-07 17:18:30.161 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2174808830022812\n",
      "2021-09-07 17:18:30.162 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4302369356155396\n",
      "2021-09-07 17:18:30.163 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2174808830022812\n",
      "2021-09-07 17:18:30.165 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:30.166 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:30.167 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24652807414531708, 'baseline_loss': 0.81683748960495, 'total_loss': 0.1618906706571579}\n",
      "2021-09-07 17:18:30.168 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18110154569149017\n",
      "2021-09-07 17:18:30.169 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2239691019058228\n",
      "2021-09-07 17:18:30.170 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18110154569149017\n",
      "2021-09-07 17:18:30.171 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:30.172 | INFO     | src.policies:train:123 - Epoch 408 / 800\n",
      "2021-09-07 17:18:30.173 | INFO     | src.policies:collect_trajectories:221 - Episode 1185\n",
      "2021-09-07 17:18:30.191 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:30.192 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 121.0\n",
      "2021-09-07 17:18:30.192 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 121.0\n",
      "2021-09-07 17:18:30.193 | INFO     | src.policies:collect_trajectories:221 - Episode 1186\n",
      "2021-09-07 17:18:30.219 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:30.219 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 175.0\n",
      "2021-09-07 17:18:30.220 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 148.0\n",
      "2021-09-07 17:18:30.220 | WARNING  | src.policies:train:144 - The actual batch size is 296, instead of 200\n",
      "2021-09-07 17:18:30.223 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:30.225 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.49461159110069275, 'baseline_loss': 1.1099061965942383, 'total_loss': 0.06034150719642639}\n",
      "2021-09-07 17:18:30.226 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6855946779251099\n",
      "2021-09-07 17:18:30.227 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6765627264976501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:30.228 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:30.229 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:30.231 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:30.232 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2718289792537689, 'baseline_loss': 1.213218331336975, 'total_loss': 0.33478018641471863}\n",
      "2021-09-07 17:18:30.233 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1636267602443695\n",
      "2021-09-07 17:18:30.233 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8592065572738647\n",
      "2021-09-07 17:18:30.235 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1636267602443695\n",
      "2021-09-07 17:18:30.236 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:30.237 | INFO     | src.policies:train:123 - Epoch 409 / 800\n",
      "2021-09-07 17:18:30.237 | INFO     | src.policies:collect_trajectories:221 - Episode 1187\n",
      "2021-09-07 17:18:30.265 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:30.265 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:30.266 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:30.268 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:30.270 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6663830280303955, 'baseline_loss': 2.17813777923584, 'total_loss': 0.4226858615875244}\n",
      "2021-09-07 17:18:30.271 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19668743014335632\n",
      "2021-09-07 17:18:30.273 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.002373218536377\n",
      "2021-09-07 17:18:30.274 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19668743014335632\n",
      "2021-09-07 17:18:30.275 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:30.276 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:30.277 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7969709634780884, 'baseline_loss': 2.2908902168273926, 'total_loss': 0.3484741449356079}\n",
      "2021-09-07 17:18:30.278 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4967578053474426\n",
      "2021-09-07 17:18:30.279 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.9113376140594482\n",
      "2021-09-07 17:18:30.280 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4967578053474426\n",
      "2021-09-07 17:18:30.281 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:30.283 | INFO     | src.policies:train:123 - Epoch 410 / 800\n",
      "2021-09-07 17:18:30.283 | INFO     | src.policies:collect_trajectories:221 - Episode 1188\n",
      "2021-09-07 17:18:30.312 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:30.312 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:30.313 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:30.315 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:30.317 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.737723171710968, 'baseline_loss': 2.7853176593780518, 'total_loss': 0.6549356579780579}\n",
      "2021-09-07 17:18:30.318 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3461271822452545\n",
      "2021-09-07 17:18:30.319 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.585799217224121\n",
      "2021-09-07 17:18:30.321 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3461271822452545\n",
      "2021-09-07 17:18:30.322 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:30.323 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:30.324 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.786343514919281, 'baseline_loss': 2.997563123703003, 'total_loss': 0.7124380469322205}\n",
      "2021-09-07 17:18:30.325 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4825682044029236\n",
      "2021-09-07 17:18:30.326 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.812813758850098\n",
      "2021-09-07 17:18:30.327 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4825682044029236\n",
      "2021-09-07 17:18:30.328 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:30.330 | INFO     | src.policies:train:123 - Epoch 411 / 800\n",
      "2021-09-07 17:18:30.330 | INFO     | src.policies:collect_trajectories:221 - Episode 1189\n",
      "2021-09-07 17:18:30.334 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:30.335 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 20.0\n",
      "2021-09-07 17:18:30.335 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 20.0\n",
      "2021-09-07 17:18:30.335 | INFO     | src.policies:collect_trajectories:221 - Episode 1190\n",
      "2021-09-07 17:18:30.366 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:30.367 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:30.367 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 110.0\n",
      "2021-09-07 17:18:30.368 | WARNING  | src.policies:train:144 - The actual batch size is 220, instead of 200\n",
      "2021-09-07 17:18:30.370 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:30.372 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7482410669326782, 'baseline_loss': 1.765940546989441, 'total_loss': 0.13472920656204224}\n",
      "2021-09-07 17:18:30.373 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5448031425476074\n",
      "2021-09-07 17:18:30.374 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8813515901565552\n",
      "2021-09-07 17:18:30.375 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:30.376 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:30.377 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:30.379 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5219627618789673, 'baseline_loss': 1.346360445022583, 'total_loss': 0.15121746063232422}\n",
      "2021-09-07 17:18:30.379 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25958192348480225\n",
      "2021-09-07 17:18:30.380 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3901755809783936\n",
      "2021-09-07 17:18:30.381 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25958192348480225\n",
      "2021-09-07 17:18:30.382 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:30.384 | INFO     | src.policies:train:123 - Epoch 412 / 800\n",
      "2021-09-07 17:18:30.384 | INFO     | src.policies:collect_trajectories:221 - Episode 1191\n",
      "2021-09-07 17:18:30.412 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:30.413 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:30.413 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:30.415 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:30.418 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.668261706829071, 'baseline_loss': 1.69557785987854, 'total_loss': 0.17952722311019897}\n",
      "2021-09-07 17:18:30.419 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25066953897476196\n",
      "2021-09-07 17:18:30.420 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.429137945175171\n",
      "2021-09-07 17:18:30.421 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25066953897476196\n",
      "2021-09-07 17:18:30.422 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:30.423 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:30.424 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6279918551445007, 'baseline_loss': 1.7026944160461426, 'total_loss': 0.22335535287857056}\n",
      "2021-09-07 17:18:30.425 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2334846705198288\n",
      "2021-09-07 17:18:30.426 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.994166374206543\n",
      "2021-09-07 17:18:30.427 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2334846705198288\n",
      "2021-09-07 17:18:30.428 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:30.429 | INFO     | src.policies:train:123 - Epoch 413 / 800\n",
      "2021-09-07 17:18:30.430 | INFO     | src.policies:collect_trajectories:221 - Episode 1192\n",
      "2021-09-07 17:18:30.514 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:30.515 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:30.515 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:30.517 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:30.520 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5469052791595459, 'baseline_loss': 1.5807085037231445, 'total_loss': 0.24344897270202637}\n",
      "2021-09-07 17:18:30.521 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18523317575454712\n",
      "2021-09-07 17:18:30.522 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.116863250732422\n",
      "2021-09-07 17:18:30.523 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18523317575454712\n",
      "2021-09-07 17:18:30.524 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:30.526 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:30.527 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6237955689430237, 'baseline_loss': 1.4776397943496704, 'total_loss': 0.11502432823181152}\n",
      "2021-09-07 17:18:30.528 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31508371233940125\n",
      "2021-09-07 17:18:30.529 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.508592128753662\n",
      "2021-09-07 17:18:30.530 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31508371233940125\n",
      "2021-09-07 17:18:30.531 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:30.532 | INFO     | src.policies:train:123 - Epoch 414 / 800\n",
      "2021-09-07 17:18:30.533 | INFO     | src.policies:collect_trajectories:221 - Episode 1193\n",
      "2021-09-07 17:18:30.562 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:30.562 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:30.563 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:30.565 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:30.567 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.12787207961082458, 'baseline_loss': 0.7066315412521362, 'total_loss': 0.22544369101524353}\n",
      "2021-09-07 17:18:30.568 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09344770759344101\n",
      "2021-09-07 17:18:30.569 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5123869180679321\n",
      "2021-09-07 17:18:30.570 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09344770759344101\n",
      "2021-09-07 17:18:30.571 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:30.572 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:30.573 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.13931000232696533, 'baseline_loss': 0.6396005153656006, 'total_loss': 0.18049025535583496}\n",
      "2021-09-07 17:18:30.574 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4276638329029083\n",
      "2021-09-07 17:18:30.575 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7681257724761963\n",
      "2021-09-07 17:18:30.576 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4276638329029083\n",
      "2021-09-07 17:18:30.577 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:30.578 | INFO     | src.policies:train:123 - Epoch 415 / 800\n",
      "2021-09-07 17:18:30.579 | INFO     | src.policies:collect_trajectories:221 - Episode 1194\n",
      "2021-09-07 17:18:30.607 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:30.608 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:30.608 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:30.610 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:30.612 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6491007208824158, 'baseline_loss': 1.8542789220809937, 'total_loss': 0.27803874015808105}\n",
      "2021-09-07 17:18:30.613 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5222460031509399\n",
      "2021-09-07 17:18:30.614 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.016965627670288\n",
      "2021-09-07 17:18:30.615 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:30.616 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:30.618 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:30.619 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5763123035430908, 'baseline_loss': 1.6173568964004517, 'total_loss': 0.232366144657135}\n",
      "2021-09-07 17:18:30.620 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21576149761676788\n",
      "2021-09-07 17:18:30.620 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.5507261753082275\n",
      "2021-09-07 17:18:30.621 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21576149761676788\n",
      "2021-09-07 17:18:30.622 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:30.624 | INFO     | src.policies:train:123 - Epoch 416 / 800\n",
      "2021-09-07 17:18:30.624 | INFO     | src.policies:collect_trajectories:221 - Episode 1195\n",
      "2021-09-07 17:18:30.653 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:30.653 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:30.654 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:30.657 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:30.659 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.18899936974048615, 'baseline_loss': 0.36808180809020996, 'total_loss': -0.0049584656953811646}\n",
      "2021-09-07 17:18:30.660 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1260567009449005\n",
      "2021-09-07 17:18:30.661 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1095333099365234\n",
      "2021-09-07 17:18:30.662 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1260567009449005\n",
      "2021-09-07 17:18:30.663 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:30.664 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:30.665 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2817022502422333, 'baseline_loss': 0.45956355333328247, 'total_loss': -0.05192047357559204}\n",
      "2021-09-07 17:18:30.666 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21803918480873108\n",
      "2021-09-07 17:18:30.667 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0595793724060059\n",
      "2021-09-07 17:18:30.668 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21803918480873108\n",
      "2021-09-07 17:18:30.669 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:30.670 | INFO     | src.policies:train:123 - Epoch 417 / 800\n",
      "2021-09-07 17:18:30.671 | INFO     | src.policies:collect_trajectories:221 - Episode 1196\n",
      "2021-09-07 17:18:30.698 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:30.698 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 193.0\n",
      "2021-09-07 17:18:30.699 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 193.0\n",
      "2021-09-07 17:18:30.699 | INFO     | src.policies:collect_trajectories:221 - Episode 1197\n",
      "2021-09-07 17:18:30.846 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:30.846 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:30.847 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 196.5\n",
      "2021-09-07 17:18:30.847 | WARNING  | src.policies:train:144 - The actual batch size is 393, instead of 200\n",
      "2021-09-07 17:18:30.850 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:30.852 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1481236219406128, 'baseline_loss': 0.4206337332725525, 'total_loss': 0.06219324469566345}\n",
      "2021-09-07 17:18:30.853 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11342601478099823\n",
      "2021-09-07 17:18:30.854 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4618372917175293\n",
      "2021-09-07 17:18:30.856 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11342601478099823\n",
      "2021-09-07 17:18:30.857 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:30.858 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:30.859 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.18534383177757263, 'baseline_loss': 0.46690574288368225, 'total_loss': 0.048109039664268494}\n",
      "2021-09-07 17:18:30.860 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14065426588058472\n",
      "2021-09-07 17:18:30.861 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5381463766098022\n",
      "2021-09-07 17:18:30.862 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14065426588058472\n",
      "2021-09-07 17:18:30.863 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:30.864 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:30.865 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.18656492233276367, 'baseline_loss': 0.4285025894641876, 'total_loss': 0.02768637239933014}\n",
      "2021-09-07 17:18:30.866 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09965992718935013\n",
      "2021-09-07 17:18:30.867 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5709532499313354\n",
      "2021-09-07 17:18:30.868 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09965992718935013\n",
      "2021-09-07 17:18:30.869 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:30.870 | INFO     | src.policies:train:123 - Epoch 418 / 800\n",
      "2021-09-07 17:18:30.870 | INFO     | src.policies:collect_trajectories:221 - Episode 1198\n",
      "2021-09-07 17:18:30.898 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:30.898 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:30.899 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:30.901 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:30.904 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.47179508209228516, 'baseline_loss': 1.290360927581787, 'total_loss': 0.1733853816986084}\n",
      "2021-09-07 17:18:30.905 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1199910044670105\n",
      "2021-09-07 17:18:30.906 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.231671690940857\n",
      "2021-09-07 17:18:30.907 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1199910044670105\n",
      "2021-09-07 17:18:30.907 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:30.909 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:30.910 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4968940317630768, 'baseline_loss': 1.2951713800430298, 'total_loss': 0.1506916582584381}\n",
      "2021-09-07 17:18:30.910 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.44670137763023376\n",
      "2021-09-07 17:18:30.911 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2801034450531006\n",
      "2021-09-07 17:18:30.912 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.44670137763023376\n",
      "2021-09-07 17:18:30.913 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:30.915 | INFO     | src.policies:train:123 - Epoch 419 / 800\n",
      "2021-09-07 17:18:30.915 | INFO     | src.policies:collect_trajectories:221 - Episode 1199\n",
      "2021-09-07 17:18:30.943 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:30.944 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 179.0\n",
      "2021-09-07 17:18:30.944 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 179.0\n",
      "2021-09-07 17:18:30.944 | INFO     | src.policies:collect_trajectories:221 - Episode 1200\n",
      "2021-09-07 17:18:30.973 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:30.973 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:30.974 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 189.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:30.974 | WARNING  | src.policies:train:144 - The actual batch size is 379, instead of 200\n",
      "2021-09-07 17:18:30.977 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:30.980 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.21794359385967255, 'baseline_loss': 0.460184246301651, 'total_loss': 0.012148529291152954}\n",
      "2021-09-07 17:18:30.981 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19535812735557556\n",
      "2021-09-07 17:18:30.982 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5485113859176636\n",
      "2021-09-07 17:18:30.983 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19535812735557556\n",
      "2021-09-07 17:18:30.984 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:30.985 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:30.986 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.15412920713424683, 'baseline_loss': 0.4708040654659271, 'total_loss': 0.08127282559871674}\n",
      "2021-09-07 17:18:30.987 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18903806805610657\n",
      "2021-09-07 17:18:30.988 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.444810152053833\n",
      "2021-09-07 17:18:30.989 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18903806805610657\n",
      "2021-09-07 17:18:30.990 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:30.991 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:30.992 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.15388058125972748, 'baseline_loss': 0.42223456501960754, 'total_loss': 0.057236701250076294}\n",
      "2021-09-07 17:18:30.993 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.34800663590431213\n",
      "2021-09-07 17:18:30.994 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.680844783782959\n",
      "2021-09-07 17:18:30.995 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.34800663590431213\n",
      "2021-09-07 17:18:30.996 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:30.997 | INFO     | src.policies:train:123 - Epoch 420 / 800\n",
      "2021-09-07 17:18:30.997 | INFO     | src.policies:collect_trajectories:221 - Episode 1201\n",
      "2021-09-07 17:18:31.146 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:31.147 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:31.147 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:31.149 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:31.152 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.09456992894411087, 'baseline_loss': 0.5131973624229431, 'total_loss': 0.16202875971794128}\n",
      "2021-09-07 17:18:31.153 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30618909001350403\n",
      "2021-09-07 17:18:31.154 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8594181537628174\n",
      "2021-09-07 17:18:31.155 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30618909001350403\n",
      "2021-09-07 17:18:31.156 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:31.157 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:31.159 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.09454014152288437, 'baseline_loss': 0.6374197006225586, 'total_loss': 0.22416970133781433}\n",
      "2021-09-07 17:18:31.159 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17725349962711334\n",
      "2021-09-07 17:18:31.160 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.0733494758605957\n",
      "2021-09-07 17:18:31.161 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17725349962711334\n",
      "2021-09-07 17:18:31.163 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:31.164 | INFO     | src.policies:train:123 - Epoch 421 / 800\n",
      "2021-09-07 17:18:31.165 | INFO     | src.policies:collect_trajectories:221 - Episode 1202\n",
      "2021-09-07 17:18:31.203 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:31.203 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:31.204 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:31.205 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:31.207 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.21836133301258087, 'baseline_loss': 0.4391065835952759, 'total_loss': 0.0011919587850570679}\n",
      "2021-09-07 17:18:31.208 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17217451333999634\n",
      "2021-09-07 17:18:31.210 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4540706872940063\n",
      "2021-09-07 17:18:31.211 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17217451333999634\n",
      "2021-09-07 17:18:31.212 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:31.213 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:31.214 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.34923458099365234, 'baseline_loss': 0.48287540674209595, 'total_loss': -0.10779687762260437}\n",
      "2021-09-07 17:18:31.215 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22031402587890625\n",
      "2021-09-07 17:18:31.216 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9310792684555054\n",
      "2021-09-07 17:18:31.218 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22031402587890625\n",
      "2021-09-07 17:18:31.219 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:31.221 | INFO     | src.policies:train:123 - Epoch 422 / 800\n",
      "2021-09-07 17:18:31.222 | INFO     | src.policies:collect_trajectories:221 - Episode 1203\n",
      "2021-09-07 17:18:31.252 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:31.253 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:31.253 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:31.255 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:31.257 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20791178941726685, 'baseline_loss': 0.4903028905391693, 'total_loss': 0.03723965585231781}\n",
      "2021-09-07 17:18:31.258 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1810179203748703\n",
      "2021-09-07 17:18:31.259 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.523362398147583\n",
      "2021-09-07 17:18:31.260 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1810179203748703\n",
      "2021-09-07 17:18:31.261 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:31.262 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:31.263 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29179105162620544, 'baseline_loss': 0.543401300907135, 'total_loss': -0.02009040117263794}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:31.264 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.05726742744445801\n",
      "2021-09-07 17:18:31.265 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0902591943740845\n",
      "2021-09-07 17:18:31.266 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.05726742744445801\n",
      "2021-09-07 17:18:31.267 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:31.268 | INFO     | src.policies:train:123 - Epoch 423 / 800\n",
      "2021-09-07 17:18:31.269 | INFO     | src.policies:collect_trajectories:221 - Episode 1204\n",
      "2021-09-07 17:18:31.297 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:31.298 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:31.298 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:31.302 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:31.304 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1448352187871933, 'baseline_loss': 0.3986305296421051, 'total_loss': 0.05448004603385925}\n",
      "2021-09-07 17:18:31.305 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19104944169521332\n",
      "2021-09-07 17:18:31.306 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2000640630722046\n",
      "2021-09-07 17:18:31.307 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19104944169521332\n",
      "2021-09-07 17:18:31.309 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:31.310 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:31.311 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2663325071334839, 'baseline_loss': 0.3716696798801422, 'total_loss': -0.08049766719341278}\n",
      "2021-09-07 17:18:31.312 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1228475272655487\n",
      "2021-09-07 17:18:31.313 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0651546716690063\n",
      "2021-09-07 17:18:31.314 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1228475272655487\n",
      "2021-09-07 17:18:31.315 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:31.317 | INFO     | src.policies:train:123 - Epoch 424 / 800\n",
      "2021-09-07 17:18:31.317 | INFO     | src.policies:collect_trajectories:221 - Episode 1205\n",
      "2021-09-07 17:18:31.337 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:31.338 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 137.0\n",
      "2021-09-07 17:18:31.338 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 137.0\n",
      "2021-09-07 17:18:31.339 | INFO     | src.policies:collect_trajectories:221 - Episode 1206\n",
      "2021-09-07 17:18:31.371 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:31.371 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:31.372 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 168.5\n",
      "2021-09-07 17:18:31.372 | WARNING  | src.policies:train:144 - The actual batch size is 337, instead of 200\n",
      "2021-09-07 17:18:31.375 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:31.376 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24176424741744995, 'baseline_loss': 0.3917214274406433, 'total_loss': -0.045903533697128296}\n",
      "2021-09-07 17:18:31.377 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12777647376060486\n",
      "2021-09-07 17:18:31.378 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0612245798110962\n",
      "2021-09-07 17:18:31.379 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12777647376060486\n",
      "2021-09-07 17:18:31.380 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:31.381 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:31.382 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36562874913215637, 'baseline_loss': 0.48083388805389404, 'total_loss': -0.12521180510520935}\n",
      "2021-09-07 17:18:31.383 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3081474304199219\n",
      "2021-09-07 17:18:31.384 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5869298577308655\n",
      "2021-09-07 17:18:31.385 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3081474304199219\n",
      "2021-09-07 17:18:31.386 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:31.387 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:31.388 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.288031667470932, 'baseline_loss': 0.4496789574623108, 'total_loss': -0.06319218873977661}\n",
      "2021-09-07 17:18:31.389 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12744636833667755\n",
      "2021-09-07 17:18:31.390 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5531489253044128\n",
      "2021-09-07 17:18:31.391 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12744636833667755\n",
      "2021-09-07 17:18:31.392 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:31.394 | INFO     | src.policies:train:123 - Epoch 425 / 800\n",
      "2021-09-07 17:18:31.394 | INFO     | src.policies:collect_trajectories:221 - Episode 1207\n",
      "2021-09-07 17:18:31.426 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:31.427 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:31.427 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:31.429 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:31.431 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.33983638882637024, 'baseline_loss': 0.5962673425674438, 'total_loss': -0.041702717542648315}\n",
      "2021-09-07 17:18:31.432 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1950451135635376\n",
      "2021-09-07 17:18:31.433 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4732896089553833\n",
      "2021-09-07 17:18:31.434 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1950451135635376\n",
      "2021-09-07 17:18:31.435 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4732896089553833\n",
      "2021-09-07 17:18:31.436 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:31.437 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3601036071777344, 'baseline_loss': 0.5959961414337158, 'total_loss': -0.062105536460876465}\n",
      "2021-09-07 17:18:31.438 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30412375926971436\n",
      "2021-09-07 17:18:31.439 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5329413414001465\n",
      "2021-09-07 17:18:31.441 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30412375926971436\n",
      "2021-09-07 17:18:31.442 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999989867210388\n",
      "2021-09-07 17:18:31.444 | INFO     | src.policies:train:123 - Epoch 426 / 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:31.445 | INFO     | src.policies:collect_trajectories:221 - Episode 1208\n",
      "2021-09-07 17:18:31.465 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:31.466 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 106.0\n",
      "2021-09-07 17:18:31.466 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 106.0\n",
      "2021-09-07 17:18:31.467 | INFO     | src.policies:collect_trajectories:221 - Episode 1209\n",
      "2021-09-07 17:18:31.497 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:31.498 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:31.498 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 153.0\n",
      "2021-09-07 17:18:31.499 | WARNING  | src.policies:train:144 - The actual batch size is 306, instead of 200\n",
      "2021-09-07 17:18:31.501 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:31.504 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.38766592741012573, 'baseline_loss': 1.1007559299468994, 'total_loss': 0.16271203756332397}\n",
      "2021-09-07 17:18:31.505 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20004932582378387\n",
      "2021-09-07 17:18:31.505 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0384567975997925\n",
      "2021-09-07 17:18:31.506 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20004932582378387\n",
      "2021-09-07 17:18:31.507 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:31.508 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:31.510 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.34129688143730164, 'baseline_loss': 0.7621655464172363, 'total_loss': 0.03978589177131653}\n",
      "2021-09-07 17:18:31.510 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1822679042816162\n",
      "2021-09-07 17:18:31.511 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5492693781852722\n",
      "2021-09-07 17:18:31.512 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1822679042816162\n",
      "2021-09-07 17:18:31.513 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:31.514 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:31.516 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.33038330078125, 'baseline_loss': 0.9709630012512207, 'total_loss': 0.15509819984436035}\n",
      "2021-09-07 17:18:31.516 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14522667229175568\n",
      "2021-09-07 17:18:31.517 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9283561706542969\n",
      "2021-09-07 17:18:31.518 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14522667229175568\n",
      "2021-09-07 17:18:31.519 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:31.520 | INFO     | src.policies:train:123 - Epoch 427 / 800\n",
      "2021-09-07 17:18:31.521 | INFO     | src.policies:collect_trajectories:221 - Episode 1210\n",
      "2021-09-07 17:18:31.551 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:31.552 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:31.552 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:31.555 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:31.556 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4907304644584656, 'baseline_loss': 0.8363122344017029, 'total_loss': -0.07257434725761414}\n",
      "2021-09-07 17:18:31.557 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1247103214263916\n",
      "2021-09-07 17:18:31.558 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7164620757102966\n",
      "2021-09-07 17:18:31.559 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1247103214263916\n",
      "2021-09-07 17:18:31.560 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:31.562 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:31.563 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3987826108932495, 'baseline_loss': 0.6674268245697021, 'total_loss': -0.06506919860839844}\n",
      "2021-09-07 17:18:31.564 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11008753627538681\n",
      "2021-09-07 17:18:31.564 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6762731671333313\n",
      "2021-09-07 17:18:31.565 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11008753627538681\n",
      "2021-09-07 17:18:31.566 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:31.568 | INFO     | src.policies:train:123 - Epoch 428 / 800\n",
      "2021-09-07 17:18:31.568 | INFO     | src.policies:collect_trajectories:221 - Episode 1211\n",
      "2021-09-07 17:18:31.599 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:31.599 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:31.600 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:31.602 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:31.604 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2596839666366577, 'baseline_loss': 0.4267991781234741, 'total_loss': -0.046284377574920654}\n",
      "2021-09-07 17:18:31.605 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08050158619880676\n",
      "2021-09-07 17:18:31.606 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.957565426826477\n",
      "2021-09-07 17:18:31.608 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08050158619880676\n",
      "2021-09-07 17:18:31.608 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:31.610 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:31.611 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.19158263504505157, 'baseline_loss': 0.41517630219459534, 'total_loss': 0.016005516052246094}\n",
      "2021-09-07 17:18:31.612 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.147660031914711\n",
      "2021-09-07 17:18:31.612 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3041651248931885\n",
      "2021-09-07 17:18:31.613 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.147660031914711\n",
      "2021-09-07 17:18:31.614 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:31.616 | INFO     | src.policies:train:123 - Epoch 429 / 800\n",
      "2021-09-07 17:18:31.616 | INFO     | src.policies:collect_trajectories:221 - Episode 1212\n",
      "2021-09-07 17:18:31.703 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:31.704 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:31.704 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:31.706 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:31.708 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5317820906639099, 'baseline_loss': 1.558742880821228, 'total_loss': 0.2475893497467041}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:31.709 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.39560645818710327\n",
      "2021-09-07 17:18:31.710 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.8080506324768066\n",
      "2021-09-07 17:18:31.712 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.39560645818710327\n",
      "2021-09-07 17:18:31.713 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:31.714 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:31.715 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7150508761405945, 'baseline_loss': 1.6249600648880005, 'total_loss': 0.09742915630340576}\n",
      "2021-09-07 17:18:31.716 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.471818745136261\n",
      "2021-09-07 17:18:31.717 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.5036919116973877\n",
      "2021-09-07 17:18:31.718 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.471818745136261\n",
      "2021-09-07 17:18:31.719 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:31.720 | INFO     | src.policies:train:123 - Epoch 430 / 800\n",
      "2021-09-07 17:18:31.721 | INFO     | src.policies:collect_trajectories:221 - Episode 1213\n",
      "2021-09-07 17:18:31.742 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:31.743 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 152.0\n",
      "2021-09-07 17:18:31.743 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 152.0\n",
      "2021-09-07 17:18:31.744 | INFO     | src.policies:collect_trajectories:221 - Episode 1214\n",
      "2021-09-07 17:18:31.781 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:31.782 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:31.782 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 176.0\n",
      "2021-09-07 17:18:31.783 | WARNING  | src.policies:train:144 - The actual batch size is 352, instead of 200\n",
      "2021-09-07 17:18:31.785 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:31.787 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2840562164783478, 'baseline_loss': 0.8028223514556885, 'total_loss': 0.11735495924949646}\n",
      "2021-09-07 17:18:31.788 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12802042067050934\n",
      "2021-09-07 17:18:31.789 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6763970851898193\n",
      "2021-09-07 17:18:31.790 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12802042067050934\n",
      "2021-09-07 17:18:31.791 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:31.792 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:31.793 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23519432544708252, 'baseline_loss': 0.5577915906906128, 'total_loss': 0.04370146989822388}\n",
      "2021-09-07 17:18:31.794 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12843526899814606\n",
      "2021-09-07 17:18:31.795 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9912152290344238\n",
      "2021-09-07 17:18:31.796 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12843526899814606\n",
      "2021-09-07 17:18:31.797 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:31.798 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:31.800 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40636664628982544, 'baseline_loss': 1.0960378646850586, 'total_loss': 0.14165228605270386}\n",
      "2021-09-07 17:18:31.801 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.36849090456962585\n",
      "2021-09-07 17:18:31.802 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.927788496017456\n",
      "2021-09-07 17:18:31.803 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.36849090456962585\n",
      "2021-09-07 17:18:31.804 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:31.806 | INFO     | src.policies:train:123 - Epoch 431 / 800\n",
      "2021-09-07 17:18:31.806 | INFO     | src.policies:collect_trajectories:221 - Episode 1215\n",
      "2021-09-07 17:18:31.836 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:31.838 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:31.838 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:31.842 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:31.844 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20311476290225983, 'baseline_loss': 0.38163870573043823, 'total_loss': -0.01229541003704071}\n",
      "2021-09-07 17:18:31.845 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07159806787967682\n",
      "2021-09-07 17:18:31.846 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.392574429512024\n",
      "2021-09-07 17:18:31.847 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07159806787967682\n",
      "2021-09-07 17:18:31.848 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:31.849 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:31.850 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23351021111011505, 'baseline_loss': 0.3807488679885864, 'total_loss': -0.04313577711582184}\n",
      "2021-09-07 17:18:31.851 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2004466950893402\n",
      "2021-09-07 17:18:31.853 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0258203744888306\n",
      "2021-09-07 17:18:31.854 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2004466950893402\n",
      "2021-09-07 17:18:31.855 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:31.857 | INFO     | src.policies:train:123 - Epoch 432 / 800\n",
      "2021-09-07 17:18:31.857 | INFO     | src.policies:collect_trajectories:221 - Episode 1216\n",
      "2021-09-07 17:18:31.886 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:31.886 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:31.887 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:31.888 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:31.891 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.16423708200454712, 'baseline_loss': 0.4197094738483429, 'total_loss': 0.04561765491962433}\n",
      "2021-09-07 17:18:31.892 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08078154921531677\n",
      "2021-09-07 17:18:31.893 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.095483422279358\n",
      "2021-09-07 17:18:31.895 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08078154921531677\n",
      "2021-09-07 17:18:31.896 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:31.897 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:31.898 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23047740757465363, 'baseline_loss': 0.3649948835372925, 'total_loss': -0.047979965806007385}\n",
      "2021-09-07 17:18:31.899 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10850037634372711\n",
      "2021-09-07 17:18:31.900 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6939641833305359\n",
      "2021-09-07 17:18:31.901 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10850037634372711\n",
      "2021-09-07 17:18:31.902 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:31.903 | INFO     | src.policies:train:123 - Epoch 433 / 800\n",
      "2021-09-07 17:18:31.904 | INFO     | src.policies:collect_trajectories:221 - Episode 1217\n",
      "2021-09-07 17:18:31.935 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:31.936 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:31.936 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:31.938 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:31.940 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.26312950253486633, 'baseline_loss': 0.3887759745121002, 'total_loss': -0.06874151527881622}\n",
      "2021-09-07 17:18:31.941 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5070198774337769\n",
      "2021-09-07 17:18:31.943 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.46998295187950134\n",
      "2021-09-07 17:18:31.945 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999895691871643\n",
      "2021-09-07 17:18:31.946 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.46998295187950134\n",
      "2021-09-07 17:18:31.947 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:31.949 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32725635170936584, 'baseline_loss': 0.43683844804763794, 'total_loss': -0.10883712768554688}\n",
      "2021-09-07 17:18:31.950 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14758354425430298\n",
      "2021-09-07 17:18:31.951 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.561302900314331\n",
      "2021-09-07 17:18:31.951 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14758354425430298\n",
      "2021-09-07 17:18:31.953 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:31.954 | INFO     | src.policies:train:123 - Epoch 434 / 800\n",
      "2021-09-07 17:18:31.954 | INFO     | src.policies:collect_trajectories:221 - Episode 1218\n",
      "2021-09-07 17:18:31.983 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:31.984 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:31.984 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:31.986 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:31.988 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.33944907784461975, 'baseline_loss': 0.4624897837638855, 'total_loss': -0.108204185962677}\n",
      "2021-09-07 17:18:31.989 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1135287880897522\n",
      "2021-09-07 17:18:31.990 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.16780616343021393\n",
      "2021-09-07 17:18:31.992 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1135287880897522\n",
      "2021-09-07 17:18:31.993 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.16780616343021393\n",
      "2021-09-07 17:18:31.994 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:31.995 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2916291356086731, 'baseline_loss': 0.4308278262615204, 'total_loss': -0.0762152224779129}\n",
      "2021-09-07 17:18:31.996 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10201071202754974\n",
      "2021-09-07 17:18:31.997 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4675779640674591\n",
      "2021-09-07 17:18:31.998 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10201071202754974\n",
      "2021-09-07 17:18:31.999 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4675779640674591\n",
      "2021-09-07 17:18:32.000 | INFO     | src.policies:train:123 - Epoch 435 / 800\n",
      "2021-09-07 17:18:32.001 | INFO     | src.policies:collect_trajectories:221 - Episode 1219\n",
      "2021-09-07 17:18:32.022 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.023 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 140.0\n",
      "2021-09-07 17:18:32.023 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 140.0\n",
      "2021-09-07 17:18:32.023 | INFO     | src.policies:collect_trajectories:221 - Episode 1220\n",
      "2021-09-07 17:18:32.055 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.056 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:32.056 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 170.0\n",
      "2021-09-07 17:18:32.057 | WARNING  | src.policies:train:144 - The actual batch size is 340, instead of 200\n",
      "2021-09-07 17:18:32.060 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:32.062 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24518196284770966, 'baseline_loss': 0.38229820132255554, 'total_loss': -0.054032862186431885}\n",
      "2021-09-07 17:18:32.062 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17583371698856354\n",
      "2021-09-07 17:18:32.063 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5739423632621765\n",
      "2021-09-07 17:18:32.065 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17583371698856354\n",
      "2021-09-07 17:18:32.066 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:32.067 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:32.068 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40531066060066223, 'baseline_loss': 0.49708881974220276, 'total_loss': -0.15676625072956085}\n",
      "2021-09-07 17:18:32.069 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31568092107772827\n",
      "2021-09-07 17:18:32.070 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5562083125114441\n",
      "2021-09-07 17:18:32.072 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31568092107772827\n",
      "2021-09-07 17:18:32.073 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:32.074 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:32.076 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3216420114040375, 'baseline_loss': 0.3981342315673828, 'total_loss': -0.12257489562034607}\n",
      "2021-09-07 17:18:32.077 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2680131196975708\n",
      "2021-09-07 17:18:32.078 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2829112112522125\n",
      "2021-09-07 17:18:32.079 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2680131196975708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:32.080 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2829112112522125\n",
      "2021-09-07 17:18:32.082 | INFO     | src.policies:train:123 - Epoch 436 / 800\n",
      "2021-09-07 17:18:32.082 | INFO     | src.policies:collect_trajectories:221 - Episode 1221\n",
      "2021-09-07 17:18:32.109 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.109 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 156.0\n",
      "2021-09-07 17:18:32.110 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 156.0\n",
      "2021-09-07 17:18:32.110 | INFO     | src.policies:collect_trajectories:221 - Episode 1222\n",
      "2021-09-07 17:18:32.130 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.130 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 128.0\n",
      "2021-09-07 17:18:32.131 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 142.0\n",
      "2021-09-07 17:18:32.131 | WARNING  | src.policies:train:144 - The actual batch size is 284, instead of 200\n",
      "2021-09-07 17:18:32.134 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:32.136 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2919124364852905, 'baseline_loss': 0.3987574279308319, 'total_loss': -0.09253372251987457}\n",
      "2021-09-07 17:18:32.137 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16750457882881165\n",
      "2021-09-07 17:18:32.138 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7623881101608276\n",
      "2021-09-07 17:18:32.139 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16750457882881165\n",
      "2021-09-07 17:18:32.140 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:32.141 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:32.142 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4070481061935425, 'baseline_loss': 0.5034698247909546, 'total_loss': -0.15531319379806519}\n",
      "2021-09-07 17:18:32.143 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5863902568817139\n",
      "2021-09-07 17:18:32.144 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5681778788566589\n",
      "2021-09-07 17:18:32.145 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:32.147 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:32.148 | INFO     | src.policies:train:123 - Epoch 437 / 800\n",
      "2021-09-07 17:18:32.148 | INFO     | src.policies:collect_trajectories:221 - Episode 1223\n",
      "2021-09-07 17:18:32.171 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.172 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 154.0\n",
      "2021-09-07 17:18:32.172 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 154.0\n",
      "2021-09-07 17:18:32.172 | INFO     | src.policies:collect_trajectories:221 - Episode 1224\n",
      "2021-09-07 17:18:32.255 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.256 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:32.256 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 177.0\n",
      "2021-09-07 17:18:32.257 | WARNING  | src.policies:train:144 - The actual batch size is 354, instead of 200\n",
      "2021-09-07 17:18:32.260 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:32.262 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.37209227681159973, 'baseline_loss': 0.5871831774711609, 'total_loss': -0.07850068807601929}\n",
      "2021-09-07 17:18:32.263 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3000204563140869\n",
      "2021-09-07 17:18:32.264 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.44200462102890015\n",
      "2021-09-07 17:18:32.265 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3000204563140869\n",
      "2021-09-07 17:18:32.266 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.44200462102890015\n",
      "2021-09-07 17:18:32.267 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:32.268 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4249904155731201, 'baseline_loss': 0.9693860411643982, 'total_loss': 0.05970260500907898}\n",
      "2021-09-07 17:18:32.269 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23974865674972534\n",
      "2021-09-07 17:18:32.270 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2764604091644287\n",
      "2021-09-07 17:18:32.271 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23974865674972534\n",
      "2021-09-07 17:18:32.272 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:32.273 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:32.274 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5683714151382446, 'baseline_loss': 0.852571427822113, 'total_loss': -0.1420857012271881}\n",
      "2021-09-07 17:18:32.275 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18592749536037445\n",
      "2021-09-07 17:18:32.276 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6817528009414673\n",
      "2021-09-07 17:18:32.277 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18592749536037445\n",
      "2021-09-07 17:18:32.278 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:32.279 | INFO     | src.policies:train:123 - Epoch 438 / 800\n",
      "2021-09-07 17:18:32.280 | INFO     | src.policies:collect_trajectories:221 - Episode 1225\n",
      "2021-09-07 17:18:32.299 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.299 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 129.0\n",
      "2021-09-07 17:18:32.300 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 129.0\n",
      "2021-09-07 17:18:32.300 | INFO     | src.policies:collect_trajectories:221 - Episode 1226\n",
      "2021-09-07 17:18:32.334 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.335 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:32.335 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 164.5\n",
      "2021-09-07 17:18:32.336 | WARNING  | src.policies:train:144 - The actual batch size is 329, instead of 200\n",
      "2021-09-07 17:18:32.339 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:32.342 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5061497092247009, 'baseline_loss': 0.9658998250961304, 'total_loss': -0.023199796676635742}\n",
      "2021-09-07 17:18:32.343 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.40111374855041504\n",
      "2021-09-07 17:18:32.344 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5174031257629395\n",
      "2021-09-07 17:18:32.345 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.40111374855041504\n",
      "2021-09-07 17:18:32.346 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:32.348 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:32.349 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5939529538154602, 'baseline_loss': 1.021430492401123, 'total_loss': -0.08323770761489868}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:32.350 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2665089964866638\n",
      "2021-09-07 17:18:32.350 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9203476905822754\n",
      "2021-09-07 17:18:32.351 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2665089964866638\n",
      "2021-09-07 17:18:32.353 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:32.354 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:32.355 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.49069443345069885, 'baseline_loss': 0.9107549786567688, 'total_loss': -0.03531694412231445}\n",
      "2021-09-07 17:18:32.356 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.27168649435043335\n",
      "2021-09-07 17:18:32.357 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0931602716445923\n",
      "2021-09-07 17:18:32.358 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.27168649435043335\n",
      "2021-09-07 17:18:32.358 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:32.360 | INFO     | src.policies:train:123 - Epoch 439 / 800\n",
      "2021-09-07 17:18:32.360 | INFO     | src.policies:collect_trajectories:221 - Episode 1227\n",
      "2021-09-07 17:18:32.376 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.377 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 110.0\n",
      "2021-09-07 17:18:32.377 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 110.0\n",
      "2021-09-07 17:18:32.378 | INFO     | src.policies:collect_trajectories:221 - Episode 1228\n",
      "2021-09-07 17:18:32.408 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.408 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:32.409 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 155.0\n",
      "2021-09-07 17:18:32.410 | WARNING  | src.policies:train:144 - The actual batch size is 310, instead of 200\n",
      "2021-09-07 17:18:32.412 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:32.414 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2905319333076477, 'baseline_loss': 0.3988863229751587, 'total_loss': -0.09108877182006836}\n",
      "2021-09-07 17:18:32.415 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32829514145851135\n",
      "2021-09-07 17:18:32.416 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7337786555290222\n",
      "2021-09-07 17:18:32.417 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32829514145851135\n",
      "2021-09-07 17:18:32.418 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:32.419 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:32.420 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2761625349521637, 'baseline_loss': 0.48879119753837585, 'total_loss': -0.03176693618297577}\n",
      "2021-09-07 17:18:32.422 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2068815976381302\n",
      "2021-09-07 17:18:32.423 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.199931338429451\n",
      "2021-09-07 17:18:32.425 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2068815976381302\n",
      "2021-09-07 17:18:32.426 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.199931338429451\n",
      "2021-09-07 17:18:32.428 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:32.430 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41085362434387207, 'baseline_loss': 0.5373494029045105, 'total_loss': -0.14217892289161682}\n",
      "2021-09-07 17:18:32.431 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.26243945956230164\n",
      "2021-09-07 17:18:32.432 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3846827745437622\n",
      "2021-09-07 17:18:32.433 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.26243945956230164\n",
      "2021-09-07 17:18:32.434 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3846827745437622\n",
      "2021-09-07 17:18:32.436 | INFO     | src.policies:train:123 - Epoch 440 / 800\n",
      "2021-09-07 17:18:32.436 | INFO     | src.policies:collect_trajectories:221 - Episode 1229\n",
      "2021-09-07 17:18:32.459 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.460 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 159.0\n",
      "2021-09-07 17:18:32.460 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 159.0\n",
      "2021-09-07 17:18:32.461 | INFO     | src.policies:collect_trajectories:221 - Episode 1230\n",
      "2021-09-07 17:18:32.490 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.491 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 186.0\n",
      "2021-09-07 17:18:32.491 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 172.5\n",
      "2021-09-07 17:18:32.492 | WARNING  | src.policies:train:144 - The actual batch size is 345, instead of 200\n",
      "2021-09-07 17:18:32.495 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:32.497 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36489158868789673, 'baseline_loss': 0.6493480801582336, 'total_loss': -0.04021754860877991}\n",
      "2021-09-07 17:18:32.497 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.36818182468414307\n",
      "2021-09-07 17:18:32.498 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6586775183677673\n",
      "2021-09-07 17:18:32.499 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.36818182468414307\n",
      "2021-09-07 17:18:32.501 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:32.502 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:32.503 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.44683772325515747, 'baseline_loss': 0.6906735301017761, 'total_loss': -0.10150095820426941}\n",
      "2021-09-07 17:18:32.504 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.36612218618392944\n",
      "2021-09-07 17:18:32.505 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3943571448326111\n",
      "2021-09-07 17:18:32.506 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.36612218618392944\n",
      "2021-09-07 17:18:32.507 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3943571448326111\n",
      "2021-09-07 17:18:32.508 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:32.509 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36043453216552734, 'baseline_loss': 0.5346698760986328, 'total_loss': -0.09309959411621094}\n",
      "2021-09-07 17:18:32.510 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17648468911647797\n",
      "2021-09-07 17:18:32.511 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.11393372714519501\n",
      "2021-09-07 17:18:32.512 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17648468911647797\n",
      "2021-09-07 17:18:32.513 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.11393372714519501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:32.515 | INFO     | src.policies:train:123 - Epoch 441 / 800\n",
      "2021-09-07 17:18:32.515 | INFO     | src.policies:collect_trajectories:221 - Episode 1231\n",
      "2021-09-07 17:18:32.542 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.542 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 179.0\n",
      "2021-09-07 17:18:32.543 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 179.0\n",
      "2021-09-07 17:18:32.543 | INFO     | src.policies:collect_trajectories:221 - Episode 1232\n",
      "2021-09-07 17:18:32.565 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.565 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 127.0\n",
      "2021-09-07 17:18:32.566 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 153.0\n",
      "2021-09-07 17:18:32.566 | WARNING  | src.policies:train:144 - The actual batch size is 306, instead of 200\n",
      "2021-09-07 17:18:32.569 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:32.572 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3706475794315338, 'baseline_loss': 0.5610593557357788, 'total_loss': -0.09011790156364441}\n",
      "2021-09-07 17:18:32.573 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15303653478622437\n",
      "2021-09-07 17:18:32.574 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2499997317790985\n",
      "2021-09-07 17:18:32.575 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15303653478622437\n",
      "2021-09-07 17:18:32.576 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2499997317790985\n",
      "2021-09-07 17:18:32.577 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:32.578 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.37256693840026855, 'baseline_loss': 0.5713211894035339, 'total_loss': -0.08690634369850159}\n",
      "2021-09-07 17:18:32.579 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.27577680349349976\n",
      "2021-09-07 17:18:32.580 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.18169142305850983\n",
      "2021-09-07 17:18:32.581 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.27577680349349976\n",
      "2021-09-07 17:18:32.582 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.18169142305850983\n",
      "2021-09-07 17:18:32.583 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:32.584 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.35868674516677856, 'baseline_loss': 0.45134133100509644, 'total_loss': -0.13301607966423035}\n",
      "2021-09-07 17:18:32.585 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.43644776940345764\n",
      "2021-09-07 17:18:32.586 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5694679617881775\n",
      "2021-09-07 17:18:32.587 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.43644776940345764\n",
      "2021-09-07 17:18:32.588 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:32.589 | INFO     | src.policies:train:123 - Epoch 442 / 800\n",
      "2021-09-07 17:18:32.590 | INFO     | src.policies:collect_trajectories:221 - Episode 1233\n",
      "2021-09-07 17:18:32.613 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.614 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 167.0\n",
      "2021-09-07 17:18:32.614 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 167.0\n",
      "2021-09-07 17:18:32.615 | INFO     | src.policies:collect_trajectories:221 - Episode 1234\n",
      "2021-09-07 17:18:32.646 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.646 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:32.647 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 183.5\n",
      "2021-09-07 17:18:32.648 | WARNING  | src.policies:train:144 - The actual batch size is 367, instead of 200\n",
      "2021-09-07 17:18:32.651 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:32.653 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2563284635543823, 'baseline_loss': 0.42229336500167847, 'total_loss': -0.04518178105354309}\n",
      "2021-09-07 17:18:32.654 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3568878769874573\n",
      "2021-09-07 17:18:32.655 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.27093827724456787\n",
      "2021-09-07 17:18:32.657 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3568878769874573\n",
      "2021-09-07 17:18:32.658 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.27093827724456787\n",
      "2021-09-07 17:18:32.659 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:32.660 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24248497188091278, 'baseline_loss': 0.34843817353248596, 'total_loss': -0.0682658851146698}\n",
      "2021-09-07 17:18:32.661 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1340971291065216\n",
      "2021-09-07 17:18:32.662 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9582089781761169\n",
      "2021-09-07 17:18:32.663 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1340971291065216\n",
      "2021-09-07 17:18:32.664 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:32.665 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:32.666 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41784146428108215, 'baseline_loss': 0.6136054992675781, 'total_loss': -0.11103871464729309}\n",
      "2021-09-07 17:18:32.667 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4512626826763153\n",
      "2021-09-07 17:18:32.668 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.22887283563613892\n",
      "2021-09-07 17:18:32.669 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4512626826763153\n",
      "2021-09-07 17:18:32.670 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.22887283563613892\n",
      "2021-09-07 17:18:32.671 | INFO     | src.policies:train:123 - Epoch 443 / 800\n",
      "2021-09-07 17:18:32.672 | INFO     | src.policies:collect_trajectories:221 - Episode 1235\n",
      "2021-09-07 17:18:32.697 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.698 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 180.0\n",
      "2021-09-07 17:18:32.698 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 180.0\n",
      "2021-09-07 17:18:32.698 | INFO     | src.policies:collect_trajectories:221 - Episode 1236\n",
      "2021-09-07 17:18:32.730 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.731 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:32.731 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 190.0\n",
      "2021-09-07 17:18:32.732 | WARNING  | src.policies:train:144 - The actual batch size is 380, instead of 200\n",
      "2021-09-07 17:18:32.735 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:32.737 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4098128378391266, 'baseline_loss': 0.5254214406013489, 'total_loss': -0.14710211753845215}\n",
      "2021-09-07 17:18:32.738 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12486843019723892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:32.739 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.28209519386291504\n",
      "2021-09-07 17:18:32.740 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12486843019723892\n",
      "2021-09-07 17:18:32.796 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.28209519386291504\n",
      "2021-09-07 17:18:32.798 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:32.800 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.38458192348480225, 'baseline_loss': 0.6489148139953613, 'total_loss': -0.06012451648712158}\n",
      "2021-09-07 17:18:32.801 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2208016812801361\n",
      "2021-09-07 17:18:32.802 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5125776529312134\n",
      "2021-09-07 17:18:32.803 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2208016812801361\n",
      "2021-09-07 17:18:32.805 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:32.807 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:32.808 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4247622787952423, 'baseline_loss': 0.8006831407546997, 'total_loss': -0.024420708417892456}\n",
      "2021-09-07 17:18:32.809 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3854686915874481\n",
      "2021-09-07 17:18:32.810 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7388226389884949\n",
      "2021-09-07 17:18:32.811 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3854686915874481\n",
      "2021-09-07 17:18:32.812 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:32.813 | INFO     | src.policies:train:123 - Epoch 444 / 800\n",
      "2021-09-07 17:18:32.814 | INFO     | src.policies:collect_trajectories:221 - Episode 1237\n",
      "2021-09-07 17:18:32.955 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.956 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 196.0\n",
      "2021-09-07 17:18:32.956 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 196.0\n",
      "2021-09-07 17:18:32.957 | INFO     | src.policies:collect_trajectories:221 - Episode 1238\n",
      "2021-09-07 17:18:32.982 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:32.983 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 159.0\n",
      "2021-09-07 17:18:32.983 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 177.5\n",
      "2021-09-07 17:18:32.984 | WARNING  | src.policies:train:144 - The actual batch size is 355, instead of 200\n",
      "2021-09-07 17:18:32.987 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:32.989 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24634088575839996, 'baseline_loss': 0.35115692019462585, 'total_loss': -0.07076242566108704}\n",
      "2021-09-07 17:18:32.990 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.395445853471756\n",
      "2021-09-07 17:18:32.991 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4014647305011749\n",
      "2021-09-07 17:18:32.993 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.395445853471756\n",
      "2021-09-07 17:18:32.994 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4014647305011749\n",
      "2021-09-07 17:18:32.995 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:32.996 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32797715067863464, 'baseline_loss': 0.4656936526298523, 'total_loss': -0.0951303243637085}\n",
      "2021-09-07 17:18:32.997 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24459116160869598\n",
      "2021-09-07 17:18:32.997 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5250006914138794\n",
      "2021-09-07 17:18:32.998 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24459116160869598\n",
      "2021-09-07 17:18:32.999 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:33.001 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:33.002 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3313749432563782, 'baseline_loss': 0.5281555652618408, 'total_loss': -0.06729716062545776}\n",
      "2021-09-07 17:18:33.003 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2865014672279358\n",
      "2021-09-07 17:18:33.004 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5080696940422058\n",
      "2021-09-07 17:18:33.005 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2865014672279358\n",
      "2021-09-07 17:18:33.006 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:33.008 | INFO     | src.policies:train:123 - Epoch 445 / 800\n",
      "2021-09-07 17:18:33.008 | INFO     | src.policies:collect_trajectories:221 - Episode 1239\n",
      "2021-09-07 17:18:33.036 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:33.037 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 192.0\n",
      "2021-09-07 17:18:33.037 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 192.0\n",
      "2021-09-07 17:18:33.038 | INFO     | src.policies:collect_trajectories:221 - Episode 1240\n",
      "2021-09-07 17:18:33.065 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:33.066 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 182.0\n",
      "2021-09-07 17:18:33.066 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 187.0\n",
      "2021-09-07 17:18:33.067 | WARNING  | src.policies:train:144 - The actual batch size is 374, instead of 200\n",
      "2021-09-07 17:18:33.070 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:33.072 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.39164867997169495, 'baseline_loss': 0.6339725255966187, 'total_loss': -0.07466241717338562}\n",
      "2021-09-07 17:18:33.073 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.201705202460289\n",
      "2021-09-07 17:18:33.074 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.33290237188339233\n",
      "2021-09-07 17:18:33.075 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.201705202460289\n",
      "2021-09-07 17:18:33.076 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.33290237188339233\n",
      "2021-09-07 17:18:33.078 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:33.079 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3890634775161743, 'baseline_loss': 0.6636129021644592, 'total_loss': -0.0572570264339447}\n",
      "2021-09-07 17:18:33.080 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.29968899488449097\n",
      "2021-09-07 17:18:33.081 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.18684014678001404\n",
      "2021-09-07 17:18:33.082 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.29968899488449097\n",
      "2021-09-07 17:18:33.083 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.18684014678001404\n",
      "2021-09-07 17:18:33.084 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:33.086 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4156407117843628, 'baseline_loss': 0.909547746181488, 'total_loss': 0.039133161306381226}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:33.086 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31028109788894653\n",
      "2021-09-07 17:18:33.088 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.209540605545044\n",
      "2021-09-07 17:18:33.089 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31028109788894653\n",
      "2021-09-07 17:18:33.089 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:33.091 | INFO     | src.policies:train:123 - Epoch 446 / 800\n",
      "2021-09-07 17:18:33.091 | INFO     | src.policies:collect_trajectories:221 - Episode 1241\n",
      "2021-09-07 17:18:33.117 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:33.118 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 186.0\n",
      "2021-09-07 17:18:33.118 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 186.0\n",
      "2021-09-07 17:18:33.118 | INFO     | src.policies:collect_trajectories:221 - Episode 1242\n",
      "2021-09-07 17:18:33.147 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:33.147 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 196.0\n",
      "2021-09-07 17:18:33.148 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 191.0\n",
      "2021-09-07 17:18:33.148 | WARNING  | src.policies:train:144 - The actual batch size is 382, instead of 200\n",
      "2021-09-07 17:18:33.151 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:33.153 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32471558451652527, 'baseline_loss': 0.5057438015937805, 'total_loss': -0.07184368371963501}\n",
      "2021-09-07 17:18:33.154 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15090587735176086\n",
      "2021-09-07 17:18:33.155 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3428058326244354\n",
      "2021-09-07 17:18:33.156 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15090587735176086\n",
      "2021-09-07 17:18:33.157 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3428058326244354\n",
      "2021-09-07 17:18:33.158 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:33.159 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.35192355513572693, 'baseline_loss': 0.49204957485198975, 'total_loss': -0.10589876770973206}\n",
      "2021-09-07 17:18:33.160 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.34695687890052795\n",
      "2021-09-07 17:18:33.161 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.239706888794899\n",
      "2021-09-07 17:18:33.162 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.34695687890052795\n",
      "2021-09-07 17:18:33.163 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.239706888794899\n",
      "2021-09-07 17:18:33.164 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:33.165 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3348936438560486, 'baseline_loss': 0.41923704743385315, 'total_loss': -0.125275120139122}\n",
      "2021-09-07 17:18:33.166 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13089795410633087\n",
      "2021-09-07 17:18:33.167 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2373753786087036\n",
      "2021-09-07 17:18:33.168 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13089795410633087\n",
      "2021-09-07 17:18:33.169 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2373753786087036\n",
      "2021-09-07 17:18:33.170 | INFO     | src.policies:train:123 - Epoch 447 / 800\n",
      "2021-09-07 17:18:33.170 | INFO     | src.policies:collect_trajectories:221 - Episode 1243\n",
      "2021-09-07 17:18:33.194 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:33.194 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 155.0\n",
      "2021-09-07 17:18:33.195 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 155.0\n",
      "2021-09-07 17:18:33.195 | INFO     | src.policies:collect_trajectories:221 - Episode 1244\n",
      "2021-09-07 17:18:33.227 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:33.227 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:33.228 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 177.5\n",
      "2021-09-07 17:18:33.229 | WARNING  | src.policies:train:144 - The actual batch size is 355, instead of 200\n",
      "2021-09-07 17:18:33.231 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:33.232 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3451114892959595, 'baseline_loss': 0.47796595096588135, 'total_loss': -0.1061285138130188}\n",
      "2021-09-07 17:18:33.233 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08857624232769012\n",
      "2021-09-07 17:18:33.234 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3085584342479706\n",
      "2021-09-07 17:18:33.235 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08857624232769012\n",
      "2021-09-07 17:18:33.236 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3085584342479706\n",
      "2021-09-07 17:18:33.237 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:33.239 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3314041793346405, 'baseline_loss': 0.5072358846664429, 'total_loss': -0.07778623700141907}\n",
      "2021-09-07 17:18:33.240 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20706747472286224\n",
      "2021-09-07 17:18:33.241 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.21479445695877075\n",
      "2021-09-07 17:18:33.242 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20706747472286224\n",
      "2021-09-07 17:18:33.243 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.21479445695877075\n",
      "2021-09-07 17:18:33.244 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:33.245 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4454120397567749, 'baseline_loss': 0.6613960862159729, 'total_loss': -0.11471399664878845}\n",
      "2021-09-07 17:18:33.246 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31139254570007324\n",
      "2021-09-07 17:18:33.247 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2345052808523178\n",
      "2021-09-07 17:18:33.248 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31139254570007324\n",
      "2021-09-07 17:18:33.249 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2345052808523178\n",
      "2021-09-07 17:18:33.250 | INFO     | src.policies:train:123 - Epoch 448 / 800\n",
      "2021-09-07 17:18:33.251 | INFO     | src.policies:collect_trajectories:221 - Episode 1245\n",
      "2021-09-07 17:18:33.278 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:33.279 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 189.0\n",
      "2021-09-07 17:18:33.279 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 189.0\n",
      "2021-09-07 17:18:33.279 | INFO     | src.policies:collect_trajectories:221 - Episode 1246\n",
      "2021-09-07 17:18:33.426 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:33.427 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 189.0\n",
      "2021-09-07 17:18:33.428 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 189.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:33.429 | WARNING  | src.policies:train:144 - The actual batch size is 378, instead of 200\n",
      "2021-09-07 17:18:33.431 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:33.434 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4364531636238098, 'baseline_loss': 0.5621360540390015, 'total_loss': -0.15538513660430908}\n",
      "2021-09-07 17:18:33.435 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2369960993528366\n",
      "2021-09-07 17:18:33.436 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.0914401039481163\n",
      "2021-09-07 17:18:33.437 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2369960993528366\n",
      "2021-09-07 17:18:33.438 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.0914401039481163\n",
      "2021-09-07 17:18:33.440 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:33.441 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3659766912460327, 'baseline_loss': 0.5405627489089966, 'total_loss': -0.09569531679153442}\n",
      "2021-09-07 17:18:33.442 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20055051147937775\n",
      "2021-09-07 17:18:33.443 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.14899125695228577\n",
      "2021-09-07 17:18:33.444 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20055051147937775\n",
      "2021-09-07 17:18:33.445 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.14899125695228577\n",
      "2021-09-07 17:18:33.447 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:33.448 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40717095136642456, 'baseline_loss': 0.5701227784156799, 'total_loss': -0.1221095621585846}\n",
      "2021-09-07 17:18:33.449 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5250970721244812\n",
      "2021-09-07 17:18:33.450 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.1426297277212143\n",
      "2021-09-07 17:18:33.451 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999895691871643\n",
      "2021-09-07 17:18:33.453 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.1426297277212143\n",
      "2021-09-07 17:18:33.454 | INFO     | src.policies:train:123 - Epoch 449 / 800\n",
      "2021-09-07 17:18:33.455 | INFO     | src.policies:collect_trajectories:221 - Episode 1247\n",
      "2021-09-07 17:18:33.484 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:33.485 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 177.0\n",
      "2021-09-07 17:18:33.485 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 177.0\n",
      "2021-09-07 17:18:33.486 | INFO     | src.policies:collect_trajectories:221 - Episode 1248\n",
      "2021-09-07 17:18:33.514 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:33.515 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 178.0\n",
      "2021-09-07 17:18:33.516 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 177.5\n",
      "2021-09-07 17:18:33.517 | WARNING  | src.policies:train:144 - The actual batch size is 355, instead of 200\n",
      "2021-09-07 17:18:33.519 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:33.522 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.733422040939331, 'baseline_loss': 1.4532687664031982, 'total_loss': -0.006787657737731934}\n",
      "2021-09-07 17:18:33.523 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.549893856048584\n",
      "2021-09-07 17:18:33.524 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8251140117645264\n",
      "2021-09-07 17:18:33.526 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:33.526 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:33.528 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:33.529 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5102952718734741, 'baseline_loss': 1.0521749258041382, 'total_loss': 0.01579219102859497}\n",
      "2021-09-07 17:18:33.530 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7078098654747009\n",
      "2021-09-07 17:18:33.530 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5166234970092773\n",
      "2021-09-07 17:18:33.531 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:33.532 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:33.534 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:33.535 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6265309453010559, 'baseline_loss': 1.2161160707473755, 'total_loss': -0.018472909927368164}\n",
      "2021-09-07 17:18:33.536 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.34000706672668457\n",
      "2021-09-07 17:18:33.536 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7238768339157104\n",
      "2021-09-07 17:18:33.537 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.34000706672668457\n",
      "2021-09-07 17:18:33.538 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:33.540 | INFO     | src.policies:train:123 - Epoch 450 / 800\n",
      "2021-09-07 17:18:33.540 | INFO     | src.policies:collect_trajectories:221 - Episode 1249\n",
      "2021-09-07 17:18:33.571 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:33.572 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:33.573 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:33.575 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:33.577 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5667499303817749, 'baseline_loss': 1.0746104717254639, 'total_loss': -0.02944469451904297}\n",
      "2021-09-07 17:18:33.578 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.137070432305336\n",
      "2021-09-07 17:18:33.579 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6934951543807983\n",
      "2021-09-07 17:18:33.580 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.137070432305336\n",
      "2021-09-07 17:18:33.581 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:33.582 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:33.583 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5515479445457458, 'baseline_loss': 1.1593207120895386, 'total_loss': 0.028112411499023438}\n",
      "2021-09-07 17:18:33.584 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2164129763841629\n",
      "2021-09-07 17:18:33.585 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9801799058914185\n",
      "2021-09-07 17:18:33.586 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2164129763841629\n",
      "2021-09-07 17:18:33.587 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:33.589 | INFO     | src.policies:train:123 - Epoch 451 / 800\n",
      "2021-09-07 17:18:33.589 | INFO     | src.policies:collect_trajectories:221 - Episode 1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:33.618 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:33.618 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:33.619 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:33.621 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:33.623 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6000048518180847, 'baseline_loss': 1.255016565322876, 'total_loss': 0.02750343084335327}\n",
      "2021-09-07 17:18:33.624 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4312174618244171\n",
      "2021-09-07 17:18:33.625 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.6483101844787598\n",
      "2021-09-07 17:18:33.626 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4312174618244171\n",
      "2021-09-07 17:18:33.627 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:33.628 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:33.630 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5701146125793457, 'baseline_loss': 1.110588550567627, 'total_loss': -0.014820337295532227}\n",
      "2021-09-07 17:18:33.631 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30177852511405945\n",
      "2021-09-07 17:18:33.631 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9729602336883545\n",
      "2021-09-07 17:18:33.632 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30177852511405945\n",
      "2021-09-07 17:18:33.633 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:33.635 | INFO     | src.policies:train:123 - Epoch 452 / 800\n",
      "2021-09-07 17:18:33.635 | INFO     | src.policies:collect_trajectories:221 - Episode 1251\n",
      "2021-09-07 17:18:33.663 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:33.663 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 186.0\n",
      "2021-09-07 17:18:33.664 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 186.0\n",
      "2021-09-07 17:18:33.664 | INFO     | src.policies:collect_trajectories:221 - Episode 1252\n",
      "2021-09-07 17:18:33.698 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:33.698 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:33.699 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 193.0\n",
      "2021-09-07 17:18:33.700 | WARNING  | src.policies:train:144 - The actual batch size is 386, instead of 200\n",
      "2021-09-07 17:18:33.702 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:33.704 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3228054940700531, 'baseline_loss': 0.5250568985939026, 'total_loss': -0.06027704477310181}\n",
      "2021-09-07 17:18:33.705 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3890496790409088\n",
      "2021-09-07 17:18:33.706 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3223559856414795\n",
      "2021-09-07 17:18:33.707 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3890496790409088\n",
      "2021-09-07 17:18:33.708 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3223559856414795\n",
      "2021-09-07 17:18:33.709 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:33.711 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4149256646633148, 'baseline_loss': 0.48446667194366455, 'total_loss': -0.17269232869148254}\n",
      "2021-09-07 17:18:33.712 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.27718913555145264\n",
      "2021-09-07 17:18:33.712 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3484754264354706\n",
      "2021-09-07 17:18:33.714 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.27718913555145264\n",
      "2021-09-07 17:18:33.715 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3484754264354706\n",
      "2021-09-07 17:18:33.716 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:33.717 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.441016286611557, 'baseline_loss': 0.7280178666114807, 'total_loss': -0.07700735330581665}\n",
      "2021-09-07 17:18:33.718 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15582780539989471\n",
      "2021-09-07 17:18:33.719 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.47242283821105957\n",
      "2021-09-07 17:18:33.720 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15582780539989471\n",
      "2021-09-07 17:18:33.721 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.47242283821105957\n",
      "2021-09-07 17:18:33.722 | INFO     | src.policies:train:123 - Epoch 453 / 800\n",
      "2021-09-07 17:18:33.723 | INFO     | src.policies:collect_trajectories:221 - Episode 1253\n",
      "2021-09-07 17:18:33.752 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:33.753 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:33.753 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:33.755 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:33.757 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5122560262680054, 'baseline_loss': 1.4717063903808594, 'total_loss': 0.22359716892242432}\n",
      "2021-09-07 17:18:33.758 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3896421194076538\n",
      "2021-09-07 17:18:33.759 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.477572441101074\n",
      "2021-09-07 17:18:33.760 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3896421194076538\n",
      "2021-09-07 17:18:33.761 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:33.762 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:33.764 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5127652883529663, 'baseline_loss': 1.1515158414840698, 'total_loss': 0.0629926323890686}\n",
      "2021-09-07 17:18:33.764 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25056183338165283\n",
      "2021-09-07 17:18:33.765 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.78178870677948\n",
      "2021-09-07 17:18:33.766 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25056183338165283\n",
      "2021-09-07 17:18:33.767 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:33.769 | INFO     | src.policies:train:123 - Epoch 454 / 800\n",
      "2021-09-07 17:18:33.769 | INFO     | src.policies:collect_trajectories:221 - Episode 1254\n",
      "2021-09-07 17:18:33.802 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:33.802 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:33.803 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:33.805 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:33.807 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3521288335323334, 'baseline_loss': 0.49143916368484497, 'total_loss': -0.10640925168991089}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:33.808 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18856137990951538\n",
      "2021-09-07 17:18:33.809 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.38066989183425903\n",
      "2021-09-07 17:18:33.810 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18856137990951538\n",
      "2021-09-07 17:18:33.811 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.38066989183425903\n",
      "2021-09-07 17:18:33.813 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:33.814 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43304935097694397, 'baseline_loss': 0.6687614321708679, 'total_loss': -0.09866863489151001}\n",
      "2021-09-07 17:18:33.814 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20943184196949005\n",
      "2021-09-07 17:18:33.815 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6223289966583252\n",
      "2021-09-07 17:18:33.816 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20943184196949005\n",
      "2021-09-07 17:18:33.817 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:33.818 | INFO     | src.policies:train:123 - Epoch 455 / 800\n",
      "2021-09-07 17:18:33.819 | INFO     | src.policies:collect_trajectories:221 - Episode 1255\n",
      "2021-09-07 17:18:33.848 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:33.849 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:33.849 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:33.851 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:33.854 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45396843552589417, 'baseline_loss': 0.9964497089385986, 'total_loss': 0.04425641894340515}\n",
      "2021-09-07 17:18:33.855 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.33867838978767395\n",
      "2021-09-07 17:18:33.856 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3462449312210083\n",
      "2021-09-07 17:18:33.858 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.33867838978767395\n",
      "2021-09-07 17:18:33.858 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:33.860 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:33.861 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7171947360038757, 'baseline_loss': 1.417698860168457, 'total_loss': -0.008345305919647217}\n",
      "2021-09-07 17:18:33.862 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3756961524486542\n",
      "2021-09-07 17:18:33.862 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.923827886581421\n",
      "2021-09-07 17:18:33.863 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3756961524486542\n",
      "2021-09-07 17:18:33.865 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:33.866 | INFO     | src.policies:train:123 - Epoch 456 / 800\n",
      "2021-09-07 17:18:33.867 | INFO     | src.policies:collect_trajectories:221 - Episode 1256\n",
      "2021-09-07 17:18:33.895 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:33.896 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:33.896 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:33.899 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:33.901 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6282124519348145, 'baseline_loss': 1.5763682126998901, 'total_loss': 0.15997165441513062}\n",
      "2021-09-07 17:18:33.902 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2013239860534668\n",
      "2021-09-07 17:18:33.903 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.488654613494873\n",
      "2021-09-07 17:18:33.904 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2013239860534668\n",
      "2021-09-07 17:18:33.905 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:33.906 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:33.907 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6355292797088623, 'baseline_loss': 1.5867620706558228, 'total_loss': 0.15785175561904907}\n",
      "2021-09-07 17:18:33.908 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4057585895061493\n",
      "2021-09-07 17:18:33.909 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.7173030376434326\n",
      "2021-09-07 17:18:33.910 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4057585895061493\n",
      "2021-09-07 17:18:33.911 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:33.972 | INFO     | src.policies:train:123 - Epoch 457 / 800\n",
      "2021-09-07 17:18:33.972 | INFO     | src.policies:collect_trajectories:221 - Episode 1257\n",
      "2021-09-07 17:18:34.002 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.002 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:34.003 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:34.005 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:34.008 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.22767367959022522, 'baseline_loss': 0.37256157398223877, 'total_loss': -0.041392892599105835}\n",
      "2021-09-07 17:18:34.008 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.172059565782547\n",
      "2021-09-07 17:18:34.010 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5696260929107666\n",
      "2021-09-07 17:18:34.011 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.172059565782547\n",
      "2021-09-07 17:18:34.012 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:34.013 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:34.014 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23902961611747742, 'baseline_loss': 0.395315021276474, 'total_loss': -0.04137210547924042}\n",
      "2021-09-07 17:18:34.015 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30408987402915955\n",
      "2021-09-07 17:18:34.016 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4647369384765625\n",
      "2021-09-07 17:18:34.017 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30408987402915955\n",
      "2021-09-07 17:18:34.018 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:34.019 | INFO     | src.policies:train:123 - Epoch 458 / 800\n",
      "2021-09-07 17:18:34.020 | INFO     | src.policies:collect_trajectories:221 - Episode 1258\n",
      "2021-09-07 17:18:34.049 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.050 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:34.050 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:34.052 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:34.055 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.330289751291275, 'baseline_loss': 0.4181672930717468, 'total_loss': -0.12120610475540161}\n",
      "2021-09-07 17:18:34.056 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3505391478538513\n",
      "2021-09-07 17:18:34.057 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9796504974365234\n",
      "2021-09-07 17:18:34.059 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3505391478538513\n",
      "2021-09-07 17:18:34.060 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:34.061 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:34.062 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2611180245876312, 'baseline_loss': 0.3743806779384613, 'total_loss': -0.07392768561840057}\n",
      "2021-09-07 17:18:34.063 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08943355083465576\n",
      "2021-09-07 17:18:34.064 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8090338110923767\n",
      "2021-09-07 17:18:34.065 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08943355083465576\n",
      "2021-09-07 17:18:34.066 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:34.067 | INFO     | src.policies:train:123 - Epoch 459 / 800\n",
      "2021-09-07 17:18:34.068 | INFO     | src.policies:collect_trajectories:221 - Episode 1259\n",
      "2021-09-07 17:18:34.081 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.082 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 90.0\n",
      "2021-09-07 17:18:34.082 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 90.0\n",
      "2021-09-07 17:18:34.083 | INFO     | src.policies:collect_trajectories:221 - Episode 1260\n",
      "2021-09-07 17:18:34.116 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.117 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:34.117 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 145.0\n",
      "2021-09-07 17:18:34.118 | WARNING  | src.policies:train:144 - The actual batch size is 290, instead of 200\n",
      "2021-09-07 17:18:34.121 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:34.124 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6991531252861023, 'baseline_loss': 1.6630362272262573, 'total_loss': 0.13236498832702637}\n",
      "2021-09-07 17:18:34.125 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3613852262496948\n",
      "2021-09-07 17:18:34.126 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.7836930751800537\n",
      "2021-09-07 17:18:34.127 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3613852262496948\n",
      "2021-09-07 17:18:34.128 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:34.129 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:34.130 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6680395603179932, 'baseline_loss': 1.7829898595809937, 'total_loss': 0.22345536947250366}\n",
      "2021-09-07 17:18:34.131 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.398747980594635\n",
      "2021-09-07 17:18:34.132 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.0576698780059814\n",
      "2021-09-07 17:18:34.134 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.398747980594635\n",
      "2021-09-07 17:18:34.135 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:34.136 | INFO     | src.policies:train:123 - Epoch 460 / 800\n",
      "2021-09-07 17:18:34.137 | INFO     | src.policies:collect_trajectories:221 - Episode 1261\n",
      "2021-09-07 17:18:34.165 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.166 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:34.166 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:34.168 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:34.170 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6319109797477722, 'baseline_loss': 1.9562345743179321, 'total_loss': 0.34620630741119385}\n",
      "2021-09-07 17:18:34.171 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.34485283493995667\n",
      "2021-09-07 17:18:34.172 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.709796905517578\n",
      "2021-09-07 17:18:34.173 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.34485283493995667\n",
      "2021-09-07 17:18:34.174 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:34.175 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:34.177 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6814433932304382, 'baseline_loss': 2.1090850830078125, 'total_loss': 0.373099148273468}\n",
      "2021-09-07 17:18:34.177 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4996037781238556\n",
      "2021-09-07 17:18:34.178 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.8872592449188232\n",
      "2021-09-07 17:18:34.179 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4996037781238556\n",
      "2021-09-07 17:18:34.180 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:34.182 | INFO     | src.policies:train:123 - Epoch 461 / 800\n",
      "2021-09-07 17:18:34.182 | INFO     | src.policies:collect_trajectories:221 - Episode 1262\n",
      "2021-09-07 17:18:34.212 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.212 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:34.213 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:34.214 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:34.216 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.9640851020812988, 'baseline_loss': 3.1615467071533203, 'total_loss': 0.6166882514953613}\n",
      "2021-09-07 17:18:34.217 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.37457767128944397\n",
      "2021-09-07 17:18:34.218 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 5.279212951660156\n",
      "2021-09-07 17:18:34.220 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.37457767128944397\n",
      "2021-09-07 17:18:34.221 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999999701976776\n",
      "2021-09-07 17:18:34.222 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:34.223 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.9029824137687683, 'baseline_loss': 3.208237409591675, 'total_loss': 0.7011362910270691}\n",
      "2021-09-07 17:18:34.224 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2709762454032898\n",
      "2021-09-07 17:18:34.225 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 5.50825834274292\n",
      "2021-09-07 17:18:34.226 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2709762454032898\n",
      "2021-09-07 17:18:34.227 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:34.228 | INFO     | src.policies:train:123 - Epoch 462 / 800\n",
      "2021-09-07 17:18:34.229 | INFO     | src.policies:collect_trajectories:221 - Episode 1263\n",
      "2021-09-07 17:18:34.253 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.254 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 165.0\n",
      "2021-09-07 17:18:34.254 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 165.0\n",
      "2021-09-07 17:18:34.254 | INFO     | src.policies:collect_trajectories:221 - Episode 1264\n",
      "2021-09-07 17:18:34.285 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.286 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:34.286 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 182.5\n",
      "2021-09-07 17:18:34.287 | WARNING  | src.policies:train:144 - The actual batch size is 365, instead of 200\n",
      "2021-09-07 17:18:34.289 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:34.292 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7028524875640869, 'baseline_loss': 1.863156795501709, 'total_loss': 0.22872591018676758}\n",
      "2021-09-07 17:18:34.293 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4870094656944275\n",
      "2021-09-07 17:18:34.294 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.837662935256958\n",
      "2021-09-07 17:18:34.296 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4870094656944275\n",
      "2021-09-07 17:18:34.298 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:34.299 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:34.301 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.755574107170105, 'baseline_loss': 2.095236301422119, 'total_loss': 0.2920440435409546}\n",
      "2021-09-07 17:18:34.302 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23308677971363068\n",
      "2021-09-07 17:18:34.303 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.5254721641540527\n",
      "2021-09-07 17:18:34.304 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23308677971363068\n",
      "2021-09-07 17:18:34.305 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:34.307 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:34.308 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6336742043495178, 'baseline_loss': 1.8578827381134033, 'total_loss': 0.29526716470718384}\n",
      "2021-09-07 17:18:34.309 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2777501344680786\n",
      "2021-09-07 17:18:34.310 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.111971855163574\n",
      "2021-09-07 17:18:34.311 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2777501344680786\n",
      "2021-09-07 17:18:34.313 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:34.314 | INFO     | src.policies:train:123 - Epoch 463 / 800\n",
      "2021-09-07 17:18:34.315 | INFO     | src.policies:collect_trajectories:221 - Episode 1265\n",
      "2021-09-07 17:18:34.351 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.351 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:34.352 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:34.353 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:34.356 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8887848854064941, 'baseline_loss': 3.1565377712249756, 'total_loss': 0.6894840002059937}\n",
      "2021-09-07 17:18:34.357 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6199538111686707\n",
      "2021-09-07 17:18:34.358 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.723176002502441\n",
      "2021-09-07 17:18:34.359 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:34.361 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999999403953552\n",
      "2021-09-07 17:18:34.362 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:34.363 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8625401258468628, 'baseline_loss': 3.053194046020508, 'total_loss': 0.6640568971633911}\n",
      "2021-09-07 17:18:34.364 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7735550403594971\n",
      "2021-09-07 17:18:34.364 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.671370506286621\n",
      "2021-09-07 17:18:34.365 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:34.366 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:34.368 | INFO     | src.policies:train:123 - Epoch 464 / 800\n",
      "2021-09-07 17:18:34.368 | INFO     | src.policies:collect_trajectories:221 - Episode 1266\n",
      "2021-09-07 17:18:34.398 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.399 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:34.399 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:34.401 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:34.403 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4599061608314514, 'baseline_loss': 1.3159239292144775, 'total_loss': 0.19805580377578735}\n",
      "2021-09-07 17:18:34.404 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2275109440088272\n",
      "2021-09-07 17:18:34.405 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7108559608459473\n",
      "2021-09-07 17:18:34.406 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2275109440088272\n",
      "2021-09-07 17:18:34.407 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:34.408 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:34.409 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3931386172771454, 'baseline_loss': 1.0936763286590576, 'total_loss': 0.15369954705238342}\n",
      "2021-09-07 17:18:34.410 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6067012548446655\n",
      "2021-09-07 17:18:34.411 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1707724332809448\n",
      "2021-09-07 17:18:34.412 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:34.413 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:34.414 | INFO     | src.policies:train:123 - Epoch 465 / 800\n",
      "2021-09-07 17:18:34.415 | INFO     | src.policies:collect_trajectories:221 - Episode 1267\n",
      "2021-09-07 17:18:34.442 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.443 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 183.0\n",
      "2021-09-07 17:18:34.444 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 183.0\n",
      "2021-09-07 17:18:34.444 | INFO     | src.policies:collect_trajectories:221 - Episode 1268\n",
      "2021-09-07 17:18:34.522 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:34.523 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:34.523 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 191.5\n",
      "2021-09-07 17:18:34.524 | WARNING  | src.policies:train:144 - The actual batch size is 383, instead of 200\n",
      "2021-09-07 17:18:34.526 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:34.530 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5933182835578918, 'baseline_loss': 1.5986636877059937, 'total_loss': 0.20601356029510498}\n",
      "2021-09-07 17:18:34.531 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.26852476596832275\n",
      "2021-09-07 17:18:34.532 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.5573196411132812\n",
      "2021-09-07 17:18:34.534 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.26852476596832275\n",
      "2021-09-07 17:18:34.535 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:34.536 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:34.538 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6878174543380737, 'baseline_loss': 1.474725365638733, 'total_loss': 0.049545228481292725}\n",
      "2021-09-07 17:18:34.539 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6443709135055542\n",
      "2021-09-07 17:18:34.540 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9092994928359985\n",
      "2021-09-07 17:18:34.541 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:34.543 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:34.544 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:34.545 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45299893617630005, 'baseline_loss': 1.2935094833374023, 'total_loss': 0.19375580549240112}\n",
      "2021-09-07 17:18:34.546 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.43079835176467896\n",
      "2021-09-07 17:18:34.547 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4321755170822144\n",
      "2021-09-07 17:18:34.548 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.43079835176467896\n",
      "2021-09-07 17:18:34.549 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:34.551 | INFO     | src.policies:train:123 - Epoch 466 / 800\n",
      "2021-09-07 17:18:34.551 | INFO     | src.policies:collect_trajectories:221 - Episode 1269\n",
      "2021-09-07 17:18:34.579 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.580 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 184.0\n",
      "2021-09-07 17:18:34.580 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 184.0\n",
      "2021-09-07 17:18:34.581 | INFO     | src.policies:collect_trajectories:221 - Episode 1270\n",
      "2021-09-07 17:18:34.586 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.587 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 23.0\n",
      "2021-09-07 17:18:34.587 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 103.5\n",
      "2021-09-07 17:18:34.588 | WARNING  | src.policies:train:144 - The actual batch size is 207, instead of 200\n",
      "2021-09-07 17:18:34.589 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:34.591 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.39212217926979065, 'baseline_loss': 1.0614030361175537, 'total_loss': 0.1385793387889862}\n",
      "2021-09-07 17:18:34.592 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.42820504307746887\n",
      "2021-09-07 17:18:34.593 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0315241813659668\n",
      "2021-09-07 17:18:34.594 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.42820504307746887\n",
      "2021-09-07 17:18:34.595 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:34.596 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:34.597 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2373986393213272, 'baseline_loss': 0.8722032308578491, 'total_loss': 0.19870297610759735}\n",
      "2021-09-07 17:18:34.598 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17140766978263855\n",
      "2021-09-07 17:18:34.599 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.0178489685058594\n",
      "2021-09-07 17:18:34.600 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17140766978263855\n",
      "2021-09-07 17:18:34.601 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:34.602 | INFO     | src.policies:train:123 - Epoch 467 / 800\n",
      "2021-09-07 17:18:34.603 | INFO     | src.policies:collect_trajectories:221 - Episode 1271\n",
      "2021-09-07 17:18:34.630 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.630 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 182.0\n",
      "2021-09-07 17:18:34.631 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 182.0\n",
      "2021-09-07 17:18:34.631 | INFO     | src.policies:collect_trajectories:221 - Episode 1272\n",
      "2021-09-07 17:18:34.663 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.663 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:34.664 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 191.0\n",
      "2021-09-07 17:18:34.664 | WARNING  | src.policies:train:144 - The actual batch size is 382, instead of 200\n",
      "2021-09-07 17:18:34.667 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:34.670 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4169730246067047, 'baseline_loss': 0.999157190322876, 'total_loss': 0.08260557055473328}\n",
      "2021-09-07 17:18:34.671 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4042537808418274\n",
      "2021-09-07 17:18:34.672 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9452439546585083\n",
      "2021-09-07 17:18:34.673 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4042537808418274\n",
      "2021-09-07 17:18:34.674 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:34.675 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:34.676 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43501487374305725, 'baseline_loss': 1.1485180854797363, 'total_loss': 0.1392441689968109}\n",
      "2021-09-07 17:18:34.677 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.28057044744491577\n",
      "2021-09-07 17:18:34.677 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9485440254211426\n",
      "2021-09-07 17:18:34.679 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.28057044744491577\n",
      "2021-09-07 17:18:34.680 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:34.682 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:34.683 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4823371171951294, 'baseline_loss': 1.2338465452194214, 'total_loss': 0.1345861554145813}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:34.684 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.26410624384880066\n",
      "2021-09-07 17:18:34.684 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1952199935913086\n",
      "2021-09-07 17:18:34.686 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.26410624384880066\n",
      "2021-09-07 17:18:34.687 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:34.688 | INFO     | src.policies:train:123 - Epoch 468 / 800\n",
      "2021-09-07 17:18:34.689 | INFO     | src.policies:collect_trajectories:221 - Episode 1273\n",
      "2021-09-07 17:18:34.716 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.717 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:34.717 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:34.719 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:34.721 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4256834387779236, 'baseline_loss': 1.5076817274093628, 'total_loss': 0.3281574249267578}\n",
      "2021-09-07 17:18:34.723 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16407406330108643\n",
      "2021-09-07 17:18:34.724 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.994135856628418\n",
      "2021-09-07 17:18:34.725 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16407406330108643\n",
      "2021-09-07 17:18:34.726 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:34.728 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:34.729 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3555070161819458, 'baseline_loss': 1.3313392400741577, 'total_loss': 0.31016260385513306}\n",
      "2021-09-07 17:18:34.730 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.601449191570282\n",
      "2021-09-07 17:18:34.731 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9361958503723145\n",
      "2021-09-07 17:18:34.732 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:34.733 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:34.734 | INFO     | src.policies:train:123 - Epoch 469 / 800\n",
      "2021-09-07 17:18:34.735 | INFO     | src.policies:collect_trajectories:221 - Episode 1274\n",
      "2021-09-07 17:18:34.765 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.766 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:34.766 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:34.768 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:34.771 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.28955185413360596, 'baseline_loss': 0.9389817714691162, 'total_loss': 0.17993903160095215}\n",
      "2021-09-07 17:18:34.772 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3501479923725128\n",
      "2021-09-07 17:18:34.773 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9102542996406555\n",
      "2021-09-07 17:18:34.774 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3501479923725128\n",
      "2021-09-07 17:18:34.775 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:34.777 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:34.778 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4071277976036072, 'baseline_loss': 1.060089111328125, 'total_loss': 0.12291675806045532}\n",
      "2021-09-07 17:18:34.779 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.38052812218666077\n",
      "2021-09-07 17:18:34.780 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5323802828788757\n",
      "2021-09-07 17:18:34.782 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.38052812218666077\n",
      "2021-09-07 17:18:34.783 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:34.784 | INFO     | src.policies:train:123 - Epoch 470 / 800\n",
      "2021-09-07 17:18:34.785 | INFO     | src.policies:collect_trajectories:221 - Episode 1275\n",
      "2021-09-07 17:18:34.810 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.810 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 150.0\n",
      "2021-09-07 17:18:34.811 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 150.0\n",
      "2021-09-07 17:18:34.811 | INFO     | src.policies:collect_trajectories:221 - Episode 1276\n",
      "2021-09-07 17:18:34.841 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.842 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:34.842 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 175.0\n",
      "2021-09-07 17:18:34.843 | WARNING  | src.policies:train:144 - The actual batch size is 350, instead of 200\n",
      "2021-09-07 17:18:34.846 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:34.848 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4101463556289673, 'baseline_loss': 1.4498707056045532, 'total_loss': 0.3147889971733093}\n",
      "2021-09-07 17:18:34.849 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.39082056283950806\n",
      "2021-09-07 17:18:34.850 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.398104190826416\n",
      "2021-09-07 17:18:34.851 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.39082056283950806\n",
      "2021-09-07 17:18:34.852 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:34.853 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:34.854 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3840799331665039, 'baseline_loss': 1.2347010374069214, 'total_loss': 0.2332705855369568}\n",
      "2021-09-07 17:18:34.855 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5554696321487427\n",
      "2021-09-07 17:18:34.856 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.397584319114685\n",
      "2021-09-07 17:18:34.858 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:34.858 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:34.860 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:34.861 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3894878029823303, 'baseline_loss': 1.3680996894836426, 'total_loss': 0.29456204175949097}\n",
      "2021-09-07 17:18:34.861 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18174414336681366\n",
      "2021-09-07 17:18:34.862 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5102338790893555\n",
      "2021-09-07 17:18:34.863 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18174414336681366\n",
      "2021-09-07 17:18:34.864 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:34.866 | INFO     | src.policies:train:123 - Epoch 471 / 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:34.866 | INFO     | src.policies:collect_trajectories:221 - Episode 1277\n",
      "2021-09-07 17:18:34.896 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.897 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:34.897 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:34.899 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:34.901 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.28201940655708313, 'baseline_loss': 0.6047320365905762, 'total_loss': 0.020346611738204956}\n",
      "2021-09-07 17:18:34.902 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30891484022140503\n",
      "2021-09-07 17:18:34.903 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5286729335784912\n",
      "2021-09-07 17:18:34.904 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30891484022140503\n",
      "2021-09-07 17:18:34.905 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:34.907 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:34.908 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.16715914011001587, 'baseline_loss': 0.487629771232605, 'total_loss': 0.07665574550628662}\n",
      "2021-09-07 17:18:34.909 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4549106955528259\n",
      "2021-09-07 17:18:34.910 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.778102159500122\n",
      "2021-09-07 17:18:34.911 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4549106955528259\n",
      "2021-09-07 17:18:34.912 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:34.913 | INFO     | src.policies:train:123 - Epoch 472 / 800\n",
      "2021-09-07 17:18:34.914 | INFO     | src.policies:collect_trajectories:221 - Episode 1278\n",
      "2021-09-07 17:18:34.940 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:34.940 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 168.0\n",
      "2021-09-07 17:18:34.941 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 168.0\n",
      "2021-09-07 17:18:34.942 | INFO     | src.policies:collect_trajectories:221 - Episode 1279\n",
      "2021-09-07 17:18:35.102 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:35.103 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:35.104 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 184.0\n",
      "2021-09-07 17:18:35.104 | WARNING  | src.policies:train:144 - The actual batch size is 368, instead of 200\n",
      "2021-09-07 17:18:35.106 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:35.109 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43309837579727173, 'baseline_loss': 1.3365875482559204, 'total_loss': 0.23519539833068848}\n",
      "2021-09-07 17:18:35.110 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2897450923919678\n",
      "2021-09-07 17:18:35.110 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.358940839767456\n",
      "2021-09-07 17:18:35.111 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2897450923919678\n",
      "2021-09-07 17:18:35.112 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:35.113 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:35.114 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3308199346065521, 'baseline_loss': 1.0586881637573242, 'total_loss': 0.19852414727210999}\n",
      "2021-09-07 17:18:35.115 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.37526237964630127\n",
      "2021-09-07 17:18:35.116 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1909279823303223\n",
      "2021-09-07 17:18:35.117 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.37526237964630127\n",
      "2021-09-07 17:18:35.118 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:35.119 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:35.120 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.34594640135765076, 'baseline_loss': 0.9706737995147705, 'total_loss': 0.1393904983997345}\n",
      "2021-09-07 17:18:35.121 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.130752831697464\n",
      "2021-09-07 17:18:35.122 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.920293390750885\n",
      "2021-09-07 17:18:35.123 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.130752831697464\n",
      "2021-09-07 17:18:35.124 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:35.125 | INFO     | src.policies:train:123 - Epoch 473 / 800\n",
      "2021-09-07 17:18:35.126 | INFO     | src.policies:collect_trajectories:221 - Episode 1280\n",
      "2021-09-07 17:18:35.147 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:35.148 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 139.0\n",
      "2021-09-07 17:18:35.148 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 139.0\n",
      "2021-09-07 17:18:35.149 | INFO     | src.policies:collect_trajectories:221 - Episode 1281\n",
      "2021-09-07 17:18:35.180 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:35.181 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:35.181 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 169.5\n",
      "2021-09-07 17:18:35.182 | WARNING  | src.policies:train:144 - The actual batch size is 339, instead of 200\n",
      "2021-09-07 17:18:35.184 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:35.187 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2492384910583496, 'baseline_loss': 0.8288748860359192, 'total_loss': 0.16519895195960999}\n",
      "2021-09-07 17:18:35.188 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1789388358592987\n",
      "2021-09-07 17:18:35.190 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7928580641746521\n",
      "2021-09-07 17:18:35.191 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1789388358592987\n",
      "2021-09-07 17:18:35.192 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:35.193 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:35.194 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2261236310005188, 'baseline_loss': 0.8651253581047058, 'total_loss': 0.2064390480518341}\n",
      "2021-09-07 17:18:35.195 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15952089428901672\n",
      "2021-09-07 17:18:35.196 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9066398739814758\n",
      "2021-09-07 17:18:35.197 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15952089428901672\n",
      "2021-09-07 17:18:35.198 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:35.199 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:35.200 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3238007426261902, 'baseline_loss': 0.8887984752655029, 'total_loss': 0.12059849500656128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:35.201 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20422542095184326\n",
      "2021-09-07 17:18:35.202 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9440163969993591\n",
      "2021-09-07 17:18:35.203 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20422542095184326\n",
      "2021-09-07 17:18:35.204 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:35.205 | INFO     | src.policies:train:123 - Epoch 474 / 800\n",
      "2021-09-07 17:18:35.206 | INFO     | src.policies:collect_trajectories:221 - Episode 1282\n",
      "2021-09-07 17:18:35.234 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:35.235 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:35.235 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:35.237 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:35.240 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3813914954662323, 'baseline_loss': 1.2321178913116455, 'total_loss': 0.23466745018959045}\n",
      "2021-09-07 17:18:35.241 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24412968754768372\n",
      "2021-09-07 17:18:35.241 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2212222814559937\n",
      "2021-09-07 17:18:35.242 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24412968754768372\n",
      "2021-09-07 17:18:35.243 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:35.245 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:35.246 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4992903172969818, 'baseline_loss': 1.1420398950576782, 'total_loss': 0.0717296302318573}\n",
      "2021-09-07 17:18:35.247 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4957890510559082\n",
      "2021-09-07 17:18:35.247 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.067232608795166\n",
      "2021-09-07 17:18:35.248 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4957890510559082\n",
      "2021-09-07 17:18:35.249 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:35.251 | INFO     | src.policies:train:123 - Epoch 475 / 800\n",
      "2021-09-07 17:18:35.251 | INFO     | src.policies:collect_trajectories:221 - Episode 1283\n",
      "2021-09-07 17:18:35.258 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:35.259 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 40.0\n",
      "2021-09-07 17:18:35.259 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 40.0\n",
      "2021-09-07 17:18:35.260 | INFO     | src.policies:collect_trajectories:221 - Episode 1284\n",
      "2021-09-07 17:18:35.282 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:35.283 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 154.0\n",
      "2021-09-07 17:18:35.283 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 97.0\n",
      "2021-09-07 17:18:35.284 | INFO     | src.policies:collect_trajectories:221 - Episode 1285\n",
      "2021-09-07 17:18:35.307 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:35.308 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 127.0\n",
      "2021-09-07 17:18:35.308 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 107.0\n",
      "2021-09-07 17:18:35.309 | WARNING  | src.policies:train:144 - The actual batch size is 321, instead of 200\n",
      "2021-09-07 17:18:35.311 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:35.313 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4433160126209259, 'baseline_loss': 1.0968204736709595, 'total_loss': 0.10509422421455383}\n",
      "2021-09-07 17:18:35.315 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.34908926486968994\n",
      "2021-09-07 17:18:35.315 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8696318864822388\n",
      "2021-09-07 17:18:35.317 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.34908926486968994\n",
      "2021-09-07 17:18:35.318 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:35.319 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:35.320 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2642577290534973, 'baseline_loss': 0.8550550937652588, 'total_loss': 0.16326981782913208}\n",
      "2021-09-07 17:18:35.321 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20055672526359558\n",
      "2021-09-07 17:18:35.322 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.223207712173462\n",
      "2021-09-07 17:18:35.323 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20055672526359558\n",
      "2021-09-07 17:18:35.324 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:35.325 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:35.326 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43184176087379456, 'baseline_loss': 0.8747078776359558, 'total_loss': 0.00551217794418335}\n",
      "2021-09-07 17:18:35.327 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4200211465358734\n",
      "2021-09-07 17:18:35.328 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6923344731330872\n",
      "2021-09-07 17:18:35.329 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4200211465358734\n",
      "2021-09-07 17:18:35.330 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:35.332 | INFO     | src.policies:train:123 - Epoch 476 / 800\n",
      "2021-09-07 17:18:35.332 | INFO     | src.policies:collect_trajectories:221 - Episode 1286\n",
      "2021-09-07 17:18:35.361 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:35.362 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:35.362 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:35.364 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:35.366 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3532658815383911, 'baseline_loss': 1.2353183031082153, 'total_loss': 0.26439327001571655}\n",
      "2021-09-07 17:18:35.367 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.44333112239837646\n",
      "2021-09-07 17:18:35.368 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.885515809059143\n",
      "2021-09-07 17:18:35.370 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.44333112239837646\n",
      "2021-09-07 17:18:35.371 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:35.372 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:35.373 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32460516691207886, 'baseline_loss': 1.3569183349609375, 'total_loss': 0.3538540005683899}\n",
      "2021-09-07 17:18:35.374 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12622776627540588\n",
      "2021-09-07 17:18:35.375 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.289747953414917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:35.376 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12622776627540588\n",
      "2021-09-07 17:18:35.377 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:35.378 | INFO     | src.policies:train:123 - Epoch 477 / 800\n",
      "2021-09-07 17:18:35.379 | INFO     | src.policies:collect_trajectories:221 - Episode 1287\n",
      "2021-09-07 17:18:35.410 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:35.410 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:35.411 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:35.412 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:35.415 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.52718186378479, 'baseline_loss': 1.2792353630065918, 'total_loss': 0.11243581771850586}\n",
      "2021-09-07 17:18:35.416 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10470432043075562\n",
      "2021-09-07 17:18:35.417 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.1125309467315674\n",
      "2021-09-07 17:18:35.418 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10470432043075562\n",
      "2021-09-07 17:18:35.419 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:35.421 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:35.422 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6096304059028625, 'baseline_loss': 1.4975013732910156, 'total_loss': 0.13912028074264526}\n",
      "2021-09-07 17:18:35.423 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6179907321929932\n",
      "2021-09-07 17:18:35.423 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.46211838722229\n",
      "2021-09-07 17:18:35.424 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:35.425 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:35.427 | INFO     | src.policies:train:123 - Epoch 478 / 800\n",
      "2021-09-07 17:18:35.427 | INFO     | src.policies:collect_trajectories:221 - Episode 1288\n",
      "2021-09-07 17:18:35.458 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:35.458 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:35.459 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:35.462 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:35.464 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6172499060630798, 'baseline_loss': 1.8590965270996094, 'total_loss': 0.31229835748672485}\n",
      "2021-09-07 17:18:35.465 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5500233769416809\n",
      "2021-09-07 17:18:35.466 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.5628161430358887\n",
      "2021-09-07 17:18:35.467 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:35.468 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:35.469 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:35.471 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5233520865440369, 'baseline_loss': 1.5946766138076782, 'total_loss': 0.27398622035980225}\n",
      "2021-09-07 17:18:35.471 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17972347140312195\n",
      "2021-09-07 17:18:35.472 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8631901741027832\n",
      "2021-09-07 17:18:35.473 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17972347140312195\n",
      "2021-09-07 17:18:35.474 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:35.476 | INFO     | src.policies:train:123 - Epoch 479 / 800\n",
      "2021-09-07 17:18:35.476 | INFO     | src.policies:collect_trajectories:221 - Episode 1289\n",
      "2021-09-07 17:18:35.506 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:35.506 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:35.507 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:35.509 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:35.512 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2536329925060272, 'baseline_loss': 0.5286123156547546, 'total_loss': 0.010673165321350098}\n",
      "2021-09-07 17:18:35.513 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19864630699157715\n",
      "2021-09-07 17:18:35.514 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9217147827148438\n",
      "2021-09-07 17:18:35.515 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19864630699157715\n",
      "2021-09-07 17:18:35.516 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:35.517 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:35.518 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.21504639089107513, 'baseline_loss': 0.5293322205543518, 'total_loss': 0.04961971938610077}\n",
      "2021-09-07 17:18:35.519 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17364177107810974\n",
      "2021-09-07 17:18:35.521 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1213843822479248\n",
      "2021-09-07 17:18:35.522 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17364177107810974\n",
      "2021-09-07 17:18:35.523 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:35.524 | INFO     | src.policies:train:123 - Epoch 480 / 800\n",
      "2021-09-07 17:18:35.525 | INFO     | src.policies:collect_trajectories:221 - Episode 1290\n",
      "2021-09-07 17:18:35.554 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:35.555 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:35.555 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:35.557 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:35.559 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5318236351013184, 'baseline_loss': 1.3989390134811401, 'total_loss': 0.1676458716392517}\n",
      "2021-09-07 17:18:35.560 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7311015725135803\n",
      "2021-09-07 17:18:35.561 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.806289792060852\n",
      "2021-09-07 17:18:35.562 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:35.563 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:35.564 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:35.565 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6448411345481873, 'baseline_loss': 1.7833430767059326, 'total_loss': 0.24683040380477905}\n",
      "2021-09-07 17:18:35.566 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.29293182492256165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:35.567 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.584242343902588\n",
      "2021-09-07 17:18:35.568 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.29293182492256165\n",
      "2021-09-07 17:18:35.569 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:35.570 | INFO     | src.policies:train:123 - Epoch 481 / 800\n",
      "2021-09-07 17:18:35.571 | INFO     | src.policies:collect_trajectories:221 - Episode 1291\n",
      "2021-09-07 17:18:35.746 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:35.747 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:35.747 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:35.749 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:35.751 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36526283621788025, 'baseline_loss': 0.9456024169921875, 'total_loss': 0.1075383722782135}\n",
      "2021-09-07 17:18:35.752 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3242935240268707\n",
      "2021-09-07 17:18:35.753 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9186782836914062\n",
      "2021-09-07 17:18:35.754 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3242935240268707\n",
      "2021-09-07 17:18:35.756 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:35.757 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:35.758 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3592861592769623, 'baseline_loss': 1.0884885787963867, 'total_loss': 0.18495813012123108}\n",
      "2021-09-07 17:18:35.759 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30769285559654236\n",
      "2021-09-07 17:18:35.760 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5757909417152405\n",
      "2021-09-07 17:18:35.761 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30769285559654236\n",
      "2021-09-07 17:18:35.762 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:35.764 | INFO     | src.policies:train:123 - Epoch 482 / 800\n",
      "2021-09-07 17:18:35.764 | INFO     | src.policies:collect_trajectories:221 - Episode 1292\n",
      "2021-09-07 17:18:35.796 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:35.796 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:35.797 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:35.801 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:35.803 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27653852105140686, 'baseline_loss': 0.5380628705024719, 'total_loss': -0.0075070858001708984}\n",
      "2021-09-07 17:18:35.804 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14541006088256836\n",
      "2021-09-07 17:18:35.805 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3235783576965332\n",
      "2021-09-07 17:18:35.806 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14541006088256836\n",
      "2021-09-07 17:18:35.807 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:35.808 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:35.809 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.33290112018585205, 'baseline_loss': 0.633205771446228, 'total_loss': -0.016298234462738037}\n",
      "2021-09-07 17:18:35.810 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.62371826171875\n",
      "2021-09-07 17:18:35.811 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2975375652313232\n",
      "2021-09-07 17:18:35.812 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:35.813 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:35.814 | INFO     | src.policies:train:123 - Epoch 483 / 800\n",
      "2021-09-07 17:18:35.815 | INFO     | src.policies:collect_trajectories:221 - Episode 1293\n",
      "2021-09-07 17:18:35.843 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:35.844 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:35.844 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:35.846 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:35.848 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1742071956396103, 'baseline_loss': 0.48331138491630554, 'total_loss': 0.06744849681854248}\n",
      "2021-09-07 17:18:35.849 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20939631760120392\n",
      "2021-09-07 17:18:35.850 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7338426113128662\n",
      "2021-09-07 17:18:35.851 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20939631760120392\n",
      "2021-09-07 17:18:35.852 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:35.853 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:35.854 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23920589685440063, 'baseline_loss': 0.49219632148742676, 'total_loss': 0.006892263889312744}\n",
      "2021-09-07 17:18:35.855 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3200247287750244\n",
      "2021-09-07 17:18:35.856 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.865057110786438\n",
      "2021-09-07 17:18:35.857 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3200247287750244\n",
      "2021-09-07 17:18:35.858 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:35.859 | INFO     | src.policies:train:123 - Epoch 484 / 800\n",
      "2021-09-07 17:18:35.860 | INFO     | src.policies:collect_trajectories:221 - Episode 1294\n",
      "2021-09-07 17:18:35.875 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:35.876 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 106.0\n",
      "2021-09-07 17:18:35.876 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 106.0\n",
      "2021-09-07 17:18:35.877 | INFO     | src.policies:collect_trajectories:221 - Episode 1295\n",
      "2021-09-07 17:18:35.906 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:35.906 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:35.906 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 153.0\n",
      "2021-09-07 17:18:35.907 | WARNING  | src.policies:train:144 - The actual batch size is 306, instead of 200\n",
      "2021-09-07 17:18:35.910 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:35.912 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.19570398330688477, 'baseline_loss': 0.6517259478569031, 'total_loss': 0.13015899062156677}\n",
      "2021-09-07 17:18:35.912 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17592914402484894\n",
      "2021-09-07 17:18:35.913 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2354768514633179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:35.914 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17592914402484894\n",
      "2021-09-07 17:18:35.915 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:35.917 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:35.918 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23816627264022827, 'baseline_loss': 0.50181645154953, 'total_loss': 0.012741953134536743}\n",
      "2021-09-07 17:18:35.919 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13147585093975067\n",
      "2021-09-07 17:18:35.920 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5255435705184937\n",
      "2021-09-07 17:18:35.921 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13147585093975067\n",
      "2021-09-07 17:18:35.922 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:35.923 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:35.924 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2463495135307312, 'baseline_loss': 0.5767231583595276, 'total_loss': 0.04201206564903259}\n",
      "2021-09-07 17:18:35.925 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32656991481781006\n",
      "2021-09-07 17:18:35.926 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3966915607452393\n",
      "2021-09-07 17:18:35.927 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32656991481781006\n",
      "2021-09-07 17:18:35.928 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:35.930 | INFO     | src.policies:train:123 - Epoch 485 / 800\n",
      "2021-09-07 17:18:35.930 | INFO     | src.policies:collect_trajectories:221 - Episode 1296\n",
      "2021-09-07 17:18:35.940 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:35.941 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 65.0\n",
      "2021-09-07 17:18:35.942 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 65.0\n",
      "2021-09-07 17:18:35.943 | INFO     | src.policies:collect_trajectories:221 - Episode 1297\n",
      "2021-09-07 17:18:35.975 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:35.976 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:35.977 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 132.5\n",
      "2021-09-07 17:18:35.977 | WARNING  | src.policies:train:144 - The actual batch size is 265, instead of 200\n",
      "2021-09-07 17:18:35.980 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:35.983 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6139394640922546, 'baseline_loss': 1.5768017768859863, 'total_loss': 0.17446142435073853}\n",
      "2021-09-07 17:18:35.983 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25094500184059143\n",
      "2021-09-07 17:18:35.984 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9079655408859253\n",
      "2021-09-07 17:18:35.985 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25094500184059143\n",
      "2021-09-07 17:18:35.987 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:35.988 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:35.989 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6147754192352295, 'baseline_loss': 1.4592416286468506, 'total_loss': 0.1148453950881958}\n",
      "2021-09-07 17:18:35.990 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.37965208292007446\n",
      "2021-09-07 17:18:35.990 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4624478816986084\n",
      "2021-09-07 17:18:35.991 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.37965208292007446\n",
      "2021-09-07 17:18:35.992 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:35.994 | INFO     | src.policies:train:123 - Epoch 486 / 800\n",
      "2021-09-07 17:18:35.994 | INFO     | src.policies:collect_trajectories:221 - Episode 1298\n",
      "2021-09-07 17:18:36.024 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.025 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:36.026 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:36.027 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:36.031 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3171907663345337, 'baseline_loss': 0.578324556350708, 'total_loss': -0.028028488159179688}\n",
      "2021-09-07 17:18:36.032 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15746307373046875\n",
      "2021-09-07 17:18:36.033 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5232115983963013\n",
      "2021-09-07 17:18:36.034 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15746307373046875\n",
      "2021-09-07 17:18:36.036 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:36.037 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:36.038 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29054510593414307, 'baseline_loss': 0.4552001655101776, 'total_loss': -0.06294502317905426}\n",
      "2021-09-07 17:18:36.039 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4022265374660492\n",
      "2021-09-07 17:18:36.040 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.049550175666809\n",
      "2021-09-07 17:18:36.041 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4022265374660492\n",
      "2021-09-07 17:18:36.042 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:36.043 | INFO     | src.policies:train:123 - Epoch 487 / 800\n",
      "2021-09-07 17:18:36.043 | INFO     | src.policies:collect_trajectories:221 - Episode 1299\n",
      "2021-09-07 17:18:36.070 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.071 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 180.0\n",
      "2021-09-07 17:18:36.071 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 180.0\n",
      "2021-09-07 17:18:36.072 | INFO     | src.policies:collect_trajectories:221 - Episode 1300\n",
      "2021-09-07 17:18:36.100 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.100 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 191.0\n",
      "2021-09-07 17:18:36.101 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 185.5\n",
      "2021-09-07 17:18:36.101 | WARNING  | src.policies:train:144 - The actual batch size is 371, instead of 200\n",
      "2021-09-07 17:18:36.104 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:36.106 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.38505810499191284, 'baseline_loss': 0.8860359787940979, 'total_loss': 0.05795988440513611}\n",
      "2021-09-07 17:18:36.107 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.28411418199539185\n",
      "2021-09-07 17:18:36.108 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6644394993782043\n",
      "2021-09-07 17:18:36.109 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.28411418199539185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:36.110 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:36.111 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:36.112 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4831022620201111, 'baseline_loss': 1.0190833806991577, 'total_loss': 0.026439428329467773}\n",
      "2021-09-07 17:18:36.113 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.35903674364089966\n",
      "2021-09-07 17:18:36.114 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2428405284881592\n",
      "2021-09-07 17:18:36.115 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.35903674364089966\n",
      "2021-09-07 17:18:36.116 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:36.117 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:36.119 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4827518165111542, 'baseline_loss': 1.133286714553833, 'total_loss': 0.08389154076576233}\n",
      "2021-09-07 17:18:36.119 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25504210591316223\n",
      "2021-09-07 17:18:36.120 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0425641536712646\n",
      "2021-09-07 17:18:36.121 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25504210591316223\n",
      "2021-09-07 17:18:36.122 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:36.123 | INFO     | src.policies:train:123 - Epoch 488 / 800\n",
      "2021-09-07 17:18:36.124 | INFO     | src.policies:collect_trajectories:221 - Episode 1301\n",
      "2021-09-07 17:18:36.139 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.139 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 98.0\n",
      "2021-09-07 17:18:36.140 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 98.0\n",
      "2021-09-07 17:18:36.140 | INFO     | src.policies:collect_trajectories:221 - Episode 1302\n",
      "2021-09-07 17:18:36.169 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.170 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:36.170 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 149.0\n",
      "2021-09-07 17:18:36.171 | WARNING  | src.policies:train:144 - The actual batch size is 298, instead of 200\n",
      "2021-09-07 17:18:36.173 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:36.175 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5456781983375549, 'baseline_loss': 1.626937985420227, 'total_loss': 0.2677907943725586}\n",
      "2021-09-07 17:18:36.176 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.336651086807251\n",
      "2021-09-07 17:18:36.177 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.1037423610687256\n",
      "2021-09-07 17:18:36.179 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.336651086807251\n",
      "2021-09-07 17:18:36.180 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:36.181 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:36.183 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.504814624786377, 'baseline_loss': 1.7271808385849, 'total_loss': 0.358775794506073}\n",
      "2021-09-07 17:18:36.185 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2047715038061142\n",
      "2021-09-07 17:18:36.186 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.952675461769104\n",
      "2021-09-07 17:18:36.188 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2047715038061142\n",
      "2021-09-07 17:18:36.190 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:36.191 | INFO     | src.policies:train:123 - Epoch 489 / 800\n",
      "2021-09-07 17:18:36.192 | INFO     | src.policies:collect_trajectories:221 - Episode 1303\n",
      "2021-09-07 17:18:36.271 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.272 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:36.273 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:36.275 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:36.278 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3754088282585144, 'baseline_loss': 0.5946335196495056, 'total_loss': -0.0780920684337616}\n",
      "2021-09-07 17:18:36.279 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16590307652950287\n",
      "2021-09-07 17:18:36.280 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6778467297554016\n",
      "2021-09-07 17:18:36.281 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16590307652950287\n",
      "2021-09-07 17:18:36.282 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:36.283 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:36.284 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2604137361049652, 'baseline_loss': 0.585423469543457, 'total_loss': 0.032297998666763306}\n",
      "2021-09-07 17:18:36.285 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.0958624929189682\n",
      "2021-09-07 17:18:36.286 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.241654634475708\n",
      "2021-09-07 17:18:36.287 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.0958624929189682\n",
      "2021-09-07 17:18:36.288 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:36.289 | INFO     | src.policies:train:123 - Epoch 490 / 800\n",
      "2021-09-07 17:18:36.290 | INFO     | src.policies:collect_trajectories:221 - Episode 1304\n",
      "2021-09-07 17:18:36.297 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.298 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 45.0\n",
      "2021-09-07 17:18:36.298 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 45.0\n",
      "2021-09-07 17:18:36.300 | INFO     | src.policies:collect_trajectories:221 - Episode 1305\n",
      "2021-09-07 17:18:36.334 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.335 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:36.335 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 122.5\n",
      "2021-09-07 17:18:36.336 | WARNING  | src.policies:train:144 - The actual batch size is 245, instead of 200\n",
      "2021-09-07 17:18:36.338 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:36.340 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4525061547756195, 'baseline_loss': 1.2771531343460083, 'total_loss': 0.18607041239738464}\n",
      "2021-09-07 17:18:36.341 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.38835686445236206\n",
      "2021-09-07 17:18:36.342 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6347125172615051\n",
      "2021-09-07 17:18:36.343 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.38835686445236206\n",
      "2021-09-07 17:18:36.344 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:36.346 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:36.347 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.403476744890213, 'baseline_loss': 1.2739126682281494, 'total_loss': 0.2334795892238617}\n",
      "2021-09-07 17:18:36.348 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2988198697566986\n",
      "2021-09-07 17:18:36.349 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2415099143981934\n",
      "2021-09-07 17:18:36.350 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2988198697566986\n",
      "2021-09-07 17:18:36.351 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:36.352 | INFO     | src.policies:train:123 - Epoch 491 / 800\n",
      "2021-09-07 17:18:36.352 | INFO     | src.policies:collect_trajectories:221 - Episode 1306\n",
      "2021-09-07 17:18:36.382 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.382 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:36.383 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:36.384 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:36.387 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5127527117729187, 'baseline_loss': 1.5351901054382324, 'total_loss': 0.2548423409461975}\n",
      "2021-09-07 17:18:36.388 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1569313108921051\n",
      "2021-09-07 17:18:36.389 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.547602891921997\n",
      "2021-09-07 17:18:36.390 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1569313108921051\n",
      "2021-09-07 17:18:36.391 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:36.392 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:36.393 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5257331728935242, 'baseline_loss': 1.4739065170288086, 'total_loss': 0.21122008562088013}\n",
      "2021-09-07 17:18:36.394 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23573023080825806\n",
      "2021-09-07 17:18:36.395 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8229013681411743\n",
      "2021-09-07 17:18:36.396 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23573023080825806\n",
      "2021-09-07 17:18:36.397 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:36.398 | INFO     | src.policies:train:123 - Epoch 492 / 800\n",
      "2021-09-07 17:18:36.399 | INFO     | src.policies:collect_trajectories:221 - Episode 1307\n",
      "2021-09-07 17:18:36.427 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.428 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:36.429 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:36.430 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:36.432 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7123442888259888, 'baseline_loss': 2.37947678565979, 'total_loss': 0.47739410400390625}\n",
      "2021-09-07 17:18:36.433 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1822064369916916\n",
      "2021-09-07 17:18:36.434 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.684499979019165\n",
      "2021-09-07 17:18:36.435 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1822064369916916\n",
      "2021-09-07 17:18:36.436 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:36.437 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:36.439 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7395550608634949, 'baseline_loss': 2.3516786098480225, 'total_loss': 0.43628424406051636}\n",
      "2021-09-07 17:18:36.440 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23865793645381927\n",
      "2021-09-07 17:18:36.441 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.6284499168395996\n",
      "2021-09-07 17:18:36.442 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23865793645381927\n",
      "2021-09-07 17:18:36.444 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999999403953552\n",
      "2021-09-07 17:18:36.445 | INFO     | src.policies:train:123 - Epoch 493 / 800\n",
      "2021-09-07 17:18:36.446 | INFO     | src.policies:collect_trajectories:221 - Episode 1308\n",
      "2021-09-07 17:18:36.478 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.479 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:36.480 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:36.483 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:36.485 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7172029614448547, 'baseline_loss': 2.3508050441741943, 'total_loss': 0.45819956064224243}\n",
      "2021-09-07 17:18:36.486 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2887672483921051\n",
      "2021-09-07 17:18:36.487 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.5116071701049805\n",
      "2021-09-07 17:18:36.488 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2887672483921051\n",
      "2021-09-07 17:18:36.489 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:36.491 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:36.492 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6831048727035522, 'baseline_loss': 2.2865288257598877, 'total_loss': 0.4601595401763916}\n",
      "2021-09-07 17:18:36.493 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5305010080337524\n",
      "2021-09-07 17:18:36.494 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.096168041229248\n",
      "2021-09-07 17:18:36.495 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:36.496 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:36.498 | INFO     | src.policies:train:123 - Epoch 494 / 800\n",
      "2021-09-07 17:18:36.498 | INFO     | src.policies:collect_trajectories:221 - Episode 1309\n",
      "2021-09-07 17:18:36.527 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.528 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:36.528 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:36.534 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:36.536 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4708167016506195, 'baseline_loss': 1.3147982358932495, 'total_loss': 0.18658241629600525}\n",
      "2021-09-07 17:18:36.537 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2913386821746826\n",
      "2021-09-07 17:18:36.538 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.2784171104431152\n",
      "2021-09-07 17:18:36.539 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2913386821746826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:36.540 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:36.542 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:36.543 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5774255990982056, 'baseline_loss': 1.287003993988037, 'total_loss': 0.06607639789581299}\n",
      "2021-09-07 17:18:36.544 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21309787034988403\n",
      "2021-09-07 17:18:36.545 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.252711534500122\n",
      "2021-09-07 17:18:36.546 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21309787034988403\n",
      "2021-09-07 17:18:36.547 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:36.548 | INFO     | src.policies:train:123 - Epoch 495 / 800\n",
      "2021-09-07 17:18:36.549 | INFO     | src.policies:collect_trajectories:221 - Episode 1310\n",
      "2021-09-07 17:18:36.569 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.569 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 133.0\n",
      "2021-09-07 17:18:36.570 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 133.0\n",
      "2021-09-07 17:18:36.570 | INFO     | src.policies:collect_trajectories:221 - Episode 1311\n",
      "2021-09-07 17:18:36.607 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.607 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:36.608 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 166.5\n",
      "2021-09-07 17:18:36.609 | WARNING  | src.policies:train:144 - The actual batch size is 333, instead of 200\n",
      "2021-09-07 17:18:36.611 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:36.613 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5248532891273499, 'baseline_loss': 1.0567854642868042, 'total_loss': 0.003539443016052246}\n",
      "2021-09-07 17:18:36.614 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30850276350975037\n",
      "2021-09-07 17:18:36.614 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3155548572540283\n",
      "2021-09-07 17:18:36.615 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30850276350975037\n",
      "2021-09-07 17:18:36.616 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:36.617 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:36.618 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5116106271743774, 'baseline_loss': 1.0218554735183716, 'total_loss': -0.0006828904151916504}\n",
      "2021-09-07 17:18:36.620 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5471336245536804\n",
      "2021-09-07 17:18:36.620 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7480645775794983\n",
      "2021-09-07 17:18:36.622 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:36.622 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:36.624 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:36.625 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4530491530895233, 'baseline_loss': 1.2015361785888672, 'total_loss': 0.14771893620491028}\n",
      "2021-09-07 17:18:36.626 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19402985274791718\n",
      "2021-09-07 17:18:36.626 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.799555778503418\n",
      "2021-09-07 17:18:36.627 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19402985274791718\n",
      "2021-09-07 17:18:36.628 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:36.630 | INFO     | src.policies:train:123 - Epoch 496 / 800\n",
      "2021-09-07 17:18:36.630 | INFO     | src.policies:collect_trajectories:221 - Episode 1312\n",
      "2021-09-07 17:18:36.659 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.660 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:36.660 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:36.662 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:36.665 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.09379442036151886, 'baseline_loss': 0.5410313010215759, 'total_loss': 0.1767212301492691}\n",
      "2021-09-07 17:18:36.665 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12088640034198761\n",
      "2021-09-07 17:18:36.666 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7457741498947144\n",
      "2021-09-07 17:18:36.667 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12088640034198761\n",
      "2021-09-07 17:18:36.668 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:36.670 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:36.671 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.04857888072729111, 'baseline_loss': 0.8167192339897156, 'total_loss': 0.3597807288169861}\n",
      "2021-09-07 17:18:36.672 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4567748010158539\n",
      "2021-09-07 17:18:36.672 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.20852530002594\n",
      "2021-09-07 17:18:36.673 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4567748010158539\n",
      "2021-09-07 17:18:36.675 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:36.676 | INFO     | src.policies:train:123 - Epoch 497 / 800\n",
      "2021-09-07 17:18:36.676 | INFO     | src.policies:collect_trajectories:221 - Episode 1313\n",
      "2021-09-07 17:18:36.706 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.707 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:36.707 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:36.709 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:36.712 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.26312288641929626, 'baseline_loss': 0.48953285813331604, 'total_loss': -0.018356457352638245}\n",
      "2021-09-07 17:18:36.712 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1179061159491539\n",
      "2021-09-07 17:18:36.713 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0269873142242432\n",
      "2021-09-07 17:18:36.714 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1179061159491539\n",
      "2021-09-07 17:18:36.715 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:36.716 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:36.717 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20117700099945068, 'baseline_loss': 0.44803035259246826, 'total_loss': 0.022838175296783447}\n",
      "2021-09-07 17:18:36.718 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25692859292030334\n",
      "2021-09-07 17:18:36.719 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.549384593963623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:36.720 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25692859292030334\n",
      "2021-09-07 17:18:36.721 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:36.723 | INFO     | src.policies:train:123 - Epoch 498 / 800\n",
      "2021-09-07 17:18:36.723 | INFO     | src.policies:collect_trajectories:221 - Episode 1314\n",
      "2021-09-07 17:18:36.753 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.754 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:36.754 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:36.756 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:36.759 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.14623822271823883, 'baseline_loss': 0.5569321513175964, 'total_loss': 0.1322278529405594}\n",
      "2021-09-07 17:18:36.760 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25923410058021545\n",
      "2021-09-07 17:18:36.761 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8597683906555176\n",
      "2021-09-07 17:18:36.793 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25923410058021545\n",
      "2021-09-07 17:18:36.819 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:36.821 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:36.822 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.05921083316206932, 'baseline_loss': 0.5886594653129578, 'total_loss': 0.23511889576911926}\n",
      "2021-09-07 17:18:36.824 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15610520541667938\n",
      "2021-09-07 17:18:36.825 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9976874589920044\n",
      "2021-09-07 17:18:36.826 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15610520541667938\n",
      "2021-09-07 17:18:36.828 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:36.829 | INFO     | src.policies:train:123 - Epoch 499 / 800\n",
      "2021-09-07 17:18:36.830 | INFO     | src.policies:collect_trajectories:221 - Episode 1315\n",
      "2021-09-07 17:18:36.855 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.855 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 183.0\n",
      "2021-09-07 17:18:36.856 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 183.0\n",
      "2021-09-07 17:18:36.856 | INFO     | src.policies:collect_trajectories:221 - Episode 1316\n",
      "2021-09-07 17:18:36.887 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.888 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:36.888 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 191.5\n",
      "2021-09-07 17:18:36.889 | WARNING  | src.policies:train:144 - The actual batch size is 383, instead of 200\n",
      "2021-09-07 17:18:36.892 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:36.895 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1729753613471985, 'baseline_loss': 0.46997231245040894, 'total_loss': 0.06201079487800598}\n",
      "2021-09-07 17:18:36.896 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22319720685482025\n",
      "2021-09-07 17:18:36.897 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3883394002914429\n",
      "2021-09-07 17:18:36.898 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22319720685482025\n",
      "2021-09-07 17:18:36.899 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:36.900 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:36.902 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.08524064719676971, 'baseline_loss': 0.47232571244239807, 'total_loss': 0.15092220902442932}\n",
      "2021-09-07 17:18:36.902 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10650630295276642\n",
      "2021-09-07 17:18:36.903 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6313847303390503\n",
      "2021-09-07 17:18:36.905 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10650630295276642\n",
      "2021-09-07 17:18:36.906 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:36.907 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:36.909 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23729731142520905, 'baseline_loss': 0.4681256115436554, 'total_loss': -0.0032345056533813477}\n",
      "2021-09-07 17:18:36.910 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23725923895835876\n",
      "2021-09-07 17:18:36.911 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2575082778930664\n",
      "2021-09-07 17:18:36.912 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23725923895835876\n",
      "2021-09-07 17:18:36.913 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:36.914 | INFO     | src.policies:train:123 - Epoch 500 / 800\n",
      "2021-09-07 17:18:36.915 | INFO     | src.policies:collect_trajectories:221 - Episode 1317\n",
      "2021-09-07 17:18:36.946 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.946 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:36.947 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:36.949 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:36.951 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4180925488471985, 'baseline_loss': 0.9593647122383118, 'total_loss': 0.0615898072719574}\n",
      "2021-09-07 17:18:36.952 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1025826558470726\n",
      "2021-09-07 17:18:36.954 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2637559175491333\n",
      "2021-09-07 17:18:36.955 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1025826558470726\n",
      "2021-09-07 17:18:36.956 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:36.957 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:36.958 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4954111576080322, 'baseline_loss': 1.1684293746948242, 'total_loss': 0.08880352973937988}\n",
      "2021-09-07 17:18:36.959 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.0963696613907814\n",
      "2021-09-07 17:18:36.960 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5155528783798218\n",
      "2021-09-07 17:18:36.962 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.0963696613907814\n",
      "2021-09-07 17:18:36.963 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:36.964 | INFO     | src.policies:train:123 - Epoch 501 / 800\n",
      "2021-09-07 17:18:36.965 | INFO     | src.policies:collect_trajectories:221 - Episode 1318\n",
      "2021-09-07 17:18:36.993 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:36.993 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:36.994 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:36.995 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:36.998 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.16689759492874146, 'baseline_loss': 0.4295600652694702, 'total_loss': 0.04788243770599365}\n",
      "2021-09-07 17:18:36.999 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15999653935432434\n",
      "2021-09-07 17:18:37.000 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.942746639251709\n",
      "2021-09-07 17:18:37.002 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15999653935432434\n",
      "2021-09-07 17:18:37.004 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:37.005 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:37.006 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1976129710674286, 'baseline_loss': 0.4042070806026459, 'total_loss': 0.004490569233894348}\n",
      "2021-09-07 17:18:37.007 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3622487783432007\n",
      "2021-09-07 17:18:37.008 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6816883087158203\n",
      "2021-09-07 17:18:37.009 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3622487783432007\n",
      "2021-09-07 17:18:37.010 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:37.012 | INFO     | src.policies:train:123 - Epoch 502 / 800\n",
      "2021-09-07 17:18:37.012 | INFO     | src.policies:collect_trajectories:221 - Episode 1319\n",
      "2021-09-07 17:18:37.159 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:37.159 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:37.160 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:37.162 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:37.164 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29674845933914185, 'baseline_loss': 0.5469975471496582, 'total_loss': -0.023249685764312744}\n",
      "2021-09-07 17:18:37.165 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11858449876308441\n",
      "2021-09-07 17:18:37.166 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1872525215148926\n",
      "2021-09-07 17:18:37.167 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11858449876308441\n",
      "2021-09-07 17:18:37.168 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:37.169 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:37.170 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24266992509365082, 'baseline_loss': 0.44806984066963196, 'total_loss': -0.01863500475883484}\n",
      "2021-09-07 17:18:37.171 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09135671705007553\n",
      "2021-09-07 17:18:37.171 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6633986234664917\n",
      "2021-09-07 17:18:37.172 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09135671705007553\n",
      "2021-09-07 17:18:37.173 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:37.175 | INFO     | src.policies:train:123 - Epoch 503 / 800\n",
      "2021-09-07 17:18:37.175 | INFO     | src.policies:collect_trajectories:221 - Episode 1320\n",
      "2021-09-07 17:18:37.204 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:37.205 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:37.206 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:37.208 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:37.210 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.08307873457670212, 'baseline_loss': 0.6267754435539246, 'total_loss': 0.23030897974967957}\n",
      "2021-09-07 17:18:37.211 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16478495299816132\n",
      "2021-09-07 17:18:37.212 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4901875257492065\n",
      "2021-09-07 17:18:37.213 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16478495299816132\n",
      "2021-09-07 17:18:37.214 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:37.215 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:37.216 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.11976312845945358, 'baseline_loss': 0.573774516582489, 'total_loss': 0.16712412238121033}\n",
      "2021-09-07 17:18:37.217 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2460709661245346\n",
      "2021-09-07 17:18:37.218 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5424388647079468\n",
      "2021-09-07 17:18:37.219 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2460709661245346\n",
      "2021-09-07 17:18:37.220 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:37.221 | INFO     | src.policies:train:123 - Epoch 504 / 800\n",
      "2021-09-07 17:18:37.222 | INFO     | src.policies:collect_trajectories:221 - Episode 1321\n",
      "2021-09-07 17:18:37.250 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:37.251 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:37.251 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:37.253 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:37.256 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.19835281372070312, 'baseline_loss': 0.379710853099823, 'total_loss': -0.008497387170791626}\n",
      "2021-09-07 17:18:37.257 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08978353440761566\n",
      "2021-09-07 17:18:37.258 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1973963975906372\n",
      "2021-09-07 17:18:37.259 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08978353440761566\n",
      "2021-09-07 17:18:37.260 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:37.261 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:37.262 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.22898919880390167, 'baseline_loss': 0.4094057083129883, 'total_loss': -0.024286344647407532}\n",
      "2021-09-07 17:18:37.263 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2832590639591217\n",
      "2021-09-07 17:18:37.264 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.302200198173523\n",
      "2021-09-07 17:18:37.265 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2832590639591217\n",
      "2021-09-07 17:18:37.266 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:37.267 | INFO     | src.policies:train:123 - Epoch 505 / 800\n",
      "2021-09-07 17:18:37.268 | INFO     | src.policies:collect_trajectories:221 - Episode 1322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:37.296 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:37.296 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:37.297 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:37.300 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:37.302 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42909935116767883, 'baseline_loss': 0.9432332515716553, 'total_loss': 0.042517274618148804}\n",
      "2021-09-07 17:18:37.303 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14085087180137634\n",
      "2021-09-07 17:18:37.304 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4099324941635132\n",
      "2021-09-07 17:18:37.305 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14085087180137634\n",
      "2021-09-07 17:18:37.306 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:37.308 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:37.309 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5051820874214172, 'baseline_loss': 1.134155511856079, 'total_loss': 0.061895668506622314}\n",
      "2021-09-07 17:18:37.310 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31008926033973694\n",
      "2021-09-07 17:18:37.311 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.0730645656585693\n",
      "2021-09-07 17:18:37.312 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31008926033973694\n",
      "2021-09-07 17:18:37.313 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:37.314 | INFO     | src.policies:train:123 - Epoch 506 / 800\n",
      "2021-09-07 17:18:37.315 | INFO     | src.policies:collect_trajectories:221 - Episode 1323\n",
      "2021-09-07 17:18:37.386 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:37.387 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:37.387 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:37.390 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:37.392 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2624529004096985, 'baseline_loss': 0.3645927309989929, 'total_loss': -0.08015653491020203}\n",
      "2021-09-07 17:18:37.392 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2680361568927765\n",
      "2021-09-07 17:18:37.393 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1064906120300293\n",
      "2021-09-07 17:18:37.395 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2680361568927765\n",
      "2021-09-07 17:18:37.396 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:37.397 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:37.398 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2949473559856415, 'baseline_loss': 0.435202419757843, 'total_loss': -0.07734614610671997}\n",
      "2021-09-07 17:18:37.399 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.291072815656662\n",
      "2021-09-07 17:18:37.400 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.780720591545105\n",
      "2021-09-07 17:18:37.401 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.291072815656662\n",
      "2021-09-07 17:18:37.402 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:37.404 | INFO     | src.policies:train:123 - Epoch 507 / 800\n",
      "2021-09-07 17:18:37.404 | INFO     | src.policies:collect_trajectories:221 - Episode 1324\n",
      "2021-09-07 17:18:37.434 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:37.434 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:37.435 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:37.436 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:37.440 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2314426451921463, 'baseline_loss': 0.3753506541252136, 'total_loss': -0.04376731812953949}\n",
      "2021-09-07 17:18:37.441 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08997994661331177\n",
      "2021-09-07 17:18:37.442 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.021910309791565\n",
      "2021-09-07 17:18:37.443 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08997994661331177\n",
      "2021-09-07 17:18:37.445 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:37.446 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:37.447 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24408628046512604, 'baseline_loss': 0.3880472183227539, 'total_loss': -0.050062671303749084}\n",
      "2021-09-07 17:18:37.448 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07435687631368637\n",
      "2021-09-07 17:18:37.449 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3323651552200317\n",
      "2021-09-07 17:18:37.450 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07435687631368637\n",
      "2021-09-07 17:18:37.451 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:37.452 | INFO     | src.policies:train:123 - Epoch 508 / 800\n",
      "2021-09-07 17:18:37.453 | INFO     | src.policies:collect_trajectories:221 - Episode 1325\n",
      "2021-09-07 17:18:37.482 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:37.483 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:37.483 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:37.485 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:37.487 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3423447906970978, 'baseline_loss': 0.5993952751159668, 'total_loss': -0.04264715313911438}\n",
      "2021-09-07 17:18:37.489 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17705278098583221\n",
      "2021-09-07 17:18:37.490 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3689228296279907\n",
      "2021-09-07 17:18:37.491 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17705278098583221\n",
      "2021-09-07 17:18:37.492 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3689228296279907\n",
      "2021-09-07 17:18:37.493 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:37.494 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4631914794445038, 'baseline_loss': 0.7668542265892029, 'total_loss': -0.07976436614990234}\n",
      "2021-09-07 17:18:37.495 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21844364702701569\n",
      "2021-09-07 17:18:37.496 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7418003678321838\n",
      "2021-09-07 17:18:37.497 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21844364702701569\n",
      "2021-09-07 17:18:37.498 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:37.499 | INFO     | src.policies:train:123 - Epoch 509 / 800\n",
      "2021-09-07 17:18:37.500 | INFO     | src.policies:collect_trajectories:221 - Episode 1326\n",
      "2021-09-07 17:18:37.529 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:37.530 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:37.530 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:37.532 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:37.535 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.058391470462083817, 'baseline_loss': 0.5033005475997925, 'total_loss': 0.19325880706310272}\n",
      "2021-09-07 17:18:37.536 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11148972064256668\n",
      "2021-09-07 17:18:37.537 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.886231541633606\n",
      "2021-09-07 17:18:37.538 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11148972064256668\n",
      "2021-09-07 17:18:37.539 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:37.540 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:37.541 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.14710666239261627, 'baseline_loss': 0.4831322431564331, 'total_loss': 0.09445945918560028}\n",
      "2021-09-07 17:18:37.542 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4768526256084442\n",
      "2021-09-07 17:18:37.543 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5782958269119263\n",
      "2021-09-07 17:18:37.544 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4768526256084442\n",
      "2021-09-07 17:18:37.545 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:37.546 | INFO     | src.policies:train:123 - Epoch 510 / 800\n",
      "2021-09-07 17:18:37.547 | INFO     | src.policies:collect_trajectories:221 - Episode 1327\n",
      "2021-09-07 17:18:37.575 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:37.575 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:37.576 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:37.578 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:37.580 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45120877027511597, 'baseline_loss': 0.9484072923660278, 'total_loss': 0.02299487590789795}\n",
      "2021-09-07 17:18:37.581 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3294964134693146\n",
      "2021-09-07 17:18:37.582 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2299045324325562\n",
      "2021-09-07 17:18:37.583 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3294964134693146\n",
      "2021-09-07 17:18:37.584 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:37.586 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:37.587 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3921368718147278, 'baseline_loss': 0.7310483455657959, 'total_loss': -0.026612699031829834}\n",
      "2021-09-07 17:18:37.588 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11616937071084976\n",
      "2021-09-07 17:18:37.589 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1576074361801147\n",
      "2021-09-07 17:18:37.590 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11616937071084976\n",
      "2021-09-07 17:18:37.591 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:37.592 | INFO     | src.policies:train:123 - Epoch 511 / 800\n",
      "2021-09-07 17:18:37.592 | INFO     | src.policies:collect_trajectories:221 - Episode 1328\n",
      "2021-09-07 17:18:37.622 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:37.623 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:37.623 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:37.625 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:37.628 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3914634585380554, 'baseline_loss': 0.6511020660400391, 'total_loss': -0.06591242551803589}\n",
      "2021-09-07 17:18:37.629 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11649852246046066\n",
      "2021-09-07 17:18:37.630 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8686009049415588\n",
      "2021-09-07 17:18:37.631 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11649852246046066\n",
      "2021-09-07 17:18:37.632 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:37.633 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:37.634 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36559608578681946, 'baseline_loss': 0.6174934506416321, 'total_loss': -0.05684936046600342}\n",
      "2021-09-07 17:18:37.635 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.38504791259765625\n",
      "2021-09-07 17:18:37.636 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5658185482025146\n",
      "2021-09-07 17:18:37.637 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.38504791259765625\n",
      "2021-09-07 17:18:37.638 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:37.639 | INFO     | src.policies:train:123 - Epoch 512 / 800\n",
      "2021-09-07 17:18:37.640 | INFO     | src.policies:collect_trajectories:221 - Episode 1329\n",
      "2021-09-07 17:18:37.670 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:37.670 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:37.671 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:37.675 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:37.677 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5192036628723145, 'baseline_loss': 1.4179420471191406, 'total_loss': 0.18976736068725586}\n",
      "2021-09-07 17:18:37.678 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1922091841697693\n",
      "2021-09-07 17:18:37.679 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.266491413116455\n",
      "2021-09-07 17:18:37.680 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1922091841697693\n",
      "2021-09-07 17:18:37.681 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:37.683 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:37.684 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6031035780906677, 'baseline_loss': 1.2752861976623535, 'total_loss': 0.03453952074050903}\n",
      "2021-09-07 17:18:37.685 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.610246479511261\n",
      "2021-09-07 17:18:37.686 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.461503505706787\n",
      "2021-09-07 17:18:37.687 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999913573265076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:37.688 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:37.689 | INFO     | src.policies:train:123 - Epoch 513 / 800\n",
      "2021-09-07 17:18:37.690 | INFO     | src.policies:collect_trajectories:221 - Episode 1330\n",
      "2021-09-07 17:18:37.718 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:37.719 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:37.719 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:37.723 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:37.726 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5639148950576782, 'baseline_loss': 0.9142826199531555, 'total_loss': -0.10677358508110046}\n",
      "2021-09-07 17:18:37.727 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.37325596809387207\n",
      "2021-09-07 17:18:37.728 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8058521151542664\n",
      "2021-09-07 17:18:37.730 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.37325596809387207\n",
      "2021-09-07 17:18:37.731 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:37.733 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:37.734 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45298686623573303, 'baseline_loss': 0.8053874373435974, 'total_loss': -0.050293147563934326}\n",
      "2021-09-07 17:18:37.736 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.47340163588523865\n",
      "2021-09-07 17:18:37.737 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9065651297569275\n",
      "2021-09-07 17:18:37.739 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.47340163588523865\n",
      "2021-09-07 17:18:37.740 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:37.742 | INFO     | src.policies:train:123 - Epoch 514 / 800\n",
      "2021-09-07 17:18:37.743 | INFO     | src.policies:collect_trajectories:221 - Episode 1331\n",
      "2021-09-07 17:18:37.776 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:37.777 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:37.777 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:37.779 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:37.781 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6920675039291382, 'baseline_loss': 1.5647940635681152, 'total_loss': 0.09032952785491943}\n",
      "2021-09-07 17:18:37.783 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5448582768440247\n",
      "2021-09-07 17:18:37.784 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.5482335090637207\n",
      "2021-09-07 17:18:37.785 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:37.786 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:37.787 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:37.789 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5629391670227051, 'baseline_loss': 1.4854391813278198, 'total_loss': 0.17978042364120483}\n",
      "2021-09-07 17:18:37.790 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21272969245910645\n",
      "2021-09-07 17:18:37.792 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.351604700088501\n",
      "2021-09-07 17:18:37.793 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21272969245910645\n",
      "2021-09-07 17:18:37.794 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:37.795 | INFO     | src.policies:train:123 - Epoch 515 / 800\n",
      "2021-09-07 17:18:37.795 | INFO     | src.policies:collect_trajectories:221 - Episode 1332\n",
      "2021-09-07 17:18:37.825 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:37.826 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:37.826 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:37.828 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:37.831 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.9337195754051208, 'baseline_loss': 3.4781110286712646, 'total_loss': 0.8053359389305115}\n",
      "2021-09-07 17:18:37.832 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21603131294250488\n",
      "2021-09-07 17:18:37.833 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 5.351903438568115\n",
      "2021-09-07 17:18:37.834 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21603131294250488\n",
      "2021-09-07 17:18:37.835 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:37.836 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:37.837 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.9257338643074036, 'baseline_loss': 3.368088960647583, 'total_loss': 0.7583106160163879}\n",
      "2021-09-07 17:18:37.838 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.360906183719635\n",
      "2021-09-07 17:18:37.839 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 5.202143669128418\n",
      "2021-09-07 17:18:37.840 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.360906183719635\n",
      "2021-09-07 17:18:37.842 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:37.843 | INFO     | src.policies:train:123 - Epoch 516 / 800\n",
      "2021-09-07 17:18:37.844 | INFO     | src.policies:collect_trajectories:221 - Episode 1333\n",
      "2021-09-07 17:18:37.925 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:37.925 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:37.926 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:37.927 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:37.930 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5373240113258362, 'baseline_loss': 1.463240385055542, 'total_loss': 0.19429618120193481}\n",
      "2021-09-07 17:18:37.931 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14022044837474823\n",
      "2021-09-07 17:18:37.932 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.2874271869659424\n",
      "2021-09-07 17:18:37.933 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14022044837474823\n",
      "2021-09-07 17:18:37.934 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:37.935 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:37.936 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5039131045341492, 'baseline_loss': 1.439369797706604, 'total_loss': 0.21577179431915283}\n",
      "2021-09-07 17:18:37.937 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2904076874256134\n",
      "2021-09-07 17:18:37.938 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.7722721099853516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:37.939 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2904076874256134\n",
      "2021-09-07 17:18:37.940 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:37.942 | INFO     | src.policies:train:123 - Epoch 517 / 800\n",
      "2021-09-07 17:18:37.943 | INFO     | src.policies:collect_trajectories:221 - Episode 1334\n",
      "2021-09-07 17:18:37.972 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:37.973 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:37.973 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:37.975 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:37.978 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6502429246902466, 'baseline_loss': 2.0584049224853516, 'total_loss': 0.3789595365524292}\n",
      "2021-09-07 17:18:37.979 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30450913310050964\n",
      "2021-09-07 17:18:37.980 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.112776517868042\n",
      "2021-09-07 17:18:37.982 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30450913310050964\n",
      "2021-09-07 17:18:37.983 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:37.985 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:37.986 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6680796146392822, 'baseline_loss': 2.031851291656494, 'total_loss': 0.34784603118896484}\n",
      "2021-09-07 17:18:37.987 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.630585253238678\n",
      "2021-09-07 17:18:37.988 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.380734443664551\n",
      "2021-09-07 17:18:37.989 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:37.990 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:37.992 | INFO     | src.policies:train:123 - Epoch 518 / 800\n",
      "2021-09-07 17:18:37.992 | INFO     | src.policies:collect_trajectories:221 - Episode 1335\n",
      "2021-09-07 17:18:38.022 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.022 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:38.023 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:38.024 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:38.027 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5867897272109985, 'baseline_loss': 1.5158251523971558, 'total_loss': 0.17112284898757935}\n",
      "2021-09-07 17:18:38.029 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2605259418487549\n",
      "2021-09-07 17:18:38.029 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.4125540256500244\n",
      "2021-09-07 17:18:38.030 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2605259418487549\n",
      "2021-09-07 17:18:38.031 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:38.033 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:38.034 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5807346105575562, 'baseline_loss': 1.5138678550720215, 'total_loss': 0.1761993169784546}\n",
      "2021-09-07 17:18:38.035 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18642033636569977\n",
      "2021-09-07 17:18:38.035 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.104557752609253\n",
      "2021-09-07 17:18:38.037 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18642033636569977\n",
      "2021-09-07 17:18:38.038 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:38.039 | INFO     | src.policies:train:123 - Epoch 519 / 800\n",
      "2021-09-07 17:18:38.040 | INFO     | src.policies:collect_trajectories:221 - Episode 1336\n",
      "2021-09-07 17:18:38.069 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.069 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:38.070 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:38.072 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:38.074 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.675987720489502, 'baseline_loss': 2.4345335960388184, 'total_loss': 0.5412790775299072}\n",
      "2021-09-07 17:18:38.075 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.752802312374115\n",
      "2021-09-07 17:18:38.076 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.81573486328125\n",
      "2021-09-07 17:18:38.077 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:38.078 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:38.079 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:38.080 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5744608640670776, 'baseline_loss': 2.4082016944885254, 'total_loss': 0.6296399831771851}\n",
      "2021-09-07 17:18:38.081 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3286774754524231\n",
      "2021-09-07 17:18:38.082 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.7258687019348145\n",
      "2021-09-07 17:18:38.083 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3286774754524231\n",
      "2021-09-07 17:18:38.084 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:38.085 | INFO     | src.policies:train:123 - Epoch 520 / 800\n",
      "2021-09-07 17:18:38.086 | INFO     | src.policies:collect_trajectories:221 - Episode 1337\n",
      "2021-09-07 17:18:38.114 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.115 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:38.115 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:38.117 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:38.119 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6050065755844116, 'baseline_loss': 1.6425209045410156, 'total_loss': 0.2162538766860962}\n",
      "2021-09-07 17:18:38.120 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3039417266845703\n",
      "2021-09-07 17:18:38.121 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.6428310871124268\n",
      "2021-09-07 17:18:38.122 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3039417266845703\n",
      "2021-09-07 17:18:38.123 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999999403953552\n",
      "2021-09-07 17:18:38.124 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:38.126 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5905430316925049, 'baseline_loss': 1.4644490480422974, 'total_loss': 0.1416814923286438}\n",
      "2021-09-07 17:18:38.126 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31842276453971863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:38.127 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9533582925796509\n",
      "2021-09-07 17:18:38.128 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31842276453971863\n",
      "2021-09-07 17:18:38.129 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:38.131 | INFO     | src.policies:train:123 - Epoch 521 / 800\n",
      "2021-09-07 17:18:38.131 | INFO     | src.policies:collect_trajectories:221 - Episode 1338\n",
      "2021-09-07 17:18:38.140 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.141 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 53.0\n",
      "2021-09-07 17:18:38.141 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 53.0\n",
      "2021-09-07 17:18:38.141 | INFO     | src.policies:collect_trajectories:221 - Episode 1339\n",
      "2021-09-07 17:18:38.162 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.163 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 131.0\n",
      "2021-09-07 17:18:38.163 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 92.0\n",
      "2021-09-07 17:18:38.164 | INFO     | src.policies:collect_trajectories:221 - Episode 1340\n",
      "2021-09-07 17:18:38.191 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.192 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 158.0\n",
      "2021-09-07 17:18:38.192 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 114.0\n",
      "2021-09-07 17:18:38.193 | WARNING  | src.policies:train:144 - The actual batch size is 342, instead of 200\n",
      "2021-09-07 17:18:38.196 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:38.197 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42827174067497253, 'baseline_loss': 0.6792863607406616, 'total_loss': -0.08862856030464172}\n",
      "2021-09-07 17:18:38.198 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21304497122764587\n",
      "2021-09-07 17:18:38.199 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.831207275390625\n",
      "2021-09-07 17:18:38.200 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21304497122764587\n",
      "2021-09-07 17:18:38.201 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:38.203 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:38.204 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31210964918136597, 'baseline_loss': 0.6531866192817688, 'total_loss': 0.014483660459518433}\n",
      "2021-09-07 17:18:38.205 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22401230037212372\n",
      "2021-09-07 17:18:38.205 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2383135557174683\n",
      "2021-09-07 17:18:38.206 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22401230037212372\n",
      "2021-09-07 17:18:38.207 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:38.208 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:38.209 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27473291754722595, 'baseline_loss': 0.49310287833213806, 'total_loss': -0.02818147838115692}\n",
      "2021-09-07 17:18:38.210 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3955416679382324\n",
      "2021-09-07 17:18:38.211 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9936193227767944\n",
      "2021-09-07 17:18:38.212 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3955416679382324\n",
      "2021-09-07 17:18:38.213 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:38.214 | INFO     | src.policies:train:123 - Epoch 522 / 800\n",
      "2021-09-07 17:18:38.215 | INFO     | src.policies:collect_trajectories:221 - Episode 1341\n",
      "2021-09-07 17:18:38.231 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.232 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 104.0\n",
      "2021-09-07 17:18:38.233 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 104.0\n",
      "2021-09-07 17:18:38.233 | INFO     | src.policies:collect_trajectories:221 - Episode 1342\n",
      "2021-09-07 17:18:38.262 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.263 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:38.263 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 152.0\n",
      "2021-09-07 17:18:38.264 | WARNING  | src.policies:train:144 - The actual batch size is 304, instead of 200\n",
      "2021-09-07 17:18:38.267 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:38.269 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.09768391400575638, 'baseline_loss': 0.7190843224525452, 'total_loss': 0.2618582546710968}\n",
      "2021-09-07 17:18:38.270 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2559660077095032\n",
      "2021-09-07 17:18:38.271 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6294832229614258\n",
      "2021-09-07 17:18:38.272 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2559660077095032\n",
      "2021-09-07 17:18:38.273 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:38.274 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:38.275 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.22406817972660065, 'baseline_loss': 0.9228862524032593, 'total_loss': 0.237374946475029}\n",
      "2021-09-07 17:18:38.276 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4206809401512146\n",
      "2021-09-07 17:18:38.276 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.025132417678833\n",
      "2021-09-07 17:18:38.277 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4206809401512146\n",
      "2021-09-07 17:18:38.279 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:38.280 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:38.281 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.30493462085723877, 'baseline_loss': 0.9627217054367065, 'total_loss': 0.1764262318611145}\n",
      "2021-09-07 17:18:38.282 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3555934727191925\n",
      "2021-09-07 17:18:38.283 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.144324779510498\n",
      "2021-09-07 17:18:38.284 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3555934727191925\n",
      "2021-09-07 17:18:38.285 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:38.286 | INFO     | src.policies:train:123 - Epoch 523 / 800\n",
      "2021-09-07 17:18:38.287 | INFO     | src.policies:collect_trajectories:221 - Episode 1343\n",
      "2021-09-07 17:18:38.316 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.317 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:38.317 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:38.319 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:38.322 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.11898521333932877, 'baseline_loss': 0.4788737893104553, 'total_loss': 0.1204516813158989}\n",
      "2021-09-07 17:18:38.322 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24109013378620148\n",
      "2021-09-07 17:18:38.324 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5099471807479858\n",
      "2021-09-07 17:18:38.325 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24109013378620148\n",
      "2021-09-07 17:18:38.326 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:38.327 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:38.328 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20087888836860657, 'baseline_loss': 0.5130429267883301, 'total_loss': 0.05564257502555847}\n",
      "2021-09-07 17:18:38.329 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15921318531036377\n",
      "2021-09-07 17:18:38.330 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.555138111114502\n",
      "2021-09-07 17:18:38.331 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15921318531036377\n",
      "2021-09-07 17:18:38.332 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:38.334 | INFO     | src.policies:train:123 - Epoch 524 / 800\n",
      "2021-09-07 17:18:38.334 | INFO     | src.policies:collect_trajectories:221 - Episode 1344\n",
      "2021-09-07 17:18:38.364 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.364 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:38.365 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:38.367 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:38.369 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32826101779937744, 'baseline_loss': 0.6497154831886292, 'total_loss': -0.003403276205062866}\n",
      "2021-09-07 17:18:38.370 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3281683623790741\n",
      "2021-09-07 17:18:38.371 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6342575550079346\n",
      "2021-09-07 17:18:38.373 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3281683623790741\n",
      "2021-09-07 17:18:38.374 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:38.375 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:38.376 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2740224301815033, 'baseline_loss': 0.6117537021636963, 'total_loss': 0.03185442090034485}\n",
      "2021-09-07 17:18:38.377 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18836365640163422\n",
      "2021-09-07 17:18:38.378 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4921085834503174\n",
      "2021-09-07 17:18:38.379 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18836365640163422\n",
      "2021-09-07 17:18:38.380 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:38.381 | INFO     | src.policies:train:123 - Epoch 525 / 800\n",
      "2021-09-07 17:18:38.381 | INFO     | src.policies:collect_trajectories:221 - Episode 1345\n",
      "2021-09-07 17:18:38.403 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.404 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 142.0\n",
      "2021-09-07 17:18:38.404 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 142.0\n",
      "2021-09-07 17:18:38.405 | INFO     | src.policies:collect_trajectories:221 - Episode 1346\n",
      "2021-09-07 17:18:38.590 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.591 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:38.592 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 171.0\n",
      "2021-09-07 17:18:38.592 | WARNING  | src.policies:train:144 - The actual batch size is 342, instead of 200\n",
      "2021-09-07 17:18:38.595 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:38.597 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41250813007354736, 'baseline_loss': 0.6369138360023499, 'total_loss': -0.09405121207237244}\n",
      "2021-09-07 17:18:38.598 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4110206365585327\n",
      "2021-09-07 17:18:38.599 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0483672618865967\n",
      "2021-09-07 17:18:38.600 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4110206365585327\n",
      "2021-09-07 17:18:38.601 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:38.602 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:38.603 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3246665298938751, 'baseline_loss': 0.7175176739692688, 'total_loss': 0.03409230709075928}\n",
      "2021-09-07 17:18:38.604 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10642322152853012\n",
      "2021-09-07 17:18:38.605 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0259873867034912\n",
      "2021-09-07 17:18:38.607 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10642322152853012\n",
      "2021-09-07 17:18:38.608 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:38.609 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:38.610 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4020119905471802, 'baseline_loss': 0.797242283821106, 'total_loss': -0.0033908486366271973}\n",
      "2021-09-07 17:18:38.611 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15570665895938873\n",
      "2021-09-07 17:18:38.612 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8838688135147095\n",
      "2021-09-07 17:18:38.613 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15570665895938873\n",
      "2021-09-07 17:18:38.614 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:38.615 | INFO     | src.policies:train:123 - Epoch 526 / 800\n",
      "2021-09-07 17:18:38.615 | INFO     | src.policies:collect_trajectories:221 - Episode 1347\n",
      "2021-09-07 17:18:38.641 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.641 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 178.0\n",
      "2021-09-07 17:18:38.642 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 178.0\n",
      "2021-09-07 17:18:38.643 | INFO     | src.policies:collect_trajectories:221 - Episode 1348\n",
      "2021-09-07 17:18:38.673 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.674 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:38.674 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 189.0\n",
      "2021-09-07 17:18:38.675 | WARNING  | src.policies:train:144 - The actual batch size is 378, instead of 200\n",
      "2021-09-07 17:18:38.678 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:38.681 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.19598792493343353, 'baseline_loss': 0.6332249641418457, 'total_loss': 0.12062455713748932}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:38.682 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13892114162445068\n",
      "2021-09-07 17:18:38.683 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5000180006027222\n",
      "2021-09-07 17:18:38.684 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13892114162445068\n",
      "2021-09-07 17:18:38.685 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:38.686 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:38.687 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3052327036857605, 'baseline_loss': 0.6739030480384827, 'total_loss': 0.031718820333480835}\n",
      "2021-09-07 17:18:38.688 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3018686771392822\n",
      "2021-09-07 17:18:38.689 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2500495910644531\n",
      "2021-09-07 17:18:38.690 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3018686771392822\n",
      "2021-09-07 17:18:38.690 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:38.692 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:38.693 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.19714727997779846, 'baseline_loss': 0.6631935238838196, 'total_loss': 0.13444948196411133}\n",
      "2021-09-07 17:18:38.693 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23243649303913116\n",
      "2021-09-07 17:18:38.694 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5404837131500244\n",
      "2021-09-07 17:18:38.695 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23243649303913116\n",
      "2021-09-07 17:18:38.696 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:38.697 | INFO     | src.policies:train:123 - Epoch 527 / 800\n",
      "2021-09-07 17:18:38.698 | INFO     | src.policies:collect_trajectories:221 - Episode 1349\n",
      "2021-09-07 17:18:38.725 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.726 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:38.726 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:38.728 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:38.731 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.22281566262245178, 'baseline_loss': 0.40353021025657654, 'total_loss': -0.021050557494163513}\n",
      "2021-09-07 17:18:38.732 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3389146625995636\n",
      "2021-09-07 17:18:38.733 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8319520950317383\n",
      "2021-09-07 17:18:38.734 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3389146625995636\n",
      "2021-09-07 17:18:38.735 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:38.737 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:38.738 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.34494179487228394, 'baseline_loss': 0.421132355928421, 'total_loss': -0.13437561690807343}\n",
      "2021-09-07 17:18:38.739 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7991443872451782\n",
      "2021-09-07 17:18:38.739 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1229939460754395\n",
      "2021-09-07 17:18:38.740 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:38.741 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:38.743 | INFO     | src.policies:train:123 - Epoch 528 / 800\n",
      "2021-09-07 17:18:38.743 | INFO     | src.policies:collect_trajectories:221 - Episode 1350\n",
      "2021-09-07 17:18:38.772 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.773 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:38.773 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:38.775 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:38.777 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.44161146879196167, 'baseline_loss': 1.289218544960022, 'total_loss': 0.20299780368804932}\n",
      "2021-09-07 17:18:38.778 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15655001997947693\n",
      "2021-09-07 17:18:38.779 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9892276525497437\n",
      "2021-09-07 17:18:38.780 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15655001997947693\n",
      "2021-09-07 17:18:38.781 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:38.782 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:38.783 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5178602337837219, 'baseline_loss': 1.3593908548355103, 'total_loss': 0.1618351936340332}\n",
      "2021-09-07 17:18:38.784 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3050706386566162\n",
      "2021-09-07 17:18:38.785 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5747836828231812\n",
      "2021-09-07 17:18:38.786 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3050706386566162\n",
      "2021-09-07 17:18:38.787 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:38.789 | INFO     | src.policies:train:123 - Epoch 529 / 800\n",
      "2021-09-07 17:18:38.789 | INFO     | src.policies:collect_trajectories:221 - Episode 1351\n",
      "2021-09-07 17:18:38.822 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.822 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:38.823 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:38.825 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:38.827 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2790534198284149, 'baseline_loss': 0.4681258499622345, 'total_loss': -0.04499049484729767}\n",
      "2021-09-07 17:18:38.828 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14315125346183777\n",
      "2021-09-07 17:18:38.829 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4186657667160034\n",
      "2021-09-07 17:18:38.831 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14315125346183777\n",
      "2021-09-07 17:18:38.832 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:38.833 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:38.834 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.34215739369392395, 'baseline_loss': 0.5347961783409119, 'total_loss': -0.07475930452346802}\n",
      "2021-09-07 17:18:38.835 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2509658932685852\n",
      "2021-09-07 17:18:38.835 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2978904247283936\n",
      "2021-09-07 17:18:38.836 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2509658932685852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:38.837 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:38.839 | INFO     | src.policies:train:123 - Epoch 530 / 800\n",
      "2021-09-07 17:18:38.839 | INFO     | src.policies:collect_trajectories:221 - Episode 1352\n",
      "2021-09-07 17:18:38.870 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.871 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:38.871 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:38.873 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:38.876 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5183829665184021, 'baseline_loss': 1.0907915830612183, 'total_loss': 0.02701282501220703}\n",
      "2021-09-07 17:18:38.877 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1956808865070343\n",
      "2021-09-07 17:18:38.878 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8198119401931763\n",
      "2021-09-07 17:18:38.879 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1956808865070343\n",
      "2021-09-07 17:18:38.881 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:38.882 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:38.884 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6022546887397766, 'baseline_loss': 1.0997223854064941, 'total_loss': -0.05239349603652954}\n",
      "2021-09-07 17:18:38.885 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.28420308232307434\n",
      "2021-09-07 17:18:38.886 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4128940105438232\n",
      "2021-09-07 17:18:38.887 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.28420308232307434\n",
      "2021-09-07 17:18:38.888 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:38.889 | INFO     | src.policies:train:123 - Epoch 531 / 800\n",
      "2021-09-07 17:18:38.889 | INFO     | src.policies:collect_trajectories:221 - Episode 1353\n",
      "2021-09-07 17:18:38.919 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.920 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:38.921 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:38.925 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:38.927 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4139358401298523, 'baseline_loss': 0.828959584236145, 'total_loss': 0.0005439519882202148}\n",
      "2021-09-07 17:18:38.928 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15683293342590332\n",
      "2021-09-07 17:18:38.930 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3295457363128662\n",
      "2021-09-07 17:18:38.931 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15683293342590332\n",
      "2021-09-07 17:18:38.933 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:38.934 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:38.935 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.44725754857063293, 'baseline_loss': 0.9736663103103638, 'total_loss': 0.03957560658454895}\n",
      "2021-09-07 17:18:38.936 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2532612085342407\n",
      "2021-09-07 17:18:38.937 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.434634804725647\n",
      "2021-09-07 17:18:38.938 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2532612085342407\n",
      "2021-09-07 17:18:38.939 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:38.941 | INFO     | src.policies:train:123 - Epoch 532 / 800\n",
      "2021-09-07 17:18:38.942 | INFO     | src.policies:collect_trajectories:221 - Episode 1354\n",
      "2021-09-07 17:18:38.971 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:38.972 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 194.0\n",
      "2021-09-07 17:18:38.972 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 194.0\n",
      "2021-09-07 17:18:38.972 | INFO     | src.policies:collect_trajectories:221 - Episode 1355\n",
      "2021-09-07 17:18:39.004 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:39.004 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:39.005 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 197.0\n",
      "2021-09-07 17:18:39.005 | WARNING  | src.policies:train:144 - The actual batch size is 394, instead of 200\n",
      "2021-09-07 17:18:39.008 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:39.010 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2649913430213928, 'baseline_loss': 0.615611732006073, 'total_loss': 0.04281452298164368}\n",
      "2021-09-07 17:18:39.011 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.27037352323532104\n",
      "2021-09-07 17:18:39.012 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.616815447807312\n",
      "2021-09-07 17:18:39.013 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.27037352323532104\n",
      "2021-09-07 17:18:39.014 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:39.015 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:39.017 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4095495641231537, 'baseline_loss': 0.6114897727966309, 'total_loss': -0.10380467772483826}\n",
      "2021-09-07 17:18:39.017 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21860019862651825\n",
      "2021-09-07 17:18:39.018 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5680751800537109\n",
      "2021-09-07 17:18:39.020 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21860019862651825\n",
      "2021-09-07 17:18:39.021 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:39.022 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:39.023 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.295596718788147, 'baseline_loss': 0.7010254859924316, 'total_loss': 0.05491602420806885}\n",
      "2021-09-07 17:18:39.024 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12792392075061798\n",
      "2021-09-07 17:18:39.025 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.49889498949050903\n",
      "2021-09-07 17:18:39.026 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12792392075061798\n",
      "2021-09-07 17:18:39.027 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49889498949050903\n",
      "2021-09-07 17:18:39.028 | INFO     | src.policies:train:123 - Epoch 533 / 800\n",
      "2021-09-07 17:18:39.029 | INFO     | src.policies:collect_trajectories:221 - Episode 1356\n",
      "2021-09-07 17:18:39.058 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:39.059 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:39.059 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:39.060 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:39.063 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.18301011621952057, 'baseline_loss': 0.436050683259964, 'total_loss': 0.035015225410461426}\n",
      "2021-09-07 17:18:39.064 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6024095416069031\n",
      "2021-09-07 17:18:39.065 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.414191722869873\n",
      "2021-09-07 17:18:39.066 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:39.067 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:39.068 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:39.069 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1980394423007965, 'baseline_loss': 0.390633761882782, 'total_loss': -0.0027225613594055176}\n",
      "2021-09-07 17:18:39.069 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1490706205368042\n",
      "2021-09-07 17:18:39.118 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1346416473388672\n",
      "2021-09-07 17:18:39.135 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1490706205368042\n",
      "2021-09-07 17:18:39.136 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:39.138 | INFO     | src.policies:train:123 - Epoch 534 / 800\n",
      "2021-09-07 17:18:39.138 | INFO     | src.policies:collect_trajectories:221 - Episode 1357\n",
      "2021-09-07 17:18:39.282 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:39.283 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:39.283 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:39.285 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:39.288 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3208450675010681, 'baseline_loss': 0.4786112904548645, 'total_loss': -0.08153942227363586}\n",
      "2021-09-07 17:18:39.289 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13238291442394257\n",
      "2021-09-07 17:18:39.290 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2719745934009552\n",
      "2021-09-07 17:18:39.292 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13238291442394257\n",
      "2021-09-07 17:18:39.293 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2719745934009552\n",
      "2021-09-07 17:18:39.294 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:39.295 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.35238224267959595, 'baseline_loss': 0.45882460474967957, 'total_loss': -0.12296994030475616}\n",
      "2021-09-07 17:18:39.296 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18820656836032867\n",
      "2021-09-07 17:18:39.297 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8566992282867432\n",
      "2021-09-07 17:18:39.298 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18820656836032867\n",
      "2021-09-07 17:18:39.300 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:39.302 | INFO     | src.policies:train:123 - Epoch 535 / 800\n",
      "2021-09-07 17:18:39.302 | INFO     | src.policies:collect_trajectories:221 - Episode 1358\n",
      "2021-09-07 17:18:39.330 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:39.331 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:39.331 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:39.334 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:39.336 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4348534047603607, 'baseline_loss': 0.8425219655036926, 'total_loss': -0.013592422008514404}\n",
      "2021-09-07 17:18:39.337 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2698826789855957\n",
      "2021-09-07 17:18:39.338 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.942244291305542\n",
      "2021-09-07 17:18:39.339 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2698826789855957\n",
      "2021-09-07 17:18:39.340 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:39.341 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:39.343 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4693906009197235, 'baseline_loss': 0.7282359600067139, 'total_loss': -0.10527262091636658}\n",
      "2021-09-07 17:18:39.343 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6610419154167175\n",
      "2021-09-07 17:18:39.344 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5431687831878662\n",
      "2021-09-07 17:18:39.345 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:39.346 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999989867210388\n",
      "2021-09-07 17:18:39.348 | INFO     | src.policies:train:123 - Epoch 536 / 800\n",
      "2021-09-07 17:18:39.348 | INFO     | src.policies:collect_trajectories:221 - Episode 1359\n",
      "2021-09-07 17:18:39.376 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:39.377 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:39.377 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:39.380 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:39.383 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5235196948051453, 'baseline_loss': 1.704201102256775, 'total_loss': 0.3285808563232422}\n",
      "2021-09-07 17:18:39.384 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.46358203887939453\n",
      "2021-09-07 17:18:39.385 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.3404626846313477\n",
      "2021-09-07 17:18:39.386 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.46358203887939453\n",
      "2021-09-07 17:18:39.387 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:39.388 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:39.389 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4855895936489105, 'baseline_loss': 1.6861134767532349, 'total_loss': 0.3574671447277069}\n",
      "2021-09-07 17:18:39.390 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17431677877902985\n",
      "2021-09-07 17:18:39.391 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.654794931411743\n",
      "2021-09-07 17:18:39.392 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17431677877902985\n",
      "2021-09-07 17:18:39.393 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:39.394 | INFO     | src.policies:train:123 - Epoch 537 / 800\n",
      "2021-09-07 17:18:39.394 | INFO     | src.policies:collect_trajectories:221 - Episode 1360\n",
      "2021-09-07 17:18:39.422 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:39.422 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:39.423 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:39.425 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:39.428 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6816398501396179, 'baseline_loss': 2.143871545791626, 'total_loss': 0.39029592275619507}\n",
      "2021-09-07 17:18:39.429 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5347380042076111\n",
      "2021-09-07 17:18:39.430 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.8587143421173096\n",
      "2021-09-07 17:18:39.431 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:39.432 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:39.434 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:39.435 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5404565334320068, 'baseline_loss': 1.9404603242874146, 'total_loss': 0.42977362871170044}\n",
      "2021-09-07 17:18:39.436 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.36414065957069397\n",
      "2021-09-07 17:18:39.437 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.900311231613159\n",
      "2021-09-07 17:18:39.438 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.36414065957069397\n",
      "2021-09-07 17:18:39.439 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:39.441 | INFO     | src.policies:train:123 - Epoch 538 / 800\n",
      "2021-09-07 17:18:39.441 | INFO     | src.policies:collect_trajectories:221 - Episode 1361\n",
      "2021-09-07 17:18:39.471 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:39.472 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:39.472 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:39.475 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:39.477 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.34672001004219055, 'baseline_loss': 0.6077700853347778, 'total_loss': -0.042834967374801636}\n",
      "2021-09-07 17:18:39.478 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30725541710853577\n",
      "2021-09-07 17:18:39.479 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1013997793197632\n",
      "2021-09-07 17:18:39.480 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30725541710853577\n",
      "2021-09-07 17:18:39.482 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:39.483 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:39.484 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29354751110076904, 'baseline_loss': 0.69117271900177, 'total_loss': 0.05203884840011597}\n",
      "2021-09-07 17:18:39.485 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31709304451942444\n",
      "2021-09-07 17:18:39.486 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.677706778049469\n",
      "2021-09-07 17:18:39.487 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31709304451942444\n",
      "2021-09-07 17:18:39.488 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:39.489 | INFO     | src.policies:train:123 - Epoch 539 / 800\n",
      "2021-09-07 17:18:39.490 | INFO     | src.policies:collect_trajectories:221 - Episode 1362\n",
      "2021-09-07 17:18:39.519 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:39.520 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:39.520 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:39.522 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:39.525 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3947353661060333, 'baseline_loss': 0.6587101221084595, 'total_loss': -0.06538030505180359}\n",
      "2021-09-07 17:18:39.526 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23180079460144043\n",
      "2021-09-07 17:18:39.527 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5613126158714294\n",
      "2021-09-07 17:18:39.528 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23180079460144043\n",
      "2021-09-07 17:18:39.529 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:39.531 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:39.532 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36654800176620483, 'baseline_loss': 0.5955641865730286, 'total_loss': -0.06876590847969055}\n",
      "2021-09-07 17:18:39.533 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1815178245306015\n",
      "2021-09-07 17:18:39.534 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4841229319572449\n",
      "2021-09-07 17:18:39.535 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1815178245306015\n",
      "2021-09-07 17:18:39.536 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4841229319572449\n",
      "2021-09-07 17:18:39.538 | INFO     | src.policies:train:123 - Epoch 540 / 800\n",
      "2021-09-07 17:18:39.538 | INFO     | src.policies:collect_trajectories:221 - Episode 1363\n",
      "2021-09-07 17:18:39.567 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:39.567 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:39.568 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:39.569 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:39.572 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.327412873506546, 'baseline_loss': 0.46353355050086975, 'total_loss': -0.09564609825611115}\n",
      "2021-09-07 17:18:39.573 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10847542434930801\n",
      "2021-09-07 17:18:39.573 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3985409736633301\n",
      "2021-09-07 17:18:39.575 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10847542434930801\n",
      "2021-09-07 17:18:39.576 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3985409736633301\n",
      "2021-09-07 17:18:39.577 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:39.578 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.34298163652420044, 'baseline_loss': 0.5203879475593567, 'total_loss': -0.0827876627445221}\n",
      "2021-09-07 17:18:39.579 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.27294641733169556\n",
      "2021-09-07 17:18:39.580 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.43345603346824646\n",
      "2021-09-07 17:18:39.581 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.27294641733169556\n",
      "2021-09-07 17:18:39.582 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.43345603346824646\n",
      "2021-09-07 17:18:39.583 | INFO     | src.policies:train:123 - Epoch 541 / 800\n",
      "2021-09-07 17:18:39.583 | INFO     | src.policies:collect_trajectories:221 - Episode 1364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:39.613 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:39.613 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:39.614 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:39.615 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:39.617 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5151874423027039, 'baseline_loss': 1.1617519855499268, 'total_loss': 0.06568855047225952}\n",
      "2021-09-07 17:18:39.618 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3044017255306244\n",
      "2021-09-07 17:18:39.619 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9316846132278442\n",
      "2021-09-07 17:18:39.620 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3044017255306244\n",
      "2021-09-07 17:18:39.621 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:39.622 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:39.623 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5654975771903992, 'baseline_loss': 1.3092968463897705, 'total_loss': 0.08915084600448608}\n",
      "2021-09-07 17:18:39.624 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21729691326618195\n",
      "2021-09-07 17:18:39.625 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.165235996246338\n",
      "2021-09-07 17:18:39.626 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21729691326618195\n",
      "2021-09-07 17:18:39.627 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:39.628 | INFO     | src.policies:train:123 - Epoch 542 / 800\n",
      "2021-09-07 17:18:39.629 | INFO     | src.policies:collect_trajectories:221 - Episode 1365\n",
      "2021-09-07 17:18:39.705 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:39.706 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:39.707 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:39.711 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:39.712 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3618948459625244, 'baseline_loss': 0.6303711533546448, 'total_loss': -0.046709269285202026}\n",
      "2021-09-07 17:18:39.713 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3113151490688324\n",
      "2021-09-07 17:18:39.715 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.599934995174408\n",
      "2021-09-07 17:18:39.716 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3113151490688324\n",
      "2021-09-07 17:18:39.717 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:39.719 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:39.720 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3357815146446228, 'baseline_loss': 0.711057722568512, 'total_loss': 0.01974734663963318}\n",
      "2021-09-07 17:18:39.721 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18338079750537872\n",
      "2021-09-07 17:18:39.722 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2870839834213257\n",
      "2021-09-07 17:18:39.724 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18338079750537872\n",
      "2021-09-07 17:18:39.725 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2870839834213257\n",
      "2021-09-07 17:18:39.726 | INFO     | src.policies:train:123 - Epoch 543 / 800\n",
      "2021-09-07 17:18:39.727 | INFO     | src.policies:collect_trajectories:221 - Episode 1366\n",
      "2021-09-07 17:18:39.758 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:39.759 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:39.759 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:39.761 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:39.763 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.22254501283168793, 'baseline_loss': 0.4137342870235443, 'total_loss': -0.01567786931991577}\n",
      "2021-09-07 17:18:39.764 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2312924861907959\n",
      "2021-09-07 17:18:39.765 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.505877137184143\n",
      "2021-09-07 17:18:39.766 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2312924861907959\n",
      "2021-09-07 17:18:39.767 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:39.768 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:39.770 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2540017068386078, 'baseline_loss': 0.444565087556839, 'total_loss': -0.03171916306018829}\n",
      "2021-09-07 17:18:39.771 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08619996905326843\n",
      "2021-09-07 17:18:39.772 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9679652452468872\n",
      "2021-09-07 17:18:39.773 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08619996905326843\n",
      "2021-09-07 17:18:39.774 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:39.775 | INFO     | src.policies:train:123 - Epoch 544 / 800\n",
      "2021-09-07 17:18:39.776 | INFO     | src.policies:collect_trajectories:221 - Episode 1367\n",
      "2021-09-07 17:18:39.787 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:39.788 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 75.0\n",
      "2021-09-07 17:18:39.788 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 75.0\n",
      "2021-09-07 17:18:39.788 | INFO     | src.policies:collect_trajectories:221 - Episode 1368\n",
      "2021-09-07 17:18:39.816 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:39.816 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 188.0\n",
      "2021-09-07 17:18:39.817 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 131.5\n",
      "2021-09-07 17:18:39.817 | WARNING  | src.policies:train:144 - The actual batch size is 263, instead of 200\n",
      "2021-09-07 17:18:39.820 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:39.824 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.38554203510284424, 'baseline_loss': 0.8050704002380371, 'total_loss': 0.016993165016174316}\n",
      "2021-09-07 17:18:39.825 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2166370153427124\n",
      "2021-09-07 17:18:39.826 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.37226244807243347\n",
      "2021-09-07 17:18:39.827 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2166370153427124\n",
      "2021-09-07 17:18:39.828 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.37226244807243347\n",
      "2021-09-07 17:18:39.829 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:39.831 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4536445140838623, 'baseline_loss': 0.8385508060455322, 'total_loss': -0.03436911106109619}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:39.832 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4355959892272949\n",
      "2021-09-07 17:18:39.833 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8495416641235352\n",
      "2021-09-07 17:18:39.834 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4355959892272949\n",
      "2021-09-07 17:18:39.835 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:39.836 | INFO     | src.policies:train:123 - Epoch 545 / 800\n",
      "2021-09-07 17:18:39.837 | INFO     | src.policies:collect_trajectories:221 - Episode 1369\n",
      "2021-09-07 17:18:39.866 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:39.866 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:39.867 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:39.869 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:39.871 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32832539081573486, 'baseline_loss': 0.4415681064128876, 'total_loss': -0.10754133760929108}\n",
      "2021-09-07 17:18:39.872 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22903209924697876\n",
      "2021-09-07 17:18:39.873 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.34504058957099915\n",
      "2021-09-07 17:18:39.875 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22903209924697876\n",
      "2021-09-07 17:18:39.876 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.34504058957099915\n",
      "2021-09-07 17:18:39.877 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:39.878 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.30044087767601013, 'baseline_loss': 0.42746299505233765, 'total_loss': -0.08670938014984131}\n",
      "2021-09-07 17:18:39.879 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12570612132549286\n",
      "2021-09-07 17:18:39.880 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4826969802379608\n",
      "2021-09-07 17:18:39.881 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12570612132549286\n",
      "2021-09-07 17:18:39.882 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4826969802379608\n",
      "2021-09-07 17:18:39.884 | INFO     | src.policies:train:123 - Epoch 546 / 800\n",
      "2021-09-07 17:18:39.884 | INFO     | src.policies:collect_trajectories:221 - Episode 1370\n",
      "2021-09-07 17:18:39.891 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:39.891 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 38.0\n",
      "2021-09-07 17:18:39.892 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 38.0\n",
      "2021-09-07 17:18:39.892 | INFO     | src.policies:collect_trajectories:221 - Episode 1371\n",
      "2021-09-07 17:18:39.924 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:39.924 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:39.925 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 119.0\n",
      "2021-09-07 17:18:39.925 | WARNING  | src.policies:train:144 - The actual batch size is 238, instead of 200\n",
      "2021-09-07 17:18:39.928 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:39.930 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5341610312461853, 'baseline_loss': 0.9932986497879028, 'total_loss': -0.03751170635223389}\n",
      "2021-09-07 17:18:39.931 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6714202761650085\n",
      "2021-09-07 17:18:39.931 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8848887085914612\n",
      "2021-09-07 17:18:39.933 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:39.934 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:39.935 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:39.937 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4398737847805023, 'baseline_loss': 0.9624490141868591, 'total_loss': 0.041350722312927246}\n",
      "2021-09-07 17:18:39.938 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2566479444503784\n",
      "2021-09-07 17:18:39.939 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0664397478103638\n",
      "2021-09-07 17:18:39.940 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2566479444503784\n",
      "2021-09-07 17:18:39.942 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:39.944 | INFO     | src.policies:train:123 - Epoch 547 / 800\n",
      "2021-09-07 17:18:39.944 | INFO     | src.policies:collect_trajectories:221 - Episode 1372\n",
      "2021-09-07 17:18:39.977 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:39.978 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:39.978 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:39.981 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:39.983 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3620840311050415, 'baseline_loss': 0.5147688388824463, 'total_loss': -0.10469961166381836}\n",
      "2021-09-07 17:18:39.984 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2141106128692627\n",
      "2021-09-07 17:18:39.985 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1599228382110596\n",
      "2021-09-07 17:18:39.986 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2141106128692627\n",
      "2021-09-07 17:18:39.988 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:39.989 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:39.991 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3271665573120117, 'baseline_loss': 0.5091884732246399, 'total_loss': -0.07257232069969177}\n",
      "2021-09-07 17:18:39.992 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.41142788529396057\n",
      "2021-09-07 17:18:39.993 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2609463036060333\n",
      "2021-09-07 17:18:39.994 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.41142788529396057\n",
      "2021-09-07 17:18:39.995 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2609463036060333\n",
      "2021-09-07 17:18:39.996 | INFO     | src.policies:train:123 - Epoch 548 / 800\n",
      "2021-09-07 17:18:39.996 | INFO     | src.policies:collect_trajectories:221 - Episode 1373\n",
      "2021-09-07 17:18:40.028 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:40.029 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:40.029 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:40.031 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:40.034 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4760846793651581, 'baseline_loss': 0.6441095471382141, 'total_loss': -0.15402990579605103}\n",
      "2021-09-07 17:18:40.035 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3339749276638031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:40.036 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.43449491262435913\n",
      "2021-09-07 17:18:40.038 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3339749276638031\n",
      "2021-09-07 17:18:40.039 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.43449491262435913\n",
      "2021-09-07 17:18:40.041 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:40.042 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27779650688171387, 'baseline_loss': 0.4306434392929077, 'total_loss': -0.06247478723526001}\n",
      "2021-09-07 17:18:40.043 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19702868163585663\n",
      "2021-09-07 17:18:40.044 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7401382923126221\n",
      "2021-09-07 17:18:40.045 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19702868163585663\n",
      "2021-09-07 17:18:40.046 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:40.047 | INFO     | src.policies:train:123 - Epoch 549 / 800\n",
      "2021-09-07 17:18:40.048 | INFO     | src.policies:collect_trajectories:221 - Episode 1374\n",
      "2021-09-07 17:18:40.079 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:40.080 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:40.080 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:40.082 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:40.084 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.9170924425125122, 'baseline_loss': 2.858349561691284, 'total_loss': 0.5120823383331299}\n",
      "2021-09-07 17:18:40.085 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5561507940292358\n",
      "2021-09-07 17:18:40.086 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 5.14365291595459\n",
      "2021-09-07 17:18:40.087 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:40.089 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:40.090 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:40.091 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.9760071039199829, 'baseline_loss': 3.1777641773223877, 'total_loss': 0.6128749847412109}\n",
      "2021-09-07 17:18:40.092 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2947899401187897\n",
      "2021-09-07 17:18:40.093 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 5.385214805603027\n",
      "2021-09-07 17:18:40.094 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2947899401187897\n",
      "2021-09-07 17:18:40.095 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:40.096 | INFO     | src.policies:train:123 - Epoch 550 / 800\n",
      "2021-09-07 17:18:40.096 | INFO     | src.policies:collect_trajectories:221 - Episode 1375\n",
      "2021-09-07 17:18:40.126 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:40.126 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:40.127 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:40.128 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:40.130 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3058134615421295, 'baseline_loss': 0.43427780270576477, 'total_loss': -0.08867456018924713}\n",
      "2021-09-07 17:18:40.131 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11477815359830856\n",
      "2021-09-07 17:18:40.132 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1279501914978027\n",
      "2021-09-07 17:18:40.133 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11477815359830856\n",
      "2021-09-07 17:18:40.135 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:40.136 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:40.137 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.30355149507522583, 'baseline_loss': 0.5139091610908508, 'total_loss': -0.046596914529800415}\n",
      "2021-09-07 17:18:40.138 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24208030104637146\n",
      "2021-09-07 17:18:40.138 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5539670586585999\n",
      "2021-09-07 17:18:40.139 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24208030104637146\n",
      "2021-09-07 17:18:40.140 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:40.142 | INFO     | src.policies:train:123 - Epoch 551 / 800\n",
      "2021-09-07 17:18:40.143 | INFO     | src.policies:collect_trajectories:221 - Episode 1376\n",
      "2021-09-07 17:18:40.172 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:40.173 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:40.173 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:40.176 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:40.178 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7548196911811829, 'baseline_loss': 2.785536527633667, 'total_loss': 0.6379485726356506}\n",
      "2021-09-07 17:18:40.179 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7500376105308533\n",
      "2021-09-07 17:18:40.180 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 5.828291893005371\n",
      "2021-09-07 17:18:40.182 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:40.183 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:40.184 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:40.238 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.942094087600708, 'baseline_loss': 3.065070390701294, 'total_loss': 0.590441107749939}\n",
      "2021-09-07 17:18:40.239 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.33465448021888733\n",
      "2021-09-07 17:18:40.240 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 5.8462233543396\n",
      "2021-09-07 17:18:40.242 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.33465448021888733\n",
      "2021-09-07 17:18:40.243 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:40.245 | INFO     | src.policies:train:123 - Epoch 552 / 800\n",
      "2021-09-07 17:18:40.246 | INFO     | src.policies:collect_trajectories:221 - Episode 1377\n",
      "2021-09-07 17:18:40.263 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:40.264 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 122.0\n",
      "2021-09-07 17:18:40.264 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 122.0\n",
      "2021-09-07 17:18:40.265 | INFO     | src.policies:collect_trajectories:221 - Episode 1378\n",
      "2021-09-07 17:18:40.297 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:40.297 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:40.298 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 161.0\n",
      "2021-09-07 17:18:40.300 | WARNING  | src.policies:train:144 - The actual batch size is 322, instead of 200\n",
      "2021-09-07 17:18:40.303 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:40.305 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.49292999505996704, 'baseline_loss': 1.366382122039795, 'total_loss': 0.19026106595993042}\n",
      "2021-09-07 17:18:40.306 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22806499898433685\n",
      "2021-09-07 17:18:40.307 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5438116788864136\n",
      "2021-09-07 17:18:40.309 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22806499898433685\n",
      "2021-09-07 17:18:40.310 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:40.311 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:40.312 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.48568475246429443, 'baseline_loss': 1.1737195253372192, 'total_loss': 0.10117501020431519}\n",
      "2021-09-07 17:18:40.313 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32088160514831543\n",
      "2021-09-07 17:18:40.314 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5139507055282593\n",
      "2021-09-07 17:18:40.315 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32088160514831543\n",
      "2021-09-07 17:18:40.316 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:40.317 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:40.318 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43342849612236023, 'baseline_loss': 0.9771095514297485, 'total_loss': 0.05512627959251404}\n",
      "2021-09-07 17:18:40.319 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5283859372138977\n",
      "2021-09-07 17:18:40.320 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.01459538936615\n",
      "2021-09-07 17:18:40.321 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:40.322 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:40.323 | INFO     | src.policies:train:123 - Epoch 553 / 800\n",
      "2021-09-07 17:18:40.323 | INFO     | src.policies:collect_trajectories:221 - Episode 1379\n",
      "2021-09-07 17:18:40.354 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:40.354 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:40.355 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:40.357 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:40.359 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -1.0780835151672363, 'baseline_loss': 4.079644680023193, 'total_loss': 0.9617388248443604}\n",
      "2021-09-07 17:18:40.360 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 1.3092459440231323\n",
      "2021-09-07 17:18:40.361 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 6.549615859985352\n",
      "2021-09-07 17:18:40.362 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:40.363 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999999403953552\n",
      "2021-09-07 17:18:40.364 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:40.365 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.9253438711166382, 'baseline_loss': 3.973529100418091, 'total_loss': 1.0614206790924072}\n",
      "2021-09-07 17:18:40.366 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.8538556098937988\n",
      "2021-09-07 17:18:40.367 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 6.507586479187012\n",
      "2021-09-07 17:18:40.368 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:40.369 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:40.371 | INFO     | src.policies:train:123 - Epoch 554 / 800\n",
      "2021-09-07 17:18:40.371 | INFO     | src.policies:collect_trajectories:221 - Episode 1380\n",
      "2021-09-07 17:18:40.402 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:40.403 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:40.403 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:40.405 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:40.408 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3369220793247223, 'baseline_loss': 1.1353117227554321, 'total_loss': 0.23073378205299377}\n",
      "2021-09-07 17:18:40.408 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.27360841631889343\n",
      "2021-09-07 17:18:40.409 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2721550464630127\n",
      "2021-09-07 17:18:40.410 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.27360841631889343\n",
      "2021-09-07 17:18:40.411 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:40.413 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:40.414 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.35724112391471863, 'baseline_loss': 1.0745351314544678, 'total_loss': 0.18002644181251526}\n",
      "2021-09-07 17:18:40.415 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1521914303302765\n",
      "2021-09-07 17:18:40.416 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2740296125411987\n",
      "2021-09-07 17:18:40.417 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1521914303302765\n",
      "2021-09-07 17:18:40.418 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:40.419 | INFO     | src.policies:train:123 - Epoch 555 / 800\n",
      "2021-09-07 17:18:40.419 | INFO     | src.policies:collect_trajectories:221 - Episode 1381\n",
      "2021-09-07 17:18:40.451 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:40.452 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:40.453 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:40.454 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:40.456 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4028792679309845, 'baseline_loss': 0.8437862992286682, 'total_loss': 0.01901388168334961}\n",
      "2021-09-07 17:18:40.457 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23708584904670715\n",
      "2021-09-07 17:18:40.458 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6704114675521851\n",
      "2021-09-07 17:18:40.459 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23708584904670715\n",
      "2021-09-07 17:18:40.461 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:40.462 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:40.463 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4060472548007965, 'baseline_loss': 0.699405312538147, 'total_loss': -0.05634459853172302}\n",
      "2021-09-07 17:18:40.464 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4500613510608673\n",
      "2021-09-07 17:18:40.465 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5931352376937866\n",
      "2021-09-07 17:18:40.466 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4500613510608673\n",
      "2021-09-07 17:18:40.467 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:40.468 | INFO     | src.policies:train:123 - Epoch 556 / 800\n",
      "2021-09-07 17:18:40.469 | INFO     | src.policies:collect_trajectories:221 - Episode 1382\n",
      "2021-09-07 17:18:40.496 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:40.497 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 194.0\n",
      "2021-09-07 17:18:40.497 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 194.0\n",
      "2021-09-07 17:18:40.497 | INFO     | src.policies:collect_trajectories:221 - Episode 1383\n",
      "2021-09-07 17:18:40.526 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:40.527 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 176.0\n",
      "2021-09-07 17:18:40.527 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 185.0\n",
      "2021-09-07 17:18:40.528 | WARNING  | src.policies:train:144 - The actual batch size is 370, instead of 200\n",
      "2021-09-07 17:18:40.531 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:40.533 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7066848278045654, 'baseline_loss': 2.255561113357544, 'total_loss': 0.42109572887420654}\n",
      "2021-09-07 17:18:40.534 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2800747752189636\n",
      "2021-09-07 17:18:40.535 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.7962474822998047\n",
      "2021-09-07 17:18:40.536 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2800747752189636\n",
      "2021-09-07 17:18:40.537 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:40.538 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:40.539 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6347455382347107, 'baseline_loss': 1.9721543788909912, 'total_loss': 0.3513316512107849}\n",
      "2021-09-07 17:18:40.540 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2888410985469818\n",
      "2021-09-07 17:18:40.541 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.495044708251953\n",
      "2021-09-07 17:18:40.542 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2888410985469818\n",
      "2021-09-07 17:18:40.543 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:40.544 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:40.545 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6094673871994019, 'baseline_loss': 1.8597325086593628, 'total_loss': 0.32039886713027954}\n",
      "2021-09-07 17:18:40.546 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5800610780715942\n",
      "2021-09-07 17:18:40.547 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.8724138736724854\n",
      "2021-09-07 17:18:40.548 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:40.549 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:40.550 | INFO     | src.policies:train:123 - Epoch 557 / 800\n",
      "2021-09-07 17:18:40.551 | INFO     | src.policies:collect_trajectories:221 - Episode 1384\n",
      "2021-09-07 17:18:40.580 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:40.581 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:40.581 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:40.583 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:40.587 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8043965101242065, 'baseline_loss': 2.5605897903442383, 'total_loss': 0.4758983850479126}\n",
      "2021-09-07 17:18:40.588 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2631559371948242\n",
      "2021-09-07 17:18:40.588 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.460079669952393\n",
      "2021-09-07 17:18:40.589 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2631559371948242\n",
      "2021-09-07 17:18:40.590 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:40.592 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:40.593 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6710515022277832, 'baseline_loss': 2.5449886322021484, 'total_loss': 0.601442813873291}\n",
      "2021-09-07 17:18:40.594 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6821036338806152\n",
      "2021-09-07 17:18:40.594 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.8517231941223145\n",
      "2021-09-07 17:18:40.595 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:40.596 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:40.598 | INFO     | src.policies:train:123 - Epoch 558 / 800\n",
      "2021-09-07 17:18:40.598 | INFO     | src.policies:collect_trajectories:221 - Episode 1385\n",
      "2021-09-07 17:18:40.628 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:40.629 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:40.629 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:40.631 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:40.634 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1857752948999405, 'baseline_loss': 1.2419909238815308, 'total_loss': 0.4352201819419861}\n",
      "2021-09-07 17:18:40.635 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19225527346134186\n",
      "2021-09-07 17:18:40.636 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1531188488006592\n",
      "2021-09-07 17:18:40.637 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19225527346134186\n",
      "2021-09-07 17:18:40.638 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:40.639 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:40.640 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.16178137063980103, 'baseline_loss': 1.166982650756836, 'total_loss': 0.42170995473861694}\n",
      "2021-09-07 17:18:40.641 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30273792147636414\n",
      "2021-09-07 17:18:40.642 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.174137830734253\n",
      "2021-09-07 17:18:40.643 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30273792147636414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:40.644 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:40.645 | INFO     | src.policies:train:123 - Epoch 559 / 800\n",
      "2021-09-07 17:18:40.646 | INFO     | src.policies:collect_trajectories:221 - Episode 1386\n",
      "2021-09-07 17:18:40.677 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:40.677 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:40.678 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:40.680 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:40.682 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5706575512886047, 'baseline_loss': 2.0816848278045654, 'total_loss': 0.470184862613678}\n",
      "2021-09-07 17:18:40.683 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.330820769071579\n",
      "2021-09-07 17:18:40.684 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.3308191299438477\n",
      "2021-09-07 17:18:40.685 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.330820769071579\n",
      "2021-09-07 17:18:40.686 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:40.688 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:40.689 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5755711197853088, 'baseline_loss': 1.8770670890808105, 'total_loss': 0.36296242475509644}\n",
      "2021-09-07 17:18:40.690 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25312867760658264\n",
      "2021-09-07 17:18:40.691 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.593050003051758\n",
      "2021-09-07 17:18:40.692 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25312867760658264\n",
      "2021-09-07 17:18:40.693 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:40.694 | INFO     | src.policies:train:123 - Epoch 560 / 800\n",
      "2021-09-07 17:18:40.694 | INFO     | src.policies:collect_trajectories:221 - Episode 1387\n",
      "2021-09-07 17:18:40.722 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:40.723 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:40.723 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:40.726 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:40.728 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.35341158509254456, 'baseline_loss': 0.8205056190490723, 'total_loss': 0.05684122443199158}\n",
      "2021-09-07 17:18:40.729 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.143227219581604\n",
      "2021-09-07 17:18:40.729 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2228442430496216\n",
      "2021-09-07 17:18:40.731 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.143227219581604\n",
      "2021-09-07 17:18:40.732 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:40.733 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:40.734 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2895194888114929, 'baseline_loss': 0.7873557209968567, 'total_loss': 0.10415837168693542}\n",
      "2021-09-07 17:18:40.735 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6614640355110168\n",
      "2021-09-07 17:18:40.736 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9398094415664673\n",
      "2021-09-07 17:18:40.737 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:40.883 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:40.885 | INFO     | src.policies:train:123 - Epoch 561 / 800\n",
      "2021-09-07 17:18:40.885 | INFO     | src.policies:collect_trajectories:221 - Episode 1388\n",
      "2021-09-07 17:18:40.918 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:40.919 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:40.919 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:40.921 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:40.924 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.46016690135002136, 'baseline_loss': 1.5271401405334473, 'total_loss': 0.30340316891670227}\n",
      "2021-09-07 17:18:40.925 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3645459711551666\n",
      "2021-09-07 17:18:40.926 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9746230840682983\n",
      "2021-09-07 17:18:40.927 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3645459711551666\n",
      "2021-09-07 17:18:40.928 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:40.930 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:40.931 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.46104490756988525, 'baseline_loss': 1.6048834323883057, 'total_loss': 0.3413968086242676}\n",
      "2021-09-07 17:18:40.932 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3032660186290741\n",
      "2021-09-07 17:18:40.933 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.294276475906372\n",
      "2021-09-07 17:18:40.934 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3032660186290741\n",
      "2021-09-07 17:18:40.935 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:40.936 | INFO     | src.policies:train:123 - Epoch 562 / 800\n",
      "2021-09-07 17:18:40.937 | INFO     | src.policies:collect_trajectories:221 - Episode 1389\n",
      "2021-09-07 17:18:40.966 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:40.967 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:40.967 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:40.970 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:40.972 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29552382230758667, 'baseline_loss': 0.7536936402320862, 'total_loss': 0.08132299780845642}\n",
      "2021-09-07 17:18:40.973 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.33741047978401184\n",
      "2021-09-07 17:18:40.974 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5517206192016602\n",
      "2021-09-07 17:18:40.975 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.33741047978401184\n",
      "2021-09-07 17:18:40.977 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:40.978 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:40.979 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3154134452342987, 'baseline_loss': 0.777635931968689, 'total_loss': 0.07340452075004578}\n",
      "2021-09-07 17:18:40.980 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2884061932563782\n",
      "2021-09-07 17:18:40.981 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6637214422225952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:40.982 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2884061932563782\n",
      "2021-09-07 17:18:40.983 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:40.984 | INFO     | src.policies:train:123 - Epoch 563 / 800\n",
      "2021-09-07 17:18:40.985 | INFO     | src.policies:collect_trajectories:221 - Episode 1390\n",
      "2021-09-07 17:18:41.016 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:41.016 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:41.017 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:41.019 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:41.022 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.37429848313331604, 'baseline_loss': 1.0838768482208252, 'total_loss': 0.16763994097709656}\n",
      "2021-09-07 17:18:41.024 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15219038724899292\n",
      "2021-09-07 17:18:41.025 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.717369794845581\n",
      "2021-09-07 17:18:41.026 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15219038724899292\n",
      "2021-09-07 17:18:41.027 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:41.028 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:41.030 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2705516219139099, 'baseline_loss': 0.9614604711532593, 'total_loss': 0.21017861366271973}\n",
      "2021-09-07 17:18:41.030 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13178539276123047\n",
      "2021-09-07 17:18:41.032 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8212831020355225\n",
      "2021-09-07 17:18:41.033 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13178539276123047\n",
      "2021-09-07 17:18:41.035 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:41.037 | INFO     | src.policies:train:123 - Epoch 564 / 800\n",
      "2021-09-07 17:18:41.037 | INFO     | src.policies:collect_trajectories:221 - Episode 1391\n",
      "2021-09-07 17:18:41.070 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:41.071 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:41.071 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:41.073 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:41.074 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.17484557628631592, 'baseline_loss': 0.848771870136261, 'total_loss': 0.24954035878181458}\n",
      "2021-09-07 17:18:41.075 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24525155127048492\n",
      "2021-09-07 17:18:41.076 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.374356985092163\n",
      "2021-09-07 17:18:41.077 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24525155127048492\n",
      "2021-09-07 17:18:41.078 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:41.080 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:41.081 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1224958524107933, 'baseline_loss': 0.9732785224914551, 'total_loss': 0.36414340138435364}\n",
      "2021-09-07 17:18:41.082 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.27433422207832336\n",
      "2021-09-07 17:18:41.083 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6944125890731812\n",
      "2021-09-07 17:18:41.084 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.27433422207832336\n",
      "2021-09-07 17:18:41.085 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:41.087 | INFO     | src.policies:train:123 - Epoch 565 / 800\n",
      "2021-09-07 17:18:41.088 | INFO     | src.policies:collect_trajectories:221 - Episode 1392\n",
      "2021-09-07 17:18:41.117 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:41.117 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:41.118 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:41.120 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:41.122 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.165532186627388, 'baseline_loss': 0.626507580280304, 'total_loss': 0.14772160351276398}\n",
      "2021-09-07 17:18:41.122 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1304452270269394\n",
      "2021-09-07 17:18:41.124 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7117658853530884\n",
      "2021-09-07 17:18:41.125 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1304452270269394\n",
      "2021-09-07 17:18:41.126 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:41.127 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:41.128 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2626628577709198, 'baseline_loss': 0.5941364765167236, 'total_loss': 0.03440538048744202}\n",
      "2021-09-07 17:18:41.129 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12886451184749603\n",
      "2021-09-07 17:18:41.130 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7145062685012817\n",
      "2021-09-07 17:18:41.131 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12886451184749603\n",
      "2021-09-07 17:18:41.132 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:41.133 | INFO     | src.policies:train:123 - Epoch 566 / 800\n",
      "2021-09-07 17:18:41.134 | INFO     | src.policies:collect_trajectories:221 - Episode 1393\n",
      "2021-09-07 17:18:41.162 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:41.163 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:41.163 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:41.165 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:41.167 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.14433185756206512, 'baseline_loss': 0.7366244792938232, 'total_loss': 0.2239803820848465}\n",
      "2021-09-07 17:18:41.168 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3679215610027313\n",
      "2021-09-07 17:18:41.169 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8890417814254761\n",
      "2021-09-07 17:18:41.170 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3679215610027313\n",
      "2021-09-07 17:18:41.171 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:41.172 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:41.173 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20366975665092468, 'baseline_loss': 0.6526933908462524, 'total_loss': 0.12267693877220154}\n",
      "2021-09-07 17:18:41.174 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2663496732711792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:41.175 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.090597152709961\n",
      "2021-09-07 17:18:41.176 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2663496732711792\n",
      "2021-09-07 17:18:41.177 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:41.179 | INFO     | src.policies:train:123 - Epoch 567 / 800\n",
      "2021-09-07 17:18:41.179 | INFO     | src.policies:collect_trajectories:221 - Episode 1394\n",
      "2021-09-07 17:18:41.208 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:41.209 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:41.209 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:41.211 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:41.213 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5378857851028442, 'baseline_loss': 1.8599886894226074, 'total_loss': 0.3921085596084595}\n",
      "2021-09-07 17:18:41.214 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1904505342245102\n",
      "2021-09-07 17:18:41.215 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.1601667404174805\n",
      "2021-09-07 17:18:41.216 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1904505342245102\n",
      "2021-09-07 17:18:41.217 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:41.218 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:41.219 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.613669753074646, 'baseline_loss': 1.7925057411193848, 'total_loss': 0.2825831174850464}\n",
      "2021-09-07 17:18:41.220 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.35045233368873596\n",
      "2021-09-07 17:18:41.221 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.5475077629089355\n",
      "2021-09-07 17:18:41.222 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.35045233368873596\n",
      "2021-09-07 17:18:41.223 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:41.224 | INFO     | src.policies:train:123 - Epoch 568 / 800\n",
      "2021-09-07 17:18:41.225 | INFO     | src.policies:collect_trajectories:221 - Episode 1395\n",
      "2021-09-07 17:18:41.253 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:41.254 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:41.254 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:41.256 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:41.259 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.28339728713035583, 'baseline_loss': 0.7010046243667603, 'total_loss': 0.06710502505302429}\n",
      "2021-09-07 17:18:41.259 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16520066559314728\n",
      "2021-09-07 17:18:41.260 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.10793137550354\n",
      "2021-09-07 17:18:41.262 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16520066559314728\n",
      "2021-09-07 17:18:41.263 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:41.264 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:41.265 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27769529819488525, 'baseline_loss': 0.5947476029396057, 'total_loss': 0.019678503274917603}\n",
      "2021-09-07 17:18:41.266 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31078657507896423\n",
      "2021-09-07 17:18:41.267 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1588361263275146\n",
      "2021-09-07 17:18:41.268 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31078657507896423\n",
      "2021-09-07 17:18:41.269 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:41.270 | INFO     | src.policies:train:123 - Epoch 569 / 800\n",
      "2021-09-07 17:18:41.271 | INFO     | src.policies:collect_trajectories:221 - Episode 1396\n",
      "2021-09-07 17:18:41.440 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:41.441 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:41.442 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:41.445 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:41.448 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3962725102901459, 'baseline_loss': 0.9542137384414673, 'total_loss': 0.08083435893058777}\n",
      "2021-09-07 17:18:41.449 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.26197686791419983\n",
      "2021-09-07 17:18:41.450 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3344276547431946\n",
      "2021-09-07 17:18:41.451 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.26197686791419983\n",
      "2021-09-07 17:18:41.452 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3344276547431946\n",
      "2021-09-07 17:18:41.454 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:41.455 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40354347229003906, 'baseline_loss': 0.9480739831924438, 'total_loss': 0.07049351930618286}\n",
      "2021-09-07 17:18:41.456 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08398879319429398\n",
      "2021-09-07 17:18:41.456 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2807193100452423\n",
      "2021-09-07 17:18:41.457 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08398879319429398\n",
      "2021-09-07 17:18:41.458 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2807193100452423\n",
      "2021-09-07 17:18:41.460 | INFO     | src.policies:train:123 - Epoch 570 / 800\n",
      "2021-09-07 17:18:41.461 | INFO     | src.policies:collect_trajectories:221 - Episode 1397\n",
      "2021-09-07 17:18:41.489 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:41.490 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:41.490 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:41.492 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:41.494 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6297695636749268, 'baseline_loss': 2.0555460453033447, 'total_loss': 0.3980034589767456}\n",
      "2021-09-07 17:18:41.495 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09694010019302368\n",
      "2021-09-07 17:18:41.496 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.405540704727173\n",
      "2021-09-07 17:18:41.497 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09694010019302368\n",
      "2021-09-07 17:18:41.499 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:41.500 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:41.501 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5166100859642029, 'baseline_loss': 1.7496987581253052, 'total_loss': 0.3582392930984497}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:41.502 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2049683928489685\n",
      "2021-09-07 17:18:41.503 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.4258971214294434\n",
      "2021-09-07 17:18:41.504 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2049683928489685\n",
      "2021-09-07 17:18:41.505 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:41.506 | INFO     | src.policies:train:123 - Epoch 571 / 800\n",
      "2021-09-07 17:18:41.507 | INFO     | src.policies:collect_trajectories:221 - Episode 1398\n",
      "2021-09-07 17:18:41.535 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:41.536 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:41.536 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:41.539 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:41.541 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.12263038754463196, 'baseline_loss': 0.5579952001571655, 'total_loss': 0.1563672125339508}\n",
      "2021-09-07 17:18:41.542 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15530091524124146\n",
      "2021-09-07 17:18:41.543 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.480028510093689\n",
      "2021-09-07 17:18:41.544 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15530091524124146\n",
      "2021-09-07 17:18:41.545 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:41.546 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:41.547 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1494295746088028, 'baseline_loss': 0.44306549429893494, 'total_loss': 0.07210317254066467}\n",
      "2021-09-07 17:18:41.548 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2652629613876343\n",
      "2021-09-07 17:18:41.549 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5441772937774658\n",
      "2021-09-07 17:18:41.550 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2652629613876343\n",
      "2021-09-07 17:18:41.551 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:41.552 | INFO     | src.policies:train:123 - Epoch 572 / 800\n",
      "2021-09-07 17:18:41.553 | INFO     | src.policies:collect_trajectories:221 - Episode 1399\n",
      "2021-09-07 17:18:41.581 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:41.581 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:41.582 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:41.584 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:41.586 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1910409927368164, 'baseline_loss': 0.4541924297809601, 'total_loss': 0.036055222153663635}\n",
      "2021-09-07 17:18:41.587 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.36414834856987\n",
      "2021-09-07 17:18:41.588 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.466761350631714\n",
      "2021-09-07 17:18:41.589 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.36414834856987\n",
      "2021-09-07 17:18:41.590 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:41.591 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:41.592 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1312481015920639, 'baseline_loss': 0.4579215943813324, 'total_loss': 0.0977126955986023}\n",
      "2021-09-07 17:18:41.593 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07895486801862717\n",
      "2021-09-07 17:18:41.594 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.3375024795532227\n",
      "2021-09-07 17:18:41.595 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07895486801862717\n",
      "2021-09-07 17:18:41.596 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:41.598 | INFO     | src.policies:train:123 - Epoch 573 / 800\n",
      "2021-09-07 17:18:41.598 | INFO     | src.policies:collect_trajectories:221 - Episode 1400\n",
      "2021-09-07 17:18:41.626 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:41.627 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:41.627 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:41.630 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:41.632 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.03380611166357994, 'baseline_loss': 0.6479336023330688, 'total_loss': 0.2901606857776642}\n",
      "2021-09-07 17:18:41.633 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6055949330329895\n",
      "2021-09-07 17:18:41.634 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8428596258163452\n",
      "2021-09-07 17:18:41.635 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:41.637 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:41.639 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:41.640 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.09469988197088242, 'baseline_loss': 0.6042243838310242, 'total_loss': 0.20741230249404907}\n",
      "2021-09-07 17:18:41.641 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.26153236627578735\n",
      "2021-09-07 17:18:41.643 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.300270915031433\n",
      "2021-09-07 17:18:41.644 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.26153236627578735\n",
      "2021-09-07 17:18:41.645 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:41.647 | INFO     | src.policies:train:123 - Epoch 574 / 800\n",
      "2021-09-07 17:18:41.648 | INFO     | src.policies:collect_trajectories:221 - Episode 1401\n",
      "2021-09-07 17:18:41.656 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:41.657 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 47.0\n",
      "2021-09-07 17:18:41.657 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 47.0\n",
      "2021-09-07 17:18:41.658 | INFO     | src.policies:collect_trajectories:221 - Episode 1402\n",
      "2021-09-07 17:18:41.688 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:41.688 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:41.689 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 123.5\n",
      "2021-09-07 17:18:41.689 | WARNING  | src.policies:train:144 - The actual batch size is 247, instead of 200\n",
      "2021-09-07 17:18:41.692 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:41.694 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.12141744792461395, 'baseline_loss': 0.8932361006736755, 'total_loss': 0.325200617313385}\n",
      "2021-09-07 17:18:41.695 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5820449590682983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:41.696 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.537049412727356\n",
      "2021-09-07 17:18:41.697 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:41.698 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:41.699 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:41.701 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.22554819285869598, 'baseline_loss': 0.7761709690093994, 'total_loss': 0.16253729164600372}\n",
      "2021-09-07 17:18:41.701 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3168170750141144\n",
      "2021-09-07 17:18:41.702 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6796780824661255\n",
      "2021-09-07 17:18:41.703 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3168170750141144\n",
      "2021-09-07 17:18:41.704 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:41.706 | INFO     | src.policies:train:123 - Epoch 575 / 800\n",
      "2021-09-07 17:18:41.706 | INFO     | src.policies:collect_trajectories:221 - Episode 1403\n",
      "2021-09-07 17:18:41.735 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:41.735 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:41.736 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:41.738 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:41.740 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40942153334617615, 'baseline_loss': 0.8507504463195801, 'total_loss': 0.01595368981361389}\n",
      "2021-09-07 17:18:41.741 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11840382218360901\n",
      "2021-09-07 17:18:41.743 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5711258053779602\n",
      "2021-09-07 17:18:41.744 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11840382218360901\n",
      "2021-09-07 17:18:41.745 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:41.746 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:41.747 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.44730162620544434, 'baseline_loss': 0.7869285345077515, 'total_loss': -0.053837358951568604}\n",
      "2021-09-07 17:18:41.748 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.46912747621536255\n",
      "2021-09-07 17:18:41.749 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.371178537607193\n",
      "2021-09-07 17:18:41.750 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.46912747621536255\n",
      "2021-09-07 17:18:41.751 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.371178537607193\n",
      "2021-09-07 17:18:41.752 | INFO     | src.policies:train:123 - Epoch 576 / 800\n",
      "2021-09-07 17:18:41.753 | INFO     | src.policies:collect_trajectories:221 - Episode 1404\n",
      "2021-09-07 17:18:41.780 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:41.780 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:41.781 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:41.783 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:41.785 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4676896929740906, 'baseline_loss': 1.0487942695617676, 'total_loss': 0.05670744180679321}\n",
      "2021-09-07 17:18:41.786 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21876992285251617\n",
      "2021-09-07 17:18:41.787 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3570523262023926\n",
      "2021-09-07 17:18:41.788 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21876992285251617\n",
      "2021-09-07 17:18:41.789 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:41.790 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:41.791 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.477138489484787, 'baseline_loss': 1.100224494934082, 'total_loss': 0.07297375798225403}\n",
      "2021-09-07 17:18:41.792 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.40351179242134094\n",
      "2021-09-07 17:18:41.793 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5735312700271606\n",
      "2021-09-07 17:18:41.794 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.40351179242134094\n",
      "2021-09-07 17:18:41.795 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:41.796 | INFO     | src.policies:train:123 - Epoch 577 / 800\n",
      "2021-09-07 17:18:41.797 | INFO     | src.policies:collect_trajectories:221 - Episode 1405\n",
      "2021-09-07 17:18:41.826 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:41.827 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:41.827 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:41.830 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:41.833 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6467698812484741, 'baseline_loss': 2.740067958831787, 'total_loss': 0.7232640981674194}\n",
      "2021-09-07 17:18:41.834 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.8459950685501099\n",
      "2021-09-07 17:18:41.835 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.402983665466309\n",
      "2021-09-07 17:18:41.836 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:41.837 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:41.838 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:41.840 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5985263586044312, 'baseline_loss': 2.857119560241699, 'total_loss': 0.8300334215164185}\n",
      "2021-09-07 17:18:41.841 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.26325657963752747\n",
      "2021-09-07 17:18:41.842 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.143835544586182\n",
      "2021-09-07 17:18:41.843 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.26325657963752747\n",
      "2021-09-07 17:18:41.844 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:41.845 | INFO     | src.policies:train:123 - Epoch 578 / 800\n",
      "2021-09-07 17:18:41.845 | INFO     | src.policies:collect_trajectories:221 - Episode 1406\n",
      "2021-09-07 17:18:41.875 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:41.875 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:41.876 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:41.878 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:41.880 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.403443843126297, 'baseline_loss': 0.8920751810073853, 'total_loss': 0.04259374737739563}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:41.882 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25319433212280273\n",
      "2021-09-07 17:18:41.883 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.49043965339660645\n",
      "2021-09-07 17:18:41.884 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25319433212280273\n",
      "2021-09-07 17:18:41.885 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49043965339660645\n",
      "2021-09-07 17:18:41.886 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:41.887 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3772844076156616, 'baseline_loss': 0.9291239380836487, 'total_loss': 0.08727756142616272}\n",
      "2021-09-07 17:18:41.888 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19684934616088867\n",
      "2021-09-07 17:18:41.889 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7972078323364258\n",
      "2021-09-07 17:18:41.890 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19684934616088867\n",
      "2021-09-07 17:18:41.891 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:41.892 | INFO     | src.policies:train:123 - Epoch 579 / 800\n",
      "2021-09-07 17:18:41.892 | INFO     | src.policies:collect_trajectories:221 - Episode 1407\n",
      "2021-09-07 17:18:41.922 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:41.923 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:41.923 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:41.927 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:41.929 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4525490999221802, 'baseline_loss': 1.6356676816940308, 'total_loss': 0.3652847409248352}\n",
      "2021-09-07 17:18:41.931 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13614076375961304\n",
      "2021-09-07 17:18:41.932 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.626603126525879\n",
      "2021-09-07 17:18:41.933 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13614076375961304\n",
      "2021-09-07 17:18:41.934 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:41.997 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:41.998 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31468361616134644, 'baseline_loss': 1.6800957918167114, 'total_loss': 0.5253642797470093}\n",
      "2021-09-07 17:18:41.999 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.26208779215812683\n",
      "2021-09-07 17:18:42.001 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.883216142654419\n",
      "2021-09-07 17:18:42.003 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.26208779215812683\n",
      "2021-09-07 17:18:42.005 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:42.007 | INFO     | src.policies:train:123 - Epoch 580 / 800\n",
      "2021-09-07 17:18:42.008 | INFO     | src.policies:collect_trajectories:221 - Episode 1408\n",
      "2021-09-07 17:18:42.038 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.039 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:42.039 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:42.041 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:42.044 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6610710024833679, 'baseline_loss': 2.1747000217437744, 'total_loss': 0.4262790083885193}\n",
      "2021-09-07 17:18:42.044 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.8899433016777039\n",
      "2021-09-07 17:18:42.045 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.7963476181030273\n",
      "2021-09-07 17:18:42.047 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:42.048 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:42.049 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:42.050 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5503750443458557, 'baseline_loss': 2.2440831661224365, 'total_loss': 0.5716665387153625}\n",
      "2021-09-07 17:18:42.051 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.36027640104293823\n",
      "2021-09-07 17:18:42.051 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.616946220397949\n",
      "2021-09-07 17:18:42.052 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.36027640104293823\n",
      "2021-09-07 17:18:42.053 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:42.055 | INFO     | src.policies:train:123 - Epoch 581 / 800\n",
      "2021-09-07 17:18:42.055 | INFO     | src.policies:collect_trajectories:221 - Episode 1409\n",
      "2021-09-07 17:18:42.089 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.090 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:42.091 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:42.093 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:42.095 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4820534586906433, 'baseline_loss': 1.6074029207229614, 'total_loss': 0.3216480016708374}\n",
      "2021-09-07 17:18:42.096 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3687555491924286\n",
      "2021-09-07 17:18:42.097 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7527931928634644\n",
      "2021-09-07 17:18:42.098 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3687555491924286\n",
      "2021-09-07 17:18:42.099 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:42.101 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:42.102 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.44881248474121094, 'baseline_loss': 1.519029140472412, 'total_loss': 0.3107020854949951}\n",
      "2021-09-07 17:18:42.103 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16790813207626343\n",
      "2021-09-07 17:18:42.104 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3277313709259033\n",
      "2021-09-07 17:18:42.105 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16790813207626343\n",
      "2021-09-07 17:18:42.106 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:42.107 | INFO     | src.policies:train:123 - Epoch 582 / 800\n",
      "2021-09-07 17:18:42.108 | INFO     | src.policies:collect_trajectories:221 - Episode 1410\n",
      "2021-09-07 17:18:42.138 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.139 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:42.140 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:42.142 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:42.144 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4545673429965973, 'baseline_loss': 1.2037596702575684, 'total_loss': 0.1473124921321869}\n",
      "2021-09-07 17:18:42.145 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08548345416784286\n",
      "2021-09-07 17:18:42.146 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2261079549789429\n",
      "2021-09-07 17:18:42.147 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08548345416784286\n",
      "2021-09-07 17:18:42.148 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:42.149 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:42.151 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.48054200410842896, 'baseline_loss': 1.1204025745391846, 'total_loss': 0.07965928316116333}\n",
      "2021-09-07 17:18:42.153 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13312797248363495\n",
      "2021-09-07 17:18:42.154 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8419696688652039\n",
      "2021-09-07 17:18:42.155 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13312797248363495\n",
      "2021-09-07 17:18:42.156 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:42.158 | INFO     | src.policies:train:123 - Epoch 583 / 800\n",
      "2021-09-07 17:18:42.158 | INFO     | src.policies:collect_trajectories:221 - Episode 1411\n",
      "2021-09-07 17:18:42.187 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.188 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:42.188 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:42.190 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:42.193 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5578823685646057, 'baseline_loss': 1.561416506767273, 'total_loss': 0.22282588481903076}\n",
      "2021-09-07 17:18:42.194 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20492878556251526\n",
      "2021-09-07 17:18:42.195 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.5148539543151855\n",
      "2021-09-07 17:18:42.197 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20492878556251526\n",
      "2021-09-07 17:18:42.198 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:42.199 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:42.201 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5482133626937866, 'baseline_loss': 1.5335279703140259, 'total_loss': 0.21855062246322632}\n",
      "2021-09-07 17:18:42.202 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11023370176553726\n",
      "2021-09-07 17:18:42.203 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.24714994430542\n",
      "2021-09-07 17:18:42.205 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11023370176553726\n",
      "2021-09-07 17:18:42.206 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:42.208 | INFO     | src.policies:train:123 - Epoch 584 / 800\n",
      "2021-09-07 17:18:42.209 | INFO     | src.policies:collect_trajectories:221 - Episode 1412\n",
      "2021-09-07 17:18:42.238 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.239 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:42.239 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:42.241 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:42.244 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3753420114517212, 'baseline_loss': 1.0017004013061523, 'total_loss': 0.12550818920135498}\n",
      "2021-09-07 17:18:42.245 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18030785024166107\n",
      "2021-09-07 17:18:42.246 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.031461477279663\n",
      "2021-09-07 17:18:42.247 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18030785024166107\n",
      "2021-09-07 17:18:42.248 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:42.249 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:42.250 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3811272084712982, 'baseline_loss': 1.0995069742202759, 'total_loss': 0.16862627863883972}\n",
      "2021-09-07 17:18:42.251 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14668984711170197\n",
      "2021-09-07 17:18:42.252 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1243171691894531\n",
      "2021-09-07 17:18:42.253 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14668984711170197\n",
      "2021-09-07 17:18:42.254 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:42.255 | INFO     | src.policies:train:123 - Epoch 585 / 800\n",
      "2021-09-07 17:18:42.255 | INFO     | src.policies:collect_trajectories:221 - Episode 1413\n",
      "2021-09-07 17:18:42.266 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.267 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 70.0\n",
      "2021-09-07 17:18:42.268 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 70.0\n",
      "2021-09-07 17:18:42.268 | INFO     | src.policies:collect_trajectories:221 - Episode 1414\n",
      "2021-09-07 17:18:42.284 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.285 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 95.0\n",
      "2021-09-07 17:18:42.285 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 82.5\n",
      "2021-09-07 17:18:42.286 | INFO     | src.policies:collect_trajectories:221 - Episode 1415\n",
      "2021-09-07 17:18:42.318 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.318 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:42.319 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 121.66666666666667\n",
      "2021-09-07 17:18:42.320 | WARNING  | src.policies:train:144 - The actual batch size is 365, instead of 200\n",
      "2021-09-07 17:18:42.322 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:42.324 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2940939962863922, 'baseline_loss': 0.7784709930419922, 'total_loss': 0.09514150023460388}\n",
      "2021-09-07 17:18:42.325 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2853410840034485\n",
      "2021-09-07 17:18:42.325 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7873213291168213\n",
      "2021-09-07 17:18:42.326 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2853410840034485\n",
      "2021-09-07 17:18:42.327 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:42.329 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:42.331 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.321379691362381, 'baseline_loss': 0.7431212067604065, 'total_loss': 0.050180912017822266}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:42.332 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15392541885375977\n",
      "2021-09-07 17:18:42.333 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7927494049072266\n",
      "2021-09-07 17:18:42.334 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15392541885375977\n",
      "2021-09-07 17:18:42.336 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:42.337 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:42.338 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3210279047489166, 'baseline_loss': 0.7324683666229248, 'total_loss': 0.045206278562545776}\n",
      "2021-09-07 17:18:42.340 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.28407859802246094\n",
      "2021-09-07 17:18:42.341 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1303750276565552\n",
      "2021-09-07 17:18:42.342 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.28407859802246094\n",
      "2021-09-07 17:18:42.343 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:42.344 | INFO     | src.policies:train:123 - Epoch 586 / 800\n",
      "2021-09-07 17:18:42.345 | INFO     | src.policies:collect_trajectories:221 - Episode 1416\n",
      "2021-09-07 17:18:42.374 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.374 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:42.375 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:42.377 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:42.379 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.25669920444488525, 'baseline_loss': 1.0696845054626465, 'total_loss': 0.278143048286438}\n",
      "2021-09-07 17:18:42.380 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.26364102959632874\n",
      "2021-09-07 17:18:42.381 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3338605165481567\n",
      "2021-09-07 17:18:42.382 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.26364102959632874\n",
      "2021-09-07 17:18:42.383 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:42.384 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:42.385 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23790979385375977, 'baseline_loss': 0.8802697658538818, 'total_loss': 0.20222508907318115}\n",
      "2021-09-07 17:18:42.386 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22530722618103027\n",
      "2021-09-07 17:18:42.387 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8458582162857056\n",
      "2021-09-07 17:18:42.388 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22530722618103027\n",
      "2021-09-07 17:18:42.389 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:42.390 | INFO     | src.policies:train:123 - Epoch 587 / 800\n",
      "2021-09-07 17:18:42.391 | INFO     | src.policies:collect_trajectories:221 - Episode 1417\n",
      "2021-09-07 17:18:42.414 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.415 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 146.0\n",
      "2021-09-07 17:18:42.415 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 146.0\n",
      "2021-09-07 17:18:42.415 | INFO     | src.policies:collect_trajectories:221 - Episode 1418\n",
      "2021-09-07 17:18:42.433 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.434 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 115.0\n",
      "2021-09-07 17:18:42.434 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 130.5\n",
      "2021-09-07 17:18:42.435 | WARNING  | src.policies:train:144 - The actual batch size is 261, instead of 200\n",
      "2021-09-07 17:18:42.437 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:42.440 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4871000349521637, 'baseline_loss': 0.8596144914627075, 'total_loss': -0.057292789220809937}\n",
      "2021-09-07 17:18:42.441 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2353273183107376\n",
      "2021-09-07 17:18:42.443 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.437565565109253\n",
      "2021-09-07 17:18:42.444 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2353273183107376\n",
      "2021-09-07 17:18:42.445 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:42.447 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:42.448 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3690803647041321, 'baseline_loss': 0.6467655897140503, 'total_loss': -0.045697569847106934}\n",
      "2021-09-07 17:18:42.449 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3294832110404968\n",
      "2021-09-07 17:18:42.450 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7589447498321533\n",
      "2021-09-07 17:18:42.451 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3294832110404968\n",
      "2021-09-07 17:18:42.452 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:42.454 | INFO     | src.policies:train:123 - Epoch 588 / 800\n",
      "2021-09-07 17:18:42.454 | INFO     | src.policies:collect_trajectories:221 - Episode 1419\n",
      "2021-09-07 17:18:42.485 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.485 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:42.486 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:42.488 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:42.491 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2580614686012268, 'baseline_loss': 0.6768065094947815, 'total_loss': 0.08034178614616394}\n",
      "2021-09-07 17:18:42.492 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.33867326378822327\n",
      "2021-09-07 17:18:42.493 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9305129051208496\n",
      "2021-09-07 17:18:42.494 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.33867326378822327\n",
      "2021-09-07 17:18:42.495 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:42.496 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:42.497 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29853132367134094, 'baseline_loss': 0.6829338073730469, 'total_loss': 0.042935580015182495}\n",
      "2021-09-07 17:18:42.499 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5265496373176575\n",
      "2021-09-07 17:18:42.500 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.829214334487915\n",
      "2021-09-07 17:18:42.556 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:42.558 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:42.560 | INFO     | src.policies:train:123 - Epoch 589 / 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:42.560 | INFO     | src.policies:collect_trajectories:221 - Episode 1420\n",
      "2021-09-07 17:18:42.581 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.582 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 137.0\n",
      "2021-09-07 17:18:42.582 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 137.0\n",
      "2021-09-07 17:18:42.583 | INFO     | src.policies:collect_trajectories:221 - Episode 1421\n",
      "2021-09-07 17:18:42.612 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.613 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:42.613 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 168.5\n",
      "2021-09-07 17:18:42.614 | WARNING  | src.policies:train:144 - The actual batch size is 337, instead of 200\n",
      "2021-09-07 17:18:42.616 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:42.618 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2799893319606781, 'baseline_loss': 0.9544982314109802, 'total_loss': 0.197259783744812}\n",
      "2021-09-07 17:18:42.619 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16500739753246307\n",
      "2021-09-07 17:18:42.620 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6441816091537476\n",
      "2021-09-07 17:18:42.621 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16500739753246307\n",
      "2021-09-07 17:18:42.622 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:42.623 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:42.624 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3485066294670105, 'baseline_loss': 1.0290766954421997, 'total_loss': 0.16603171825408936}\n",
      "2021-09-07 17:18:42.625 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2412712574005127\n",
      "2021-09-07 17:18:42.626 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4818308651447296\n",
      "2021-09-07 17:18:42.627 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2412712574005127\n",
      "2021-09-07 17:18:42.627 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4818308651447296\n",
      "2021-09-07 17:18:42.629 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:42.630 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3076738715171814, 'baseline_loss': 0.8840552568435669, 'total_loss': 0.13435375690460205}\n",
      "2021-09-07 17:18:42.630 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.47789326310157776\n",
      "2021-09-07 17:18:42.631 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.206943154335022\n",
      "2021-09-07 17:18:42.632 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.47789326310157776\n",
      "2021-09-07 17:18:42.633 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:42.635 | INFO     | src.policies:train:123 - Epoch 590 / 800\n",
      "2021-09-07 17:18:42.635 | INFO     | src.policies:collect_trajectories:221 - Episode 1422\n",
      "2021-09-07 17:18:42.665 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.665 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:42.666 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:42.668 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:42.670 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5596462488174438, 'baseline_loss': 1.8335946798324585, 'total_loss': 0.3571510910987854}\n",
      "2021-09-07 17:18:42.671 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18545782566070557\n",
      "2021-09-07 17:18:42.672 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.8644113540649414\n",
      "2021-09-07 17:18:42.674 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18545782566070557\n",
      "2021-09-07 17:18:42.675 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:42.676 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:42.677 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5575437545776367, 'baseline_loss': 1.7087146043777466, 'total_loss': 0.2968135476112366}\n",
      "2021-09-07 17:18:42.678 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19569167494773865\n",
      "2021-09-07 17:18:42.678 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.0002071857452393\n",
      "2021-09-07 17:18:42.680 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19569167494773865\n",
      "2021-09-07 17:18:42.680 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:42.682 | INFO     | src.policies:train:123 - Epoch 591 / 800\n",
      "2021-09-07 17:18:42.682 | INFO     | src.policies:collect_trajectories:221 - Episode 1423\n",
      "2021-09-07 17:18:42.711 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.712 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:42.712 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:42.715 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:42.717 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.410888671875, 'baseline_loss': 0.9906244874000549, 'total_loss': 0.08442357182502747}\n",
      "2021-09-07 17:18:42.718 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4305073022842407\n",
      "2021-09-07 17:18:42.719 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8352670073509216\n",
      "2021-09-07 17:18:42.720 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4305073022842407\n",
      "2021-09-07 17:18:42.721 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:42.722 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:42.723 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4723183810710907, 'baseline_loss': 1.200576663017273, 'total_loss': 0.12796995043754578}\n",
      "2021-09-07 17:18:42.724 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19694608449935913\n",
      "2021-09-07 17:18:42.725 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8239436149597168\n",
      "2021-09-07 17:18:42.726 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19694608449935913\n",
      "2021-09-07 17:18:42.727 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:42.728 | INFO     | src.policies:train:123 - Epoch 592 / 800\n",
      "2021-09-07 17:18:42.729 | INFO     | src.policies:collect_trajectories:221 - Episode 1424\n",
      "2021-09-07 17:18:42.757 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.758 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:42.758 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:42.760 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:42.763 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27190279960632324, 'baseline_loss': 0.862457811832428, 'total_loss': 0.15932610630989075}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:42.764 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2930562198162079\n",
      "2021-09-07 17:18:42.764 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0396625995635986\n",
      "2021-09-07 17:18:42.766 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2930562198162079\n",
      "2021-09-07 17:18:42.767 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:42.769 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:42.770 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.368330717086792, 'baseline_loss': 0.7901374697685242, 'total_loss': 0.026738017797470093}\n",
      "2021-09-07 17:18:42.771 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22940462827682495\n",
      "2021-09-07 17:18:42.772 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9077534079551697\n",
      "2021-09-07 17:18:42.773 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22940462827682495\n",
      "2021-09-07 17:18:42.774 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:42.776 | INFO     | src.policies:train:123 - Epoch 593 / 800\n",
      "2021-09-07 17:18:42.776 | INFO     | src.policies:collect_trajectories:221 - Episode 1425\n",
      "2021-09-07 17:18:42.805 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.807 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:42.807 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:42.809 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:42.811 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31326648592948914, 'baseline_loss': 0.8745185732841492, 'total_loss': 0.12399280071258545}\n",
      "2021-09-07 17:18:42.812 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31735098361968994\n",
      "2021-09-07 17:18:42.813 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5615553855895996\n",
      "2021-09-07 17:18:42.814 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31735098361968994\n",
      "2021-09-07 17:18:42.815 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:42.816 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:42.818 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2824383080005646, 'baseline_loss': 0.8831841945648193, 'total_loss': 0.1591537892818451}\n",
      "2021-09-07 17:18:42.818 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2701576054096222\n",
      "2021-09-07 17:18:42.819 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7964272499084473\n",
      "2021-09-07 17:18:42.820 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2701576054096222\n",
      "2021-09-07 17:18:42.821 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:42.823 | INFO     | src.policies:train:123 - Epoch 594 / 800\n",
      "2021-09-07 17:18:42.823 | INFO     | src.policies:collect_trajectories:221 - Episode 1426\n",
      "2021-09-07 17:18:42.852 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.853 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:42.853 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:42.855 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:42.857 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.28586405515670776, 'baseline_loss': 0.7830536365509033, 'total_loss': 0.1056627631187439}\n",
      "2021-09-07 17:18:42.858 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3371753990650177\n",
      "2021-09-07 17:18:42.859 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9363082051277161\n",
      "2021-09-07 17:18:42.861 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3371753990650177\n",
      "2021-09-07 17:18:42.862 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:42.863 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:42.864 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20783545076847076, 'baseline_loss': 0.7285745143890381, 'total_loss': 0.15645180642604828}\n",
      "2021-09-07 17:18:42.865 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22234709560871124\n",
      "2021-09-07 17:18:42.866 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2561002969741821\n",
      "2021-09-07 17:18:42.867 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22234709560871124\n",
      "2021-09-07 17:18:42.868 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:42.869 | INFO     | src.policies:train:123 - Epoch 595 / 800\n",
      "2021-09-07 17:18:42.870 | INFO     | src.policies:collect_trajectories:221 - Episode 1427\n",
      "2021-09-07 17:18:42.887 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.888 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 114.0\n",
      "2021-09-07 17:18:42.888 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 114.0\n",
      "2021-09-07 17:18:42.888 | INFO     | src.policies:collect_trajectories:221 - Episode 1428\n",
      "2021-09-07 17:18:42.919 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.919 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:42.920 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 157.0\n",
      "2021-09-07 17:18:42.921 | WARNING  | src.policies:train:144 - The actual batch size is 314, instead of 200\n",
      "2021-09-07 17:18:42.923 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:42.925 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5693124532699585, 'baseline_loss': 1.2023863792419434, 'total_loss': 0.031880736351013184}\n",
      "2021-09-07 17:18:42.926 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.503898024559021\n",
      "2021-09-07 17:18:42.927 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1339471340179443\n",
      "2021-09-07 17:18:42.928 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999895691871643\n",
      "2021-09-07 17:18:42.929 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:42.930 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:42.931 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.567831814289093, 'baseline_loss': 1.3767058849334717, 'total_loss': 0.12052112817764282}\n",
      "2021-09-07 17:18:42.932 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7243834733963013\n",
      "2021-09-07 17:18:42.933 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4066911935806274\n",
      "2021-09-07 17:18:42.935 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:42.936 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:42.937 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:42.938 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.44421327114105225, 'baseline_loss': 1.4502133131027222, 'total_loss': 0.28089338541030884}\n",
      "2021-09-07 17:18:42.939 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1316864788532257\n",
      "2021-09-07 17:18:42.940 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7327853441238403\n",
      "2021-09-07 17:18:42.942 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1316864788532257\n",
      "2021-09-07 17:18:42.943 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:42.945 | INFO     | src.policies:train:123 - Epoch 596 / 800\n",
      "2021-09-07 17:18:42.945 | INFO     | src.policies:collect_trajectories:221 - Episode 1429\n",
      "2021-09-07 17:18:42.977 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:42.978 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:42.978 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:42.980 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:42.982 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3785907030105591, 'baseline_loss': 1.5964138507843018, 'total_loss': 0.4196162223815918}\n",
      "2021-09-07 17:18:42.983 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19323429465293884\n",
      "2021-09-07 17:18:42.984 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1400740146636963\n",
      "2021-09-07 17:18:42.985 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19323429465293884\n",
      "2021-09-07 17:18:42.986 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:42.987 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:42.988 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.363233745098114, 'baseline_loss': 1.328141212463379, 'total_loss': 0.30083686113357544}\n",
      "2021-09-07 17:18:42.989 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11781053990125656\n",
      "2021-09-07 17:18:42.990 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9495651125907898\n",
      "2021-09-07 17:18:42.991 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11781053990125656\n",
      "2021-09-07 17:18:42.992 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:42.993 | INFO     | src.policies:train:123 - Epoch 597 / 800\n",
      "2021-09-07 17:18:42.994 | INFO     | src.policies:collect_trajectories:221 - Episode 1430\n",
      "2021-09-07 17:18:43.025 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:43.026 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:43.027 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:43.029 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:43.031 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3838503360748291, 'baseline_loss': 1.0709624290466309, 'total_loss': 0.15163087844848633}\n",
      "2021-09-07 17:18:43.032 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32697489857673645\n",
      "2021-09-07 17:18:43.033 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0086339712142944\n",
      "2021-09-07 17:18:43.034 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32697489857673645\n",
      "2021-09-07 17:18:43.035 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:43.037 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:43.038 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3768079876899719, 'baseline_loss': 1.0143704414367676, 'total_loss': 0.13037723302841187}\n",
      "2021-09-07 17:18:43.038 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13352683186531067\n",
      "2021-09-07 17:18:43.039 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6766630411148071\n",
      "2021-09-07 17:18:43.040 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13352683186531067\n",
      "2021-09-07 17:18:43.041 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:43.043 | INFO     | src.policies:train:123 - Epoch 598 / 800\n",
      "2021-09-07 17:18:43.043 | INFO     | src.policies:collect_trajectories:221 - Episode 1431\n",
      "2021-09-07 17:18:43.235 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:43.236 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:43.236 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:43.238 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:43.240 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.15509481728076935, 'baseline_loss': 0.555637776851654, 'total_loss': 0.12272407114505768}\n",
      "2021-09-07 17:18:43.241 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3086053133010864\n",
      "2021-09-07 17:18:43.242 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8134040832519531\n",
      "2021-09-07 17:18:43.244 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3086053133010864\n",
      "2021-09-07 17:18:43.245 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:43.246 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:43.247 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.14225807785987854, 'baseline_loss': 0.6553081274032593, 'total_loss': 0.1853959858417511}\n",
      "2021-09-07 17:18:43.248 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16337928175926208\n",
      "2021-09-07 17:18:43.249 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3826655149459839\n",
      "2021-09-07 17:18:43.251 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16337928175926208\n",
      "2021-09-07 17:18:43.252 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:43.253 | INFO     | src.policies:train:123 - Epoch 599 / 800\n",
      "2021-09-07 17:18:43.254 | INFO     | src.policies:collect_trajectories:221 - Episode 1432\n",
      "2021-09-07 17:18:43.283 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:43.284 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:43.285 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:43.287 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:43.289 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4420468509197235, 'baseline_loss': 1.2480344772338867, 'total_loss': 0.18197038769721985}\n",
      "2021-09-07 17:18:43.290 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2682429850101471\n",
      "2021-09-07 17:18:43.291 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.7218642234802246\n",
      "2021-09-07 17:18:43.292 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2682429850101471\n",
      "2021-09-07 17:18:43.293 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999999403953552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:43.294 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:43.295 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4885525107383728, 'baseline_loss': 1.18677818775177, 'total_loss': 0.10483658313751221}\n",
      "2021-09-07 17:18:43.296 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07817389070987701\n",
      "2021-09-07 17:18:43.297 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9368093013763428\n",
      "2021-09-07 17:18:43.299 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07817389070987701\n",
      "2021-09-07 17:18:43.300 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:43.302 | INFO     | src.policies:train:123 - Epoch 600 / 800\n",
      "2021-09-07 17:18:43.302 | INFO     | src.policies:collect_trajectories:221 - Episode 1433\n",
      "2021-09-07 17:18:43.332 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:43.332 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:43.333 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:43.335 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:43.337 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2067061960697174, 'baseline_loss': 0.5955445766448975, 'total_loss': 0.09106609225273132}\n",
      "2021-09-07 17:18:43.339 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2566111981868744\n",
      "2021-09-07 17:18:43.340 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0595899820327759\n",
      "2021-09-07 17:18:43.341 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2566111981868744\n",
      "2021-09-07 17:18:43.342 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:43.343 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:43.344 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1988086700439453, 'baseline_loss': 0.5831279754638672, 'total_loss': 0.09275531768798828}\n",
      "2021-09-07 17:18:43.344 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.27010294795036316\n",
      "2021-09-07 17:18:43.345 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6191879510879517\n",
      "2021-09-07 17:18:43.346 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.27010294795036316\n",
      "2021-09-07 17:18:43.347 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:43.349 | INFO     | src.policies:train:123 - Epoch 601 / 800\n",
      "2021-09-07 17:18:43.349 | INFO     | src.policies:collect_trajectories:221 - Episode 1434\n",
      "2021-09-07 17:18:43.496 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:43.496 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:43.497 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:43.499 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:43.502 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.190263569355011, 'baseline_loss': 0.9542644023895264, 'total_loss': 0.2868686318397522}\n",
      "2021-09-07 17:18:43.503 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14125259220600128\n",
      "2021-09-07 17:18:43.504 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.571037769317627\n",
      "2021-09-07 17:18:43.505 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14125259220600128\n",
      "2021-09-07 17:18:43.506 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:43.507 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:43.508 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2809887230396271, 'baseline_loss': 1.1022310256958008, 'total_loss': 0.2701267898082733}\n",
      "2021-09-07 17:18:43.509 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3845193684101105\n",
      "2021-09-07 17:18:43.510 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.097780704498291\n",
      "2021-09-07 17:18:43.511 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3845193684101105\n",
      "2021-09-07 17:18:43.512 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:43.513 | INFO     | src.policies:train:123 - Epoch 602 / 800\n",
      "2021-09-07 17:18:43.514 | INFO     | src.policies:collect_trajectories:221 - Episode 1435\n",
      "2021-09-07 17:18:43.523 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:43.524 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 61.0\n",
      "2021-09-07 17:18:43.524 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 61.0\n",
      "2021-09-07 17:18:43.525 | INFO     | src.policies:collect_trajectories:221 - Episode 1436\n",
      "2021-09-07 17:18:43.559 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:43.560 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:43.560 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 130.5\n",
      "2021-09-07 17:18:43.561 | WARNING  | src.policies:train:144 - The actual batch size is 261, instead of 200\n",
      "2021-09-07 17:18:43.563 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:43.566 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4699290096759796, 'baseline_loss': 1.452903389930725, 'total_loss': 0.25652268528938293}\n",
      "2021-09-07 17:18:43.567 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.46488863229751587\n",
      "2021-09-07 17:18:43.568 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7417701482772827\n",
      "2021-09-07 17:18:43.569 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.46488863229751587\n",
      "2021-09-07 17:18:43.570 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:43.572 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:43.573 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.49996551871299744, 'baseline_loss': 1.5301809310913086, 'total_loss': 0.26512494683265686}\n",
      "2021-09-07 17:18:43.574 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.950564980506897\n",
      "2021-09-07 17:18:43.575 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6575855016708374\n",
      "2021-09-07 17:18:43.576 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:43.577 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:43.579 | INFO     | src.policies:train:123 - Epoch 603 / 800\n",
      "2021-09-07 17:18:43.579 | INFO     | src.policies:collect_trajectories:221 - Episode 1437\n",
      "2021-09-07 17:18:43.606 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:43.607 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:43.607 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:43.610 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:43.612 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1880749762058258, 'baseline_loss': 0.4878750741481781, 'total_loss': 0.055862560868263245}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:43.613 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17343492805957794\n",
      "2021-09-07 17:18:43.614 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.6620967388153076\n",
      "2021-09-07 17:18:43.615 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17343492805957794\n",
      "2021-09-07 17:18:43.616 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:43.617 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:43.618 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20193175971508026, 'baseline_loss': 0.48283684253692627, 'total_loss': 0.039486661553382874}\n",
      "2021-09-07 17:18:43.619 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13193991780281067\n",
      "2021-09-07 17:18:43.620 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.0766420364379883\n",
      "2021-09-07 17:18:43.621 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13193991780281067\n",
      "2021-09-07 17:18:43.622 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:43.623 | INFO     | src.policies:train:123 - Epoch 604 / 800\n",
      "2021-09-07 17:18:43.624 | INFO     | src.policies:collect_trajectories:221 - Episode 1438\n",
      "2021-09-07 17:18:43.639 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:43.640 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 102.0\n",
      "2021-09-07 17:18:43.640 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 102.0\n",
      "2021-09-07 17:18:43.641 | INFO     | src.policies:collect_trajectories:221 - Episode 1439\n",
      "2021-09-07 17:18:43.666 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:43.666 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 171.0\n",
      "2021-09-07 17:18:43.667 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 136.5\n",
      "2021-09-07 17:18:43.667 | WARNING  | src.policies:train:144 - The actual batch size is 273, instead of 200\n",
      "2021-09-07 17:18:43.670 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:43.672 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.385219544172287, 'baseline_loss': 0.7730866074562073, 'total_loss': 0.0013237595558166504}\n",
      "2021-09-07 17:18:43.673 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25873252749443054\n",
      "2021-09-07 17:18:43.674 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5914690494537354\n",
      "2021-09-07 17:18:43.675 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25873252749443054\n",
      "2021-09-07 17:18:43.676 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:43.677 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:43.678 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4198300540447235, 'baseline_loss': 0.7179403901100159, 'total_loss': -0.060859858989715576}\n",
      "2021-09-07 17:18:43.679 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.575350821018219\n",
      "2021-09-07 17:18:43.680 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1500859260559082\n",
      "2021-09-07 17:18:43.681 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:43.682 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:43.683 | INFO     | src.policies:train:123 - Epoch 605 / 800\n",
      "2021-09-07 17:18:43.684 | INFO     | src.policies:collect_trajectories:221 - Episode 1440\n",
      "2021-09-07 17:18:43.694 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:43.694 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 66.0\n",
      "2021-09-07 17:18:43.695 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 66.0\n",
      "2021-09-07 17:18:43.695 | INFO     | src.policies:collect_trajectories:221 - Episode 1441\n",
      "2021-09-07 17:18:43.766 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:43.767 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 184.0\n",
      "2021-09-07 17:18:43.767 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 125.0\n",
      "2021-09-07 17:18:43.768 | WARNING  | src.policies:train:144 - The actual batch size is 250, instead of 200\n",
      "2021-09-07 17:18:43.771 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:43.773 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4622461199760437, 'baseline_loss': 1.303593635559082, 'total_loss': 0.18955069780349731}\n",
      "2021-09-07 17:18:43.774 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19784963130950928\n",
      "2021-09-07 17:18:43.775 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.468152642250061\n",
      "2021-09-07 17:18:43.776 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19784963130950928\n",
      "2021-09-07 17:18:43.778 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:43.779 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:43.780 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3164491653442383, 'baseline_loss': 1.0058783292770386, 'total_loss': 0.186489999294281}\n",
      "2021-09-07 17:18:43.781 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4052869379520416\n",
      "2021-09-07 17:18:43.782 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.5439281463623047\n",
      "2021-09-07 17:18:43.783 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4052869379520416\n",
      "2021-09-07 17:18:43.784 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:43.785 | INFO     | src.policies:train:123 - Epoch 606 / 800\n",
      "2021-09-07 17:18:43.785 | INFO     | src.policies:collect_trajectories:221 - Episode 1442\n",
      "2021-09-07 17:18:43.817 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:43.818 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:43.818 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:43.820 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:43.822 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.19251729547977448, 'baseline_loss': 0.48654091358184814, 'total_loss': 0.0507531613111496}\n",
      "2021-09-07 17:18:43.823 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.28362682461738586\n",
      "2021-09-07 17:18:43.824 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.5668892860412598\n",
      "2021-09-07 17:18:43.825 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.28362682461738586\n",
      "2021-09-07 17:18:43.826 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:43.828 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:43.829 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20901697874069214, 'baseline_loss': 0.40860453248023987, 'total_loss': -0.004714712500572205}\n",
      "2021-09-07 17:18:43.830 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12574395537376404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:43.831 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8420733213424683\n",
      "2021-09-07 17:18:43.832 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12574395537376404\n",
      "2021-09-07 17:18:43.833 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:43.834 | INFO     | src.policies:train:123 - Epoch 607 / 800\n",
      "2021-09-07 17:18:43.835 | INFO     | src.policies:collect_trajectories:221 - Episode 1443\n",
      "2021-09-07 17:18:43.863 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:43.864 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:43.864 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:43.867 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:43.869 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2877531349658966, 'baseline_loss': 0.5809271335601807, 'total_loss': 0.0027104318141937256}\n",
      "2021-09-07 17:18:43.870 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.46318554878234863\n",
      "2021-09-07 17:18:43.871 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.069840908050537\n",
      "2021-09-07 17:18:43.872 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.46318554878234863\n",
      "2021-09-07 17:18:43.873 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:43.874 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:43.875 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2957272231578827, 'baseline_loss': 0.5569499731063843, 'total_loss': -0.017252236604690552}\n",
      "2021-09-07 17:18:43.876 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18594089150428772\n",
      "2021-09-07 17:18:43.877 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4301486015319824\n",
      "2021-09-07 17:18:43.879 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18594089150428772\n",
      "2021-09-07 17:18:43.880 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:43.881 | INFO     | src.policies:train:123 - Epoch 608 / 800\n",
      "2021-09-07 17:18:43.882 | INFO     | src.policies:collect_trajectories:221 - Episode 1444\n",
      "2021-09-07 17:18:43.910 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:43.910 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:43.911 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:43.914 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:43.916 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4190763831138611, 'baseline_loss': 0.9405680894851685, 'total_loss': 0.051207661628723145}\n",
      "2021-09-07 17:18:43.917 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4522680342197418\n",
      "2021-09-07 17:18:43.918 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7000936269760132\n",
      "2021-09-07 17:18:43.919 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4522680342197418\n",
      "2021-09-07 17:18:43.920 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:43.922 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:43.923 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3095425069332123, 'baseline_loss': 1.1269209384918213, 'total_loss': 0.25391796231269836}\n",
      "2021-09-07 17:18:43.924 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31530311703681946\n",
      "2021-09-07 17:18:43.925 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.920680046081543\n",
      "2021-09-07 17:18:43.926 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31530311703681946\n",
      "2021-09-07 17:18:43.927 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:43.928 | INFO     | src.policies:train:123 - Epoch 609 / 800\n",
      "2021-09-07 17:18:43.929 | INFO     | src.policies:collect_trajectories:221 - Episode 1445\n",
      "2021-09-07 17:18:43.958 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:43.959 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:43.959 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:43.963 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:43.965 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.26772305369377136, 'baseline_loss': 0.8153277635574341, 'total_loss': 0.13994082808494568}\n",
      "2021-09-07 17:18:43.966 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3469935357570648\n",
      "2021-09-07 17:18:43.967 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.786709189414978\n",
      "2021-09-07 17:18:43.968 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3469935357570648\n",
      "2021-09-07 17:18:43.969 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:43.970 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:43.972 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.28929129242897034, 'baseline_loss': 0.7193818092346191, 'total_loss': 0.07039961218833923}\n",
      "2021-09-07 17:18:43.972 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16872723400592804\n",
      "2021-09-07 17:18:43.973 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7282860279083252\n",
      "2021-09-07 17:18:43.974 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16872723400592804\n",
      "2021-09-07 17:18:43.975 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:43.976 | INFO     | src.policies:train:123 - Epoch 610 / 800\n",
      "2021-09-07 17:18:43.977 | INFO     | src.policies:collect_trajectories:221 - Episode 1446\n",
      "2021-09-07 17:18:44.008 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:44.008 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:44.009 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:44.010 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:44.014 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6236733794212341, 'baseline_loss': 2.2080798149108887, 'total_loss': 0.4803665280342102}\n",
      "2021-09-07 17:18:44.015 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3892626166343689\n",
      "2021-09-07 17:18:44.016 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.208277463912964\n",
      "2021-09-07 17:18:44.017 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3892626166343689\n",
      "2021-09-07 17:18:44.018 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:44.019 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:44.020 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8071308135986328, 'baseline_loss': 2.746861219406128, 'total_loss': 0.5662997961044312}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:44.021 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.34482482075691223\n",
      "2021-09-07 17:18:44.022 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.5065619945526123\n",
      "2021-09-07 17:18:44.023 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.34482482075691223\n",
      "2021-09-07 17:18:44.024 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:44.026 | INFO     | src.policies:train:123 - Epoch 611 / 800\n",
      "2021-09-07 17:18:44.026 | INFO     | src.policies:collect_trajectories:221 - Episode 1447\n",
      "2021-09-07 17:18:44.055 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:44.056 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:44.056 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:44.058 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:44.060 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4570247530937195, 'baseline_loss': 1.0481575727462769, 'total_loss': 0.06705403327941895}\n",
      "2021-09-07 17:18:44.061 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2937585413455963\n",
      "2021-09-07 17:18:44.062 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7045599818229675\n",
      "2021-09-07 17:18:44.063 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2937585413455963\n",
      "2021-09-07 17:18:44.064 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:44.065 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:44.067 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41659000515937805, 'baseline_loss': 1.0240801572799683, 'total_loss': 0.09545007348060608}\n",
      "2021-09-07 17:18:44.068 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1659165322780609\n",
      "2021-09-07 17:18:44.068 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.855435311794281\n",
      "2021-09-07 17:18:44.069 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1659165322780609\n",
      "2021-09-07 17:18:44.070 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:44.072 | INFO     | src.policies:train:123 - Epoch 612 / 800\n",
      "2021-09-07 17:18:44.072 | INFO     | src.policies:collect_trajectories:221 - Episode 1448\n",
      "2021-09-07 17:18:44.102 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:44.103 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:44.103 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:44.105 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:44.108 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2687019109725952, 'baseline_loss': 0.6482077836990356, 'total_loss': 0.05540198087692261}\n",
      "2021-09-07 17:18:44.109 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2334917038679123\n",
      "2021-09-07 17:18:44.110 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0011205673217773\n",
      "2021-09-07 17:18:44.111 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2334917038679123\n",
      "2021-09-07 17:18:44.112 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:44.113 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:44.115 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4341149926185608, 'baseline_loss': 0.8341450691223145, 'total_loss': -0.017042458057403564}\n",
      "2021-09-07 17:18:44.115 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.26442307233810425\n",
      "2021-09-07 17:18:44.116 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.159458041191101\n",
      "2021-09-07 17:18:44.118 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.26442307233810425\n",
      "2021-09-07 17:18:44.119 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:44.121 | INFO     | src.policies:train:123 - Epoch 613 / 800\n",
      "2021-09-07 17:18:44.121 | INFO     | src.policies:collect_trajectories:221 - Episode 1449\n",
      "2021-09-07 17:18:44.155 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:44.156 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:44.156 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:44.159 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:44.161 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2796948552131653, 'baseline_loss': 0.5684965252876282, 'total_loss': 0.004553407430648804}\n",
      "2021-09-07 17:18:44.162 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4276905953884125\n",
      "2021-09-07 17:18:44.163 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6975947022438049\n",
      "2021-09-07 17:18:44.164 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4276905953884125\n",
      "2021-09-07 17:18:44.165 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:44.166 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:44.167 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.35101833939552307, 'baseline_loss': 0.6161090135574341, 'total_loss': -0.04296383261680603}\n",
      "2021-09-07 17:18:44.168 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12558892369270325\n",
      "2021-09-07 17:18:44.169 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8026731610298157\n",
      "2021-09-07 17:18:44.170 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12558892369270325\n",
      "2021-09-07 17:18:44.171 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:44.172 | INFO     | src.policies:train:123 - Epoch 614 / 800\n",
      "2021-09-07 17:18:44.173 | INFO     | src.policies:collect_trajectories:221 - Episode 1450\n",
      "2021-09-07 17:18:44.200 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:44.201 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:44.201 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:44.203 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:44.205 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24653613567352295, 'baseline_loss': 0.4389151632785797, 'total_loss': -0.027078554034233093}\n",
      "2021-09-07 17:18:44.206 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07431410253047943\n",
      "2021-09-07 17:18:44.207 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1002004146575928\n",
      "2021-09-07 17:18:44.208 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07431410253047943\n",
      "2021-09-07 17:18:44.209 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:44.210 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:44.211 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.26144635677337646, 'baseline_loss': 0.45284390449523926, 'total_loss': -0.035024404525756836}\n",
      "2021-09-07 17:18:44.212 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09167583286762238\n",
      "2021-09-07 17:18:44.213 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9416763186454773\n",
      "2021-09-07 17:18:44.214 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09167583286762238\n",
      "2021-09-07 17:18:44.215 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:44.216 | INFO     | src.policies:train:123 - Epoch 615 / 800\n",
      "2021-09-07 17:18:44.216 | INFO     | src.policies:collect_trajectories:221 - Episode 1451\n",
      "2021-09-07 17:18:44.245 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:44.246 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:44.246 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:44.249 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:44.251 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5738723874092102, 'baseline_loss': 1.717545986175537, 'total_loss': 0.28490060567855835}\n",
      "2021-09-07 17:18:44.252 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.28812849521636963\n",
      "2021-09-07 17:18:44.253 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.3848564624786377\n",
      "2021-09-07 17:18:44.254 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.28812849521636963\n",
      "2021-09-07 17:18:44.255 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:44.257 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:44.258 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7116725444793701, 'baseline_loss': 2.153733015060425, 'total_loss': 0.3651939630508423}\n",
      "2021-09-07 17:18:44.259 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21825651824474335\n",
      "2021-09-07 17:18:44.259 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.437188148498535\n",
      "2021-09-07 17:18:44.322 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21825651824474335\n",
      "2021-09-07 17:18:44.324 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:44.326 | INFO     | src.policies:train:123 - Epoch 616 / 800\n",
      "2021-09-07 17:18:44.327 | INFO     | src.policies:collect_trajectories:221 - Episode 1452\n",
      "2021-09-07 17:18:44.357 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:44.357 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:44.358 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:44.360 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:44.362 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20089907944202423, 'baseline_loss': 0.3799794018268585, 'total_loss': -0.01090937852859497}\n",
      "2021-09-07 17:18:44.363 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25758540630340576\n",
      "2021-09-07 17:18:44.364 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2591639757156372\n",
      "2021-09-07 17:18:44.365 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25758540630340576\n",
      "2021-09-07 17:18:44.366 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:44.368 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:44.369 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23635822534561157, 'baseline_loss': 0.41045376658439636, 'total_loss': -0.03113134205341339}\n",
      "2021-09-07 17:18:44.369 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.41217589378356934\n",
      "2021-09-07 17:18:44.370 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4292019605636597\n",
      "2021-09-07 17:18:44.371 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.41217589378356934\n",
      "2021-09-07 17:18:44.372 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:44.374 | INFO     | src.policies:train:123 - Epoch 617 / 800\n",
      "2021-09-07 17:18:44.375 | INFO     | src.policies:collect_trajectories:221 - Episode 1453\n",
      "2021-09-07 17:18:44.404 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:44.405 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:44.406 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:44.408 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:44.410 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6065601706504822, 'baseline_loss': 1.63448166847229, 'total_loss': 0.21068066358566284}\n",
      "2021-09-07 17:18:44.411 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5135664343833923\n",
      "2021-09-07 17:18:44.412 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.524430990219116\n",
      "2021-09-07 17:18:44.414 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:44.415 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:44.416 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:44.417 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6275513172149658, 'baseline_loss': 1.636383056640625, 'total_loss': 0.19064021110534668}\n",
      "2021-09-07 17:18:44.418 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3983875811100006\n",
      "2021-09-07 17:18:44.419 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.7639272212982178\n",
      "2021-09-07 17:18:44.421 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3983875811100006\n",
      "2021-09-07 17:18:44.422 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:44.423 | INFO     | src.policies:train:123 - Epoch 618 / 800\n",
      "2021-09-07 17:18:44.423 | INFO     | src.policies:collect_trajectories:221 - Episode 1454\n",
      "2021-09-07 17:18:44.454 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:44.455 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:44.456 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:44.458 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:44.460 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4793016016483307, 'baseline_loss': 1.5310614109039307, 'total_loss': 0.28622910380363464}\n",
      "2021-09-07 17:18:44.461 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7265862822532654\n",
      "2021-09-07 17:18:44.462 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.211374521255493\n",
      "2021-09-07 17:18:44.463 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:44.464 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:44.466 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:44.467 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5149837732315063, 'baseline_loss': 1.4602454900741577, 'total_loss': 0.2151389718055725}\n",
      "2021-09-07 17:18:44.468 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2791266143321991\n",
      "2021-09-07 17:18:44.469 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.404263496398926\n",
      "2021-09-07 17:18:44.470 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2791266143321991\n",
      "2021-09-07 17:18:44.471 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:44.472 | INFO     | src.policies:train:123 - Epoch 619 / 800\n",
      "2021-09-07 17:18:44.473 | INFO     | src.policies:collect_trajectories:221 - Episode 1455\n",
      "2021-09-07 17:18:44.501 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:44.502 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:44.502 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:44.504 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:44.507 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4953882694244385, 'baseline_loss': 1.0688233375549316, 'total_loss': 0.039023399353027344}\n",
      "2021-09-07 17:18:44.508 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21416069567203522\n",
      "2021-09-07 17:18:44.509 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7495823502540588\n",
      "2021-09-07 17:18:44.510 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21416069567203522\n",
      "2021-09-07 17:18:44.511 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:44.512 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:44.514 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.38712239265441895, 'baseline_loss': 0.921846330165863, 'total_loss': 0.07380077242851257}\n",
      "2021-09-07 17:18:44.516 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21665722131729126\n",
      "2021-09-07 17:18:44.517 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8461366891860962\n",
      "2021-09-07 17:18:44.518 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21665722131729126\n",
      "2021-09-07 17:18:44.519 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:44.521 | INFO     | src.policies:train:123 - Epoch 620 / 800\n",
      "2021-09-07 17:18:44.521 | INFO     | src.policies:collect_trajectories:221 - Episode 1456\n",
      "2021-09-07 17:18:44.552 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:44.552 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:44.553 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:44.554 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:44.557 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6127945184707642, 'baseline_loss': 2.6484596729278564, 'total_loss': 0.7114353179931641}\n",
      "2021-09-07 17:18:44.558 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.627406656742096\n",
      "2021-09-07 17:18:44.559 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.326331615447998\n",
      "2021-09-07 17:18:44.560 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:44.561 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:44.562 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:44.563 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.565036416053772, 'baseline_loss': 2.4060006141662598, 'total_loss': 0.6379638910293579}\n",
      "2021-09-07 17:18:44.564 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18951047956943512\n",
      "2021-09-07 17:18:44.565 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.912242889404297\n",
      "2021-09-07 17:18:44.566 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18951047956943512\n",
      "2021-09-07 17:18:44.567 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:44.568 | INFO     | src.policies:train:123 - Epoch 621 / 800\n",
      "2021-09-07 17:18:44.568 | INFO     | src.policies:collect_trajectories:221 - Episode 1457\n",
      "2021-09-07 17:18:44.597 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:44.597 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:44.598 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:44.600 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:44.601 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.21984077990055084, 'baseline_loss': 0.7480177283287048, 'total_loss': 0.15416808426380157}\n",
      "2021-09-07 17:18:44.602 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1277768611907959\n",
      "2021-09-07 17:18:44.603 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7228355407714844\n",
      "2021-09-07 17:18:44.605 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1277768611907959\n",
      "2021-09-07 17:18:44.606 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:44.607 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:44.608 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.21804578602313995, 'baseline_loss': 0.6940422058105469, 'total_loss': 0.12897531688213348}\n",
      "2021-09-07 17:18:44.609 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.299553781747818\n",
      "2021-09-07 17:18:44.609 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1576931476593018\n",
      "2021-09-07 17:18:44.611 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.299553781747818\n",
      "2021-09-07 17:18:44.612 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:44.613 | INFO     | src.policies:train:123 - Epoch 622 / 800\n",
      "2021-09-07 17:18:44.613 | INFO     | src.policies:collect_trajectories:221 - Episode 1458\n",
      "2021-09-07 17:18:44.643 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:44.644 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:44.644 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:44.646 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:44.648 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42464083433151245, 'baseline_loss': 0.8698882460594177, 'total_loss': 0.010303288698196411}\n",
      "2021-09-07 17:18:44.649 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22084389626979828\n",
      "2021-09-07 17:18:44.650 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.413438320159912\n",
      "2021-09-07 17:18:44.652 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22084389626979828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:44.654 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:44.656 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:44.658 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4351358413696289, 'baseline_loss': 1.10561203956604, 'total_loss': 0.11767017841339111}\n",
      "2021-09-07 17:18:44.659 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14469456672668457\n",
      "2021-09-07 17:18:44.661 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.943028450012207\n",
      "2021-09-07 17:18:44.663 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14469456672668457\n",
      "2021-09-07 17:18:44.664 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:44.666 | INFO     | src.policies:train:123 - Epoch 623 / 800\n",
      "2021-09-07 17:18:44.666 | INFO     | src.policies:collect_trajectories:221 - Episode 1459\n",
      "2021-09-07 17:18:44.695 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:44.695 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:44.696 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:44.699 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:44.701 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7689769864082336, 'baseline_loss': 2.6657891273498535, 'total_loss': 0.5639175772666931}\n",
      "2021-09-07 17:18:44.702 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6117620468139648\n",
      "2021-09-07 17:18:44.703 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.574668884277344\n",
      "2021-09-07 17:18:44.704 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:44.705 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999999403953552\n",
      "2021-09-07 17:18:44.706 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:44.708 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8374804854393005, 'baseline_loss': 2.666255235671997, 'total_loss': 0.495647132396698}\n",
      "2021-09-07 17:18:44.708 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5195081233978271\n",
      "2021-09-07 17:18:44.709 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.8903987407684326\n",
      "2021-09-07 17:18:44.710 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:44.711 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:44.713 | INFO     | src.policies:train:123 - Epoch 624 / 800\n",
      "2021-09-07 17:18:44.713 | INFO     | src.policies:collect_trajectories:221 - Episode 1460\n",
      "2021-09-07 17:18:44.742 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:44.743 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:44.743 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:44.745 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:44.747 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2258935421705246, 'baseline_loss': 0.8882890343666077, 'total_loss': 0.21825097501277924}\n",
      "2021-09-07 17:18:44.748 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.46848970651626587\n",
      "2021-09-07 17:18:44.749 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9535088539123535\n",
      "2021-09-07 17:18:44.750 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.46848970651626587\n",
      "2021-09-07 17:18:44.751 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:44.753 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:44.754 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.25795358419418335, 'baseline_loss': 1.0881472826004028, 'total_loss': 0.28612005710601807}\n",
      "2021-09-07 17:18:44.755 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.38085877895355225\n",
      "2021-09-07 17:18:44.755 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6198372840881348\n",
      "2021-09-07 17:18:44.756 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.38085877895355225\n",
      "2021-09-07 17:18:44.757 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:44.759 | INFO     | src.policies:train:123 - Epoch 625 / 800\n",
      "2021-09-07 17:18:44.759 | INFO     | src.policies:collect_trajectories:221 - Episode 1461\n",
      "2021-09-07 17:18:44.788 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:44.789 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:44.789 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:44.791 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:44.793 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.47183895111083984, 'baseline_loss': 1.144616723060608, 'total_loss': 0.10046941041946411}\n",
      "2021-09-07 17:18:44.794 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12080560624599457\n",
      "2021-09-07 17:18:44.795 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9595812559127808\n",
      "2021-09-07 17:18:44.796 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12080560624599457\n",
      "2021-09-07 17:18:44.798 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:44.799 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:44.801 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5556971430778503, 'baseline_loss': 1.4420965909957886, 'total_loss': 0.16535115242004395}\n",
      "2021-09-07 17:18:44.801 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4438484311103821\n",
      "2021-09-07 17:18:44.802 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.4891626834869385\n",
      "2021-09-07 17:18:44.803 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4438484311103821\n",
      "2021-09-07 17:18:44.804 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:44.806 | INFO     | src.policies:train:123 - Epoch 626 / 800\n",
      "2021-09-07 17:18:44.806 | INFO     | src.policies:collect_trajectories:221 - Episode 1462\n",
      "2021-09-07 17:18:44.888 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:44.889 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:44.889 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:44.891 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:44.894 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4973854422569275, 'baseline_loss': 1.6612833738327026, 'total_loss': 0.33325624465942383}\n",
      "2021-09-07 17:18:44.895 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2136746197938919\n",
      "2021-09-07 17:18:44.897 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.291459083557129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:44.898 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2136746197938919\n",
      "2021-09-07 17:18:44.899 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:44.900 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:44.901 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.49169591069221497, 'baseline_loss': 1.7329188585281372, 'total_loss': 0.37476351857185364}\n",
      "2021-09-07 17:18:44.902 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3738683760166168\n",
      "2021-09-07 17:18:44.903 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.0979790687561035\n",
      "2021-09-07 17:18:44.904 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3738683760166168\n",
      "2021-09-07 17:18:44.905 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:44.906 | INFO     | src.policies:train:123 - Epoch 627 / 800\n",
      "2021-09-07 17:18:44.907 | INFO     | src.policies:collect_trajectories:221 - Episode 1463\n",
      "2021-09-07 17:18:44.935 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:44.935 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:44.936 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:44.937 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:44.941 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7105187177658081, 'baseline_loss': 2.390636920928955, 'total_loss': 0.48479974269866943}\n",
      "2021-09-07 17:18:44.943 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16117042303085327\n",
      "2021-09-07 17:18:44.944 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.1710968017578125\n",
      "2021-09-07 17:18:44.945 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16117042303085327\n",
      "2021-09-07 17:18:44.946 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:44.948 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:44.949 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5691885352134705, 'baseline_loss': 2.230184555053711, 'total_loss': 0.545903742313385}\n",
      "2021-09-07 17:18:44.950 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3183116018772125\n",
      "2021-09-07 17:18:44.951 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.098236083984375\n",
      "2021-09-07 17:18:44.952 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3183116018772125\n",
      "2021-09-07 17:18:44.953 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:44.955 | INFO     | src.policies:train:123 - Epoch 628 / 800\n",
      "2021-09-07 17:18:44.955 | INFO     | src.policies:collect_trajectories:221 - Episode 1464\n",
      "2021-09-07 17:18:44.983 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:44.984 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:44.984 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:44.988 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:44.990 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5980497002601624, 'baseline_loss': 1.6352927684783936, 'total_loss': 0.21959668397903442}\n",
      "2021-09-07 17:18:44.991 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30619287490844727\n",
      "2021-09-07 17:18:44.992 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.9354920387268066\n",
      "2021-09-07 17:18:44.993 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30619287490844727\n",
      "2021-09-07 17:18:44.994 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:44.996 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:44.997 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.46632400155067444, 'baseline_loss': 1.3944108486175537, 'total_loss': 0.23088142275810242}\n",
      "2021-09-07 17:18:44.998 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5515891313552856\n",
      "2021-09-07 17:18:44.999 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.742202043533325\n",
      "2021-09-07 17:18:45.000 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:45.001 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:45.003 | INFO     | src.policies:train:123 - Epoch 629 / 800\n",
      "2021-09-07 17:18:45.003 | INFO     | src.policies:collect_trajectories:221 - Episode 1465\n",
      "2021-09-07 17:18:45.033 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:45.033 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:45.034 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:45.036 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:45.039 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1949513852596283, 'baseline_loss': 0.7105802893638611, 'total_loss': 0.16033875942230225}\n",
      "2021-09-07 17:18:45.040 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2809763252735138\n",
      "2021-09-07 17:18:45.041 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.176116704940796\n",
      "2021-09-07 17:18:45.042 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2809763252735138\n",
      "2021-09-07 17:18:45.043 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:45.044 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:45.045 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.25875499844551086, 'baseline_loss': 0.8082603216171265, 'total_loss': 0.14537516236305237}\n",
      "2021-09-07 17:18:45.046 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.06396740674972534\n",
      "2021-09-07 17:18:45.046 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6494413614273071\n",
      "2021-09-07 17:18:45.047 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.06396740674972534\n",
      "2021-09-07 17:18:45.048 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:45.050 | INFO     | src.policies:train:123 - Epoch 630 / 800\n",
      "2021-09-07 17:18:45.050 | INFO     | src.policies:collect_trajectories:221 - Episode 1466\n",
      "2021-09-07 17:18:45.079 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:45.079 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:45.080 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:45.082 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:45.084 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.38856425881385803, 'baseline_loss': 1.343333125114441, 'total_loss': 0.2831023037433624}\n",
      "2021-09-07 17:18:45.084 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1972937285900116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:45.085 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8550032377243042\n",
      "2021-09-07 17:18:45.087 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1972937285900116\n",
      "2021-09-07 17:18:45.088 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:45.089 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:45.091 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3694514334201813, 'baseline_loss': 1.234114408493042, 'total_loss': 0.24760577082633972}\n",
      "2021-09-07 17:18:45.092 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.40959858894348145\n",
      "2021-09-07 17:18:45.092 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6521518230438232\n",
      "2021-09-07 17:18:45.094 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.40959858894348145\n",
      "2021-09-07 17:18:45.095 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:45.096 | INFO     | src.policies:train:123 - Epoch 631 / 800\n",
      "2021-09-07 17:18:45.096 | INFO     | src.policies:collect_trajectories:221 - Episode 1467\n",
      "2021-09-07 17:18:45.125 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:45.126 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:45.126 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:45.128 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:45.130 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36624497175216675, 'baseline_loss': 0.922585129737854, 'total_loss': 0.09504759311676025}\n",
      "2021-09-07 17:18:45.131 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2204400599002838\n",
      "2021-09-07 17:18:45.132 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8765413761138916\n",
      "2021-09-07 17:18:45.133 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2204400599002838\n",
      "2021-09-07 17:18:45.134 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:45.135 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:45.136 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.33004245162010193, 'baseline_loss': 0.9023269414901733, 'total_loss': 0.12112101912498474}\n",
      "2021-09-07 17:18:45.137 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15536248683929443\n",
      "2021-09-07 17:18:45.138 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0007675886154175\n",
      "2021-09-07 17:18:45.139 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15536248683929443\n",
      "2021-09-07 17:18:45.140 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:45.142 | INFO     | src.policies:train:123 - Epoch 632 / 800\n",
      "2021-09-07 17:18:45.142 | INFO     | src.policies:collect_trajectories:221 - Episode 1468\n",
      "2021-09-07 17:18:45.170 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:45.170 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 193.0\n",
      "2021-09-07 17:18:45.171 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 193.0\n",
      "2021-09-07 17:18:45.171 | INFO     | src.policies:collect_trajectories:221 - Episode 1469\n",
      "2021-09-07 17:18:45.202 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:45.203 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:45.203 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 196.5\n",
      "2021-09-07 17:18:45.204 | WARNING  | src.policies:train:144 - The actual batch size is 393, instead of 200\n",
      "2021-09-07 17:18:45.207 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:45.208 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2466687560081482, 'baseline_loss': 0.9361090064048767, 'total_loss': 0.22138574719429016}\n",
      "2021-09-07 17:18:45.209 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.47624585032463074\n",
      "2021-09-07 17:18:45.210 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.680683434009552\n",
      "2021-09-07 17:18:45.212 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.47624585032463074\n",
      "2021-09-07 17:18:45.213 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:45.214 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:45.215 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.11181576550006866, 'baseline_loss': 0.9289791584014893, 'total_loss': 0.35267382860183716}\n",
      "2021-09-07 17:18:45.216 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2524806261062622\n",
      "2021-09-07 17:18:45.217 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.327541470527649\n",
      "2021-09-07 17:18:45.218 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2524806261062622\n",
      "2021-09-07 17:18:45.219 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:45.220 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:45.221 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3468478322029114, 'baseline_loss': 0.8752349019050598, 'total_loss': 0.09076961874961853}\n",
      "2021-09-07 17:18:45.222 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3062160909175873\n",
      "2021-09-07 17:18:45.223 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9679238200187683\n",
      "2021-09-07 17:18:45.224 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3062160909175873\n",
      "2021-09-07 17:18:45.225 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:45.226 | INFO     | src.policies:train:123 - Epoch 633 / 800\n",
      "2021-09-07 17:18:45.227 | INFO     | src.policies:collect_trajectories:221 - Episode 1470\n",
      "2021-09-07 17:18:45.258 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:45.258 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:45.259 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:45.261 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:45.263 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24721038341522217, 'baseline_loss': 0.5588101744651794, 'total_loss': 0.032194703817367554}\n",
      "2021-09-07 17:18:45.264 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6071423888206482\n",
      "2021-09-07 17:18:45.264 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.4097514152526855\n",
      "2021-09-07 17:18:45.266 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:45.267 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:45.269 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:45.270 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.15291430056095123, 'baseline_loss': 0.5261735320091248, 'total_loss': 0.11017246544361115}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:45.271 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11504877358675003\n",
      "2021-09-07 17:18:45.272 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6986887454986572\n",
      "2021-09-07 17:18:45.273 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11504877358675003\n",
      "2021-09-07 17:18:45.274 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:45.275 | INFO     | src.policies:train:123 - Epoch 634 / 800\n",
      "2021-09-07 17:18:45.275 | INFO     | src.policies:collect_trajectories:221 - Episode 1471\n",
      "2021-09-07 17:18:45.309 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:45.309 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:45.310 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:45.312 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:45.314 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43272697925567627, 'baseline_loss': 1.2527623176574707, 'total_loss': 0.19365417957305908}\n",
      "2021-09-07 17:18:45.315 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3800738453865051\n",
      "2021-09-07 17:18:45.316 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.361520767211914\n",
      "2021-09-07 17:18:45.317 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3800738453865051\n",
      "2021-09-07 17:18:45.318 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:45.319 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:45.320 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42514321208000183, 'baseline_loss': 1.377611517906189, 'total_loss': 0.26366254687309265}\n",
      "2021-09-07 17:18:45.321 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2528172433376312\n",
      "2021-09-07 17:18:45.322 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2246148586273193\n",
      "2021-09-07 17:18:45.323 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2528172433376312\n",
      "2021-09-07 17:18:45.324 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:45.325 | INFO     | src.policies:train:123 - Epoch 635 / 800\n",
      "2021-09-07 17:18:45.326 | INFO     | src.policies:collect_trajectories:221 - Episode 1472\n",
      "2021-09-07 17:18:45.357 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:45.357 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:45.358 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:45.360 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:45.363 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.25372567772865295, 'baseline_loss': 1.238552212715149, 'total_loss': 0.3655504286289215}\n",
      "2021-09-07 17:18:45.363 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3285937011241913\n",
      "2021-09-07 17:18:45.364 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.417792558670044\n",
      "2021-09-07 17:18:45.365 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3285937011241913\n",
      "2021-09-07 17:18:45.366 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:45.367 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:45.369 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3343406617641449, 'baseline_loss': 1.2733697891235352, 'total_loss': 0.3023442327976227}\n",
      "2021-09-07 17:18:45.369 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.171151801943779\n",
      "2021-09-07 17:18:45.370 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.770025372505188\n",
      "2021-09-07 17:18:45.371 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.171151801943779\n",
      "2021-09-07 17:18:45.372 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:45.373 | INFO     | src.policies:train:123 - Epoch 636 / 800\n",
      "2021-09-07 17:18:45.374 | INFO     | src.policies:collect_trajectories:221 - Episode 1473\n",
      "2021-09-07 17:18:45.653 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:45.654 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:45.654 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:45.656 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:45.659 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23061875998973846, 'baseline_loss': 0.9783987402915955, 'total_loss': 0.25858062505722046}\n",
      "2021-09-07 17:18:45.660 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.39338576793670654\n",
      "2021-09-07 17:18:45.661 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1835812330245972\n",
      "2021-09-07 17:18:45.662 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.39338576793670654\n",
      "2021-09-07 17:18:45.663 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:45.665 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:45.666 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1775396466255188, 'baseline_loss': 1.2896472215652466, 'total_loss': 0.4672839641571045}\n",
      "2021-09-07 17:18:45.666 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1956431120634079\n",
      "2021-09-07 17:18:45.667 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4197319746017456\n",
      "2021-09-07 17:18:45.668 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1956431120634079\n",
      "2021-09-07 17:18:45.669 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:45.671 | INFO     | src.policies:train:123 - Epoch 637 / 800\n",
      "2021-09-07 17:18:45.671 | INFO     | src.policies:collect_trajectories:221 - Episode 1474\n",
      "2021-09-07 17:18:45.698 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:45.699 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:45.699 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:45.702 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:45.704 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6417022943496704, 'baseline_loss': 1.9261287450790405, 'total_loss': 0.32136207818984985}\n",
      "2021-09-07 17:18:45.705 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5831899046897888\n",
      "2021-09-07 17:18:45.706 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.966404438018799\n",
      "2021-09-07 17:18:45.707 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:45.708 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:45.709 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:45.711 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6262121796607971, 'baseline_loss': 2.11067271232605, 'total_loss': 0.4291241765022278}\n",
      "2021-09-07 17:18:45.712 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18052417039871216\n",
      "2021-09-07 17:18:45.713 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.429443836212158\n",
      "2021-09-07 17:18:45.714 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18052417039871216\n",
      "2021-09-07 17:18:45.715 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999999701976776\n",
      "2021-09-07 17:18:45.716 | INFO     | src.policies:train:123 - Epoch 638 / 800\n",
      "2021-09-07 17:18:45.717 | INFO     | src.policies:collect_trajectories:221 - Episode 1475\n",
      "2021-09-07 17:18:45.742 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:45.743 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 181.0\n",
      "2021-09-07 17:18:45.743 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 181.0\n",
      "2021-09-07 17:18:45.744 | INFO     | src.policies:collect_trajectories:221 - Episode 1476\n",
      "2021-09-07 17:18:45.775 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:45.776 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:45.776 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 190.5\n",
      "2021-09-07 17:18:45.777 | WARNING  | src.policies:train:144 - The actual batch size is 381, instead of 200\n",
      "2021-09-07 17:18:45.781 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:45.783 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27319714426994324, 'baseline_loss': 0.5628824830055237, 'total_loss': 0.008244097232818604}\n",
      "2021-09-07 17:18:45.784 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3381933569908142\n",
      "2021-09-07 17:18:45.785 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.536880373954773\n",
      "2021-09-07 17:18:45.786 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3381933569908142\n",
      "2021-09-07 17:18:45.787 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:45.789 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:45.790 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3274604082107544, 'baseline_loss': 0.8149698376655579, 'total_loss': 0.08002451062202454}\n",
      "2021-09-07 17:18:45.791 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18997809290885925\n",
      "2021-09-07 17:18:45.791 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4683129787445068\n",
      "2021-09-07 17:18:45.793 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18997809290885925\n",
      "2021-09-07 17:18:45.794 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:45.795 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:45.796 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.33612295985221863, 'baseline_loss': 0.7501842975616455, 'total_loss': 0.038969188928604126}\n",
      "2021-09-07 17:18:45.797 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1556960493326187\n",
      "2021-09-07 17:18:45.798 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.195748209953308\n",
      "2021-09-07 17:18:45.799 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1556960493326187\n",
      "2021-09-07 17:18:45.801 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:45.802 | INFO     | src.policies:train:123 - Epoch 639 / 800\n",
      "2021-09-07 17:18:45.803 | INFO     | src.policies:collect_trajectories:221 - Episode 1477\n",
      "2021-09-07 17:18:45.831 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:45.832 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:45.832 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:45.833 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:45.837 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45900848507881165, 'baseline_loss': 1.5663197040557861, 'total_loss': 0.3241513669490814}\n",
      "2021-09-07 17:18:45.839 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.641697347164154\n",
      "2021-09-07 17:18:45.839 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2625010013580322\n",
      "2021-09-07 17:18:45.841 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:45.842 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:45.844 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:45.845 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42791834473609924, 'baseline_loss': 1.596380591392517, 'total_loss': 0.3702719509601593}\n",
      "2021-09-07 17:18:45.846 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17485222220420837\n",
      "2021-09-07 17:18:45.847 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.511685848236084\n",
      "2021-09-07 17:18:45.848 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17485222220420837\n",
      "2021-09-07 17:18:45.849 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:45.851 | INFO     | src.policies:train:123 - Epoch 640 / 800\n",
      "2021-09-07 17:18:45.851 | INFO     | src.policies:collect_trajectories:221 - Episode 1478\n",
      "2021-09-07 17:18:45.882 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:45.882 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:45.883 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:45.885 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:45.887 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3666788339614868, 'baseline_loss': 0.9781447052955627, 'total_loss': 0.12239351868629456}\n",
      "2021-09-07 17:18:45.888 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.33791130781173706\n",
      "2021-09-07 17:18:45.889 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7918500304222107\n",
      "2021-09-07 17:18:45.890 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.33791130781173706\n",
      "2021-09-07 17:18:45.891 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:45.892 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:45.893 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29957056045532227, 'baseline_loss': 1.0849448442459106, 'total_loss': 0.24290186166763306}\n",
      "2021-09-07 17:18:45.894 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.36428365111351013\n",
      "2021-09-07 17:18:45.895 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7270599007606506\n",
      "2021-09-07 17:18:45.896 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.36428365111351013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:45.897 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:45.898 | INFO     | src.policies:train:123 - Epoch 641 / 800\n",
      "2021-09-07 17:18:45.899 | INFO     | src.policies:collect_trajectories:221 - Episode 1479\n",
      "2021-09-07 17:18:45.928 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:45.929 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:45.929 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:45.931 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:45.933 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.275915265083313, 'baseline_loss': 0.7543948292732239, 'total_loss': 0.10128214955329895}\n",
      "2021-09-07 17:18:45.934 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25646522641181946\n",
      "2021-09-07 17:18:45.935 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6591421365737915\n",
      "2021-09-07 17:18:45.936 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25646522641181946\n",
      "2021-09-07 17:18:45.937 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:45.939 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:45.940 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2758122980594635, 'baseline_loss': 0.7016116976737976, 'total_loss': 0.0749935507774353}\n",
      "2021-09-07 17:18:45.941 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.48176395893096924\n",
      "2021-09-07 17:18:45.942 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1677826642990112\n",
      "2021-09-07 17:18:45.944 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.48176395893096924\n",
      "2021-09-07 17:18:45.945 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:45.946 | INFO     | src.policies:train:123 - Epoch 642 / 800\n",
      "2021-09-07 17:18:45.947 | INFO     | src.policies:collect_trajectories:221 - Episode 1480\n",
      "2021-09-07 17:18:45.977 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:45.977 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:45.978 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:45.980 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:45.982 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3298545181751251, 'baseline_loss': 1.9115562438964844, 'total_loss': 0.6259236335754395}\n",
      "2021-09-07 17:18:45.983 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2774656414985657\n",
      "2021-09-07 17:18:45.985 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.346189022064209\n",
      "2021-09-07 17:18:45.986 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2774656414985657\n",
      "2021-09-07 17:18:45.987 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:45.988 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:45.989 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42745575308799744, 'baseline_loss': 2.1503121852874756, 'total_loss': 0.647700309753418}\n",
      "2021-09-07 17:18:45.990 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4894510805606842\n",
      "2021-09-07 17:18:45.991 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.309539556503296\n",
      "2021-09-07 17:18:45.992 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4894510805606842\n",
      "2021-09-07 17:18:45.993 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:45.995 | INFO     | src.policies:train:123 - Epoch 643 / 800\n",
      "2021-09-07 17:18:45.995 | INFO     | src.policies:collect_trajectories:221 - Episode 1481\n",
      "2021-09-07 17:18:46.024 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:46.024 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:46.025 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:46.027 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:46.029 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.196391761302948, 'baseline_loss': 1.3250558376312256, 'total_loss': 0.4661361575126648}\n",
      "2021-09-07 17:18:46.030 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.37905827164649963\n",
      "2021-09-07 17:18:46.031 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.573129415512085\n",
      "2021-09-07 17:18:46.032 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.37905827164649963\n",
      "2021-09-07 17:18:46.033 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:46.034 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:46.035 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4227980077266693, 'baseline_loss': 1.4143667221069336, 'total_loss': 0.2843853533267975}\n",
      "2021-09-07 17:18:46.036 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.37630170583724976\n",
      "2021-09-07 17:18:46.037 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9903231859207153\n",
      "2021-09-07 17:18:46.039 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.37630170583724976\n",
      "2021-09-07 17:18:46.040 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:46.042 | INFO     | src.policies:train:123 - Epoch 644 / 800\n",
      "2021-09-07 17:18:46.042 | INFO     | src.policies:collect_trajectories:221 - Episode 1482\n",
      "2021-09-07 17:18:46.062 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:46.062 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 124.0\n",
      "2021-09-07 17:18:46.063 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 124.0\n",
      "2021-09-07 17:18:46.063 | INFO     | src.policies:collect_trajectories:221 - Episode 1483\n",
      "2021-09-07 17:18:46.093 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:46.094 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:46.094 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 162.0\n",
      "2021-09-07 17:18:46.095 | WARNING  | src.policies:train:144 - The actual batch size is 324, instead of 200\n",
      "2021-09-07 17:18:46.098 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:46.100 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.49509257078170776, 'baseline_loss': 1.381081223487854, 'total_loss': 0.19544804096221924}\n",
      "2021-09-07 17:18:46.101 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5433361530303955\n",
      "2021-09-07 17:18:46.101 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5342565774917603\n",
      "2021-09-07 17:18:46.102 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:46.104 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:46.105 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:46.106 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.46314480900764465, 'baseline_loss': 1.5077975988388062, 'total_loss': 0.2907539904117584}\n",
      "2021-09-07 17:18:46.107 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.33227112889289856\n",
      "2021-09-07 17:18:46.107 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.393627166748047\n",
      "2021-09-07 17:18:46.108 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.33227112889289856\n",
      "2021-09-07 17:18:46.109 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:46.111 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:46.112 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4626815915107727, 'baseline_loss': 1.496070384979248, 'total_loss': 0.2853536009788513}\n",
      "2021-09-07 17:18:46.112 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1925363540649414\n",
      "2021-09-07 17:18:46.113 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.1802849769592285\n",
      "2021-09-07 17:18:46.114 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1925363540649414\n",
      "2021-09-07 17:18:46.115 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:46.116 | INFO     | src.policies:train:123 - Epoch 645 / 800\n",
      "2021-09-07 17:18:46.117 | INFO     | src.policies:collect_trajectories:221 - Episode 1484\n",
      "2021-09-07 17:18:46.202 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:46.203 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:46.203 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:46.206 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:46.208 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3210684657096863, 'baseline_loss': 0.934924840927124, 'total_loss': 0.14639395475387573}\n",
      "2021-09-07 17:18:46.209 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.42982450127601624\n",
      "2021-09-07 17:18:46.210 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6108826398849487\n",
      "2021-09-07 17:18:46.211 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.42982450127601624\n",
      "2021-09-07 17:18:46.213 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:46.214 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:46.215 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.326833575963974, 'baseline_loss': 0.8311225175857544, 'total_loss': 0.0887276828289032}\n",
      "2021-09-07 17:18:46.216 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31998223066329956\n",
      "2021-09-07 17:18:46.217 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6191768646240234\n",
      "2021-09-07 17:18:46.218 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31998223066329956\n",
      "2021-09-07 17:18:46.219 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:46.221 | INFO     | src.policies:train:123 - Epoch 646 / 800\n",
      "2021-09-07 17:18:46.221 | INFO     | src.policies:collect_trajectories:221 - Episode 1485\n",
      "2021-09-07 17:18:46.250 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:46.251 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:46.251 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:46.253 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:46.256 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2884202003479004, 'baseline_loss': 0.9191946387290955, 'total_loss': 0.17117711901664734}\n",
      "2021-09-07 17:18:46.257 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15725846588611603\n",
      "2021-09-07 17:18:46.258 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7649693489074707\n",
      "2021-09-07 17:18:46.260 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15725846588611603\n",
      "2021-09-07 17:18:46.261 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:46.262 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:46.263 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32511401176452637, 'baseline_loss': 0.9442594647407532, 'total_loss': 0.14701572060585022}\n",
      "2021-09-07 17:18:46.264 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23031070828437805\n",
      "2021-09-07 17:18:46.265 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5252353549003601\n",
      "2021-09-07 17:18:46.266 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23031070828437805\n",
      "2021-09-07 17:18:46.267 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999989867210388\n",
      "2021-09-07 17:18:46.268 | INFO     | src.policies:train:123 - Epoch 647 / 800\n",
      "2021-09-07 17:18:46.269 | INFO     | src.policies:collect_trajectories:221 - Episode 1486\n",
      "2021-09-07 17:18:46.297 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:46.298 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:46.299 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:46.303 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:46.305 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.34365734457969666, 'baseline_loss': 0.8847503662109375, 'total_loss': 0.0987178385257721}\n",
      "2021-09-07 17:18:46.306 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2545258104801178\n",
      "2021-09-07 17:18:46.307 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5512428283691406\n",
      "2021-09-07 17:18:46.308 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2545258104801178\n",
      "2021-09-07 17:18:46.309 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:46.311 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:46.312 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3587063252925873, 'baseline_loss': 0.9504427313804626, 'total_loss': 0.11651504039764404}\n",
      "2021-09-07 17:18:46.313 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15033289790153503\n",
      "2021-09-07 17:18:46.314 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.478703498840332\n",
      "2021-09-07 17:18:46.315 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15033289790153503\n",
      "2021-09-07 17:18:46.316 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:46.317 | INFO     | src.policies:train:123 - Epoch 648 / 800\n",
      "2021-09-07 17:18:46.318 | INFO     | src.policies:collect_trajectories:221 - Episode 1487\n",
      "2021-09-07 17:18:46.347 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:46.347 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:46.348 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:46.349 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:46.352 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24239151179790497, 'baseline_loss': 0.4858238995075226, 'total_loss': 0.0005204379558563232}\n",
      "2021-09-07 17:18:46.353 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13303139805793762\n",
      "2021-09-07 17:18:46.354 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9879471063613892\n",
      "2021-09-07 17:18:46.355 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13303139805793762\n",
      "2021-09-07 17:18:46.356 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:46.357 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:46.359 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3637673258781433, 'baseline_loss': 0.5426762104034424, 'total_loss': -0.09242922067642212}\n",
      "2021-09-07 17:18:46.359 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20079101622104645\n",
      "2021-09-07 17:18:46.360 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.0000009536743164\n",
      "2021-09-07 17:18:46.361 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20079101622104645\n",
      "2021-09-07 17:18:46.362 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:46.364 | INFO     | src.policies:train:123 - Epoch 649 / 800\n",
      "2021-09-07 17:18:46.364 | INFO     | src.policies:collect_trajectories:221 - Episode 1488\n",
      "2021-09-07 17:18:46.392 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:46.392 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:46.393 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:46.395 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:46.398 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5082771182060242, 'baseline_loss': 2.020895004272461, 'total_loss': 0.5021703839302063}\n",
      "2021-09-07 17:18:46.399 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2513711154460907\n",
      "2021-09-07 17:18:46.400 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.7259597778320312\n",
      "2021-09-07 17:18:46.402 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2513711154460907\n",
      "2021-09-07 17:18:46.403 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:46.404 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:46.406 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4425808787345886, 'baseline_loss': 1.964040994644165, 'total_loss': 0.5394396185874939}\n",
      "2021-09-07 17:18:46.407 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2749907076358795\n",
      "2021-09-07 17:18:46.408 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.644367218017578\n",
      "2021-09-07 17:18:46.409 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2749907076358795\n",
      "2021-09-07 17:18:46.410 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:46.411 | INFO     | src.policies:train:123 - Epoch 650 / 800\n",
      "2021-09-07 17:18:46.412 | INFO     | src.policies:collect_trajectories:221 - Episode 1489\n",
      "2021-09-07 17:18:46.440 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:46.441 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:46.442 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:46.445 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:46.447 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31458884477615356, 'baseline_loss': 1.4284071922302246, 'total_loss': 0.39961475133895874}\n",
      "2021-09-07 17:18:46.448 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24912264943122864\n",
      "2021-09-07 17:18:46.448 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2999963760375977\n",
      "2021-09-07 17:18:46.450 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24912264943122864\n",
      "2021-09-07 17:18:46.451 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:46.452 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:46.453 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.37833985686302185, 'baseline_loss': 1.4676531553268433, 'total_loss': 0.3554867208003998}\n",
      "2021-09-07 17:18:46.454 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.29365646839141846\n",
      "2021-09-07 17:18:46.455 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5711382627487183\n",
      "2021-09-07 17:18:46.456 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.29365646839141846\n",
      "2021-09-07 17:18:46.457 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:46.458 | INFO     | src.policies:train:123 - Epoch 651 / 800\n",
      "2021-09-07 17:18:46.459 | INFO     | src.policies:collect_trajectories:221 - Episode 1490\n",
      "2021-09-07 17:18:46.488 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:46.489 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:46.490 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:46.491 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:46.494 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.19945919513702393, 'baseline_loss': 0.8432087898254395, 'total_loss': 0.2221451997756958}\n",
      "2021-09-07 17:18:46.495 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.40381237864494324\n",
      "2021-09-07 17:18:46.496 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.3588948249816895\n",
      "2021-09-07 17:18:46.497 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.40381237864494324\n",
      "2021-09-07 17:18:46.498 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:46.499 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:46.500 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1386791616678238, 'baseline_loss': 0.7101657390594482, 'total_loss': 0.21640370786190033}\n",
      "2021-09-07 17:18:46.501 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21293391287326813\n",
      "2021-09-07 17:18:46.502 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.098698377609253\n",
      "2021-09-07 17:18:46.503 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21293391287326813\n",
      "2021-09-07 17:18:46.505 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:46.506 | INFO     | src.policies:train:123 - Epoch 652 / 800\n",
      "2021-09-07 17:18:46.506 | INFO     | src.policies:collect_trajectories:221 - Episode 1491\n",
      "2021-09-07 17:18:46.536 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:46.537 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:46.537 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:46.540 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:46.543 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40786197781562805, 'baseline_loss': 1.2847477197647095, 'total_loss': 0.23451188206672668}\n",
      "2021-09-07 17:18:46.544 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1223452091217041\n",
      "2021-09-07 17:18:46.545 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8372920751571655\n",
      "2021-09-07 17:18:46.546 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1223452091217041\n",
      "2021-09-07 17:18:46.548 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:46.549 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:46.551 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.46924424171447754, 'baseline_loss': 1.305816650390625, 'total_loss': 0.18366408348083496}\n",
      "2021-09-07 17:18:46.551 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09865038096904755\n",
      "2021-09-07 17:18:46.552 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9314988851547241\n",
      "2021-09-07 17:18:46.553 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09865038096904755\n",
      "2021-09-07 17:18:46.554 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:46.556 | INFO     | src.policies:train:123 - Epoch 653 / 800\n",
      "2021-09-07 17:18:46.556 | INFO     | src.policies:collect_trajectories:221 - Episode 1492\n",
      "2021-09-07 17:18:46.585 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:46.586 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:46.586 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:46.588 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:46.590 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.11265193670988083, 'baseline_loss': 0.5062114596366882, 'total_loss': 0.1404537856578827}\n",
      "2021-09-07 17:18:46.591 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.05471104383468628\n",
      "2021-09-07 17:18:46.592 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2822937965393066\n",
      "2021-09-07 17:18:46.594 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.05471104383468628\n",
      "2021-09-07 17:18:46.596 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:46.597 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:46.599 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.12182970345020294, 'baseline_loss': 0.5031495690345764, 'total_loss': 0.12974508106708527}\n",
      "2021-09-07 17:18:46.600 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.42363911867141724\n",
      "2021-09-07 17:18:46.601 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8169068098068237\n",
      "2021-09-07 17:18:46.602 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.42363911867141724\n",
      "2021-09-07 17:18:46.604 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:46.605 | INFO     | src.policies:train:123 - Epoch 654 / 800\n",
      "2021-09-07 17:18:46.606 | INFO     | src.policies:collect_trajectories:221 - Episode 1493\n",
      "2021-09-07 17:18:46.635 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:46.636 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:46.637 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:46.639 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:46.641 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29464590549468994, 'baseline_loss': 0.8593721985816956, 'total_loss': 0.13504019379615784}\n",
      "2021-09-07 17:18:46.642 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13606013357639313\n",
      "2021-09-07 17:18:46.643 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.520565390586853\n",
      "2021-09-07 17:18:46.645 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13606013357639313\n",
      "2021-09-07 17:18:46.646 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:46.647 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:46.648 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36619436740875244, 'baseline_loss': 0.9870603680610657, 'total_loss': 0.1273358166217804}\n",
      "2021-09-07 17:18:46.649 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.26133671402931213\n",
      "2021-09-07 17:18:46.650 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5643807649612427\n",
      "2021-09-07 17:18:46.651 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.26133671402931213\n",
      "2021-09-07 17:18:46.652 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:46.653 | INFO     | src.policies:train:123 - Epoch 655 / 800\n",
      "2021-09-07 17:18:46.654 | INFO     | src.policies:collect_trajectories:221 - Episode 1494\n",
      "2021-09-07 17:18:46.682 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:46.682 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:46.683 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:46.686 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:46.688 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32303664088249207, 'baseline_loss': 0.8623121380805969, 'total_loss': 0.1081194281578064}\n",
      "2021-09-07 17:18:46.742 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5693127512931824\n",
      "2021-09-07 17:18:46.750 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.653084397315979\n",
      "2021-09-07 17:18:46.752 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:46.753 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:46.754 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:46.756 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.19893129169940948, 'baseline_loss': 0.7457259893417358, 'total_loss': 0.17393170297145844}\n",
      "2021-09-07 17:18:46.756 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3875068426132202\n",
      "2021-09-07 17:18:46.757 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.802161455154419\n",
      "2021-09-07 17:18:46.758 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3875068426132202\n",
      "2021-09-07 17:18:46.759 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:46.761 | INFO     | src.policies:train:123 - Epoch 656 / 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:46.761 | INFO     | src.policies:collect_trajectories:221 - Episode 1495\n",
      "2021-09-07 17:18:46.790 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:46.791 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:46.791 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:46.793 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:46.795 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.39176875352859497, 'baseline_loss': 0.8250106573104858, 'total_loss': 0.02073657512664795}\n",
      "2021-09-07 17:18:46.796 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.33280062675476074\n",
      "2021-09-07 17:18:46.797 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9901783466339111\n",
      "2021-09-07 17:18:46.798 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.33280062675476074\n",
      "2021-09-07 17:18:46.800 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:46.802 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:46.803 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31439343094825745, 'baseline_loss': 0.7847726941108704, 'total_loss': 0.07799291610717773}\n",
      "2021-09-07 17:18:46.804 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3134344220161438\n",
      "2021-09-07 17:18:46.804 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8849071860313416\n",
      "2021-09-07 17:18:46.806 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3134344220161438\n",
      "2021-09-07 17:18:46.807 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:46.808 | INFO     | src.policies:train:123 - Epoch 657 / 800\n",
      "2021-09-07 17:18:46.809 | INFO     | src.policies:collect_trajectories:221 - Episode 1496\n",
      "2021-09-07 17:18:46.837 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:46.838 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:46.838 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:46.840 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:46.843 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.11165548115968704, 'baseline_loss': 0.6286764740943909, 'total_loss': 0.202682763338089}\n",
      "2021-09-07 17:18:46.844 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12308170646429062\n",
      "2021-09-07 17:18:46.845 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.560036540031433\n",
      "2021-09-07 17:18:46.846 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12308170646429062\n",
      "2021-09-07 17:18:46.847 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:46.848 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:46.849 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.08776220679283142, 'baseline_loss': 0.8095548748970032, 'total_loss': 0.31701523065567017}\n",
      "2021-09-07 17:18:46.850 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.33054986596107483\n",
      "2021-09-07 17:18:46.851 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3882946968078613\n",
      "2021-09-07 17:18:46.852 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.33054986596107483\n",
      "2021-09-07 17:18:46.853 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:46.854 | INFO     | src.policies:train:123 - Epoch 658 / 800\n",
      "2021-09-07 17:18:46.854 | INFO     | src.policies:collect_trajectories:221 - Episode 1497\n",
      "2021-09-07 17:18:46.883 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:46.884 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:46.884 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:46.886 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:46.888 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23220476508140564, 'baseline_loss': 0.5929665565490723, 'total_loss': 0.0642785131931305}\n",
      "2021-09-07 17:18:46.889 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.262688010931015\n",
      "2021-09-07 17:18:46.890 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.2584023475646973\n",
      "2021-09-07 17:18:46.892 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.262688010931015\n",
      "2021-09-07 17:18:46.893 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:46.894 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:46.895 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1531941443681717, 'baseline_loss': 0.5938993096351624, 'total_loss': 0.14375551044940948}\n",
      "2021-09-07 17:18:46.896 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4111272394657135\n",
      "2021-09-07 17:18:46.897 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.4039180278778076\n",
      "2021-09-07 17:18:46.898 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4111272394657135\n",
      "2021-09-07 17:18:46.899 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:46.900 | INFO     | src.policies:train:123 - Epoch 659 / 800\n",
      "2021-09-07 17:18:46.901 | INFO     | src.policies:collect_trajectories:221 - Episode 1498\n",
      "2021-09-07 17:18:46.930 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:46.931 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:46.931 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:46.933 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:46.935 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.254400372505188, 'baseline_loss': 0.5744040012359619, 'total_loss': 0.03280162811279297}\n",
      "2021-09-07 17:18:46.936 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4301236867904663\n",
      "2021-09-07 17:18:46.937 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3295807838439941\n",
      "2021-09-07 17:18:46.939 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4301236867904663\n",
      "2021-09-07 17:18:46.939 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:46.941 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:46.943 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23937277495861053, 'baseline_loss': 0.6179025173187256, 'total_loss': 0.06957848370075226}\n",
      "2021-09-07 17:18:46.944 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07119984924793243\n",
      "2021-09-07 17:18:46.945 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4201390743255615\n",
      "2021-09-07 17:18:46.946 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07119984924793243\n",
      "2021-09-07 17:18:46.947 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:46.949 | INFO     | src.policies:train:123 - Epoch 660 / 800\n",
      "2021-09-07 17:18:46.950 | INFO     | src.policies:collect_trajectories:221 - Episode 1499\n",
      "2021-09-07 17:18:46.979 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:46.980 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:46.980 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:46.982 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:46.985 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.37029239535331726, 'baseline_loss': 2.1093592643737793, 'total_loss': 0.68438720703125}\n",
      "2021-09-07 17:18:46.985 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.535637617111206\n",
      "2021-09-07 17:18:46.986 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.6135239601135254\n",
      "2021-09-07 17:18:46.988 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:46.989 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:46.990 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:46.991 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4083569049835205, 'baseline_loss': 1.9001784324645996, 'total_loss': 0.5417323112487793}\n",
      "2021-09-07 17:18:46.992 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14885906875133514\n",
      "2021-09-07 17:18:46.993 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.1787192821502686\n",
      "2021-09-07 17:18:46.994 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14885906875133514\n",
      "2021-09-07 17:18:46.995 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:46.996 | INFO     | src.policies:train:123 - Epoch 661 / 800\n",
      "2021-09-07 17:18:46.997 | INFO     | src.policies:collect_trajectories:221 - Episode 1500\n",
      "2021-09-07 17:18:47.026 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:47.027 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:47.028 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:47.030 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:47.032 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.46745166182518005, 'baseline_loss': 1.33489990234375, 'total_loss': 0.19999828934669495}\n",
      "2021-09-07 17:18:47.033 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2543846666812897\n",
      "2021-09-07 17:18:47.034 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2921799421310425\n",
      "2021-09-07 17:18:47.035 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2543846666812897\n",
      "2021-09-07 17:18:47.036 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:47.037 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:47.038 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5464891791343689, 'baseline_loss': 1.3018808364868164, 'total_loss': 0.1044512391090393}\n",
      "2021-09-07 17:18:47.039 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.8615782856941223\n",
      "2021-09-07 17:18:47.041 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4805442094802856\n",
      "2021-09-07 17:18:47.042 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:47.043 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:47.045 | INFO     | src.policies:train:123 - Epoch 662 / 800\n",
      "2021-09-07 17:18:47.045 | INFO     | src.policies:collect_trajectories:221 - Episode 1501\n",
      "2021-09-07 17:18:47.075 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:47.075 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:47.075 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:47.077 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:47.079 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.18199023604393005, 'baseline_loss': 0.4103669822216034, 'total_loss': 0.023193255066871643}\n",
      "2021-09-07 17:18:47.080 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.34858569502830505\n",
      "2021-09-07 17:18:47.081 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2745414972305298\n",
      "2021-09-07 17:18:47.082 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.34858569502830505\n",
      "2021-09-07 17:18:47.083 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:47.085 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:47.086 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.19346453249454498, 'baseline_loss': 0.4029015302658081, 'total_loss': 0.00798623263835907}\n",
      "2021-09-07 17:18:47.086 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12531448900699615\n",
      "2021-09-07 17:18:47.087 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9929026365280151\n",
      "2021-09-07 17:18:47.088 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12531448900699615\n",
      "2021-09-07 17:18:47.089 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:47.091 | INFO     | src.policies:train:123 - Epoch 663 / 800\n",
      "2021-09-07 17:18:47.091 | INFO     | src.policies:collect_trajectories:221 - Episode 1502\n",
      "2021-09-07 17:18:47.102 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:47.102 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 63.0\n",
      "2021-09-07 17:18:47.103 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 63.0\n",
      "2021-09-07 17:18:47.103 | INFO     | src.policies:collect_trajectories:221 - Episode 1503\n",
      "2021-09-07 17:18:47.133 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:47.133 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:47.134 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 131.5\n",
      "2021-09-07 17:18:47.134 | WARNING  | src.policies:train:144 - The actual batch size is 263, instead of 200\n",
      "2021-09-07 17:18:47.137 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:47.138 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5337378978729248, 'baseline_loss': 1.0881545543670654, 'total_loss': 0.01033937931060791}\n",
      "2021-09-07 17:18:47.139 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20802977681159973\n",
      "2021-09-07 17:18:47.140 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.05009126663208\n",
      "2021-09-07 17:18:47.142 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20802977681159973\n",
      "2021-09-07 17:18:47.143 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:47.144 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:47.146 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.48473218083381653, 'baseline_loss': 1.0018305778503418, 'total_loss': 0.01618310809135437}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:47.147 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3569842576980591\n",
      "2021-09-07 17:18:47.147 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7523989677429199\n",
      "2021-09-07 17:18:47.149 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3569842576980591\n",
      "2021-09-07 17:18:47.149 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:47.151 | INFO     | src.policies:train:123 - Epoch 664 / 800\n",
      "2021-09-07 17:18:47.151 | INFO     | src.policies:collect_trajectories:221 - Episode 1504\n",
      "2021-09-07 17:18:47.180 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:47.181 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:47.181 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:47.183 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:47.185 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.19918681681156158, 'baseline_loss': 0.4004283845424652, 'total_loss': 0.0010273754596710205}\n",
      "2021-09-07 17:18:47.186 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17243634164333344\n",
      "2021-09-07 17:18:47.187 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.396092414855957\n",
      "2021-09-07 17:18:47.188 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17243634164333344\n",
      "2021-09-07 17:18:47.189 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:47.191 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:47.192 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1570626199245453, 'baseline_loss': 0.4199906587600708, 'total_loss': 0.05293270945549011}\n",
      "2021-09-07 17:18:47.193 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14739523828029633\n",
      "2021-09-07 17:18:47.194 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6023422479629517\n",
      "2021-09-07 17:18:47.195 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14739523828029633\n",
      "2021-09-07 17:18:47.196 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:47.198 | INFO     | src.policies:train:123 - Epoch 665 / 800\n",
      "2021-09-07 17:18:47.198 | INFO     | src.policies:collect_trajectories:221 - Episode 1505\n",
      "2021-09-07 17:18:47.226 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:47.227 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:47.227 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:47.229 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:47.232 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.35933077335357666, 'baseline_loss': 0.8621909618377686, 'total_loss': 0.07176470756530762}\n",
      "2021-09-07 17:18:47.232 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2586052119731903\n",
      "2021-09-07 17:18:47.233 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6208622455596924\n",
      "2021-09-07 17:18:47.234 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2586052119731903\n",
      "2021-09-07 17:18:47.235 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:47.237 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:47.238 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3078789710998535, 'baseline_loss': 0.7095699906349182, 'total_loss': 0.04690602421760559}\n",
      "2021-09-07 17:18:47.238 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20237170159816742\n",
      "2021-09-07 17:18:47.239 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0466437339782715\n",
      "2021-09-07 17:18:47.241 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20237170159816742\n",
      "2021-09-07 17:18:47.242 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:47.243 | INFO     | src.policies:train:123 - Epoch 666 / 800\n",
      "2021-09-07 17:18:47.244 | INFO     | src.policies:collect_trajectories:221 - Episode 1506\n",
      "2021-09-07 17:18:47.331 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:47.332 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:47.332 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:47.335 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:47.337 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27730873227119446, 'baseline_loss': 0.38459011912345886, 'total_loss': -0.08501367270946503}\n",
      "2021-09-07 17:18:47.338 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5648828148841858\n",
      "2021-09-07 17:18:47.339 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1817699670791626\n",
      "2021-09-07 17:18:47.340 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:47.341 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:47.343 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:47.344 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.19794178009033203, 'baseline_loss': 0.3786245584487915, 'total_loss': -0.00862950086593628}\n",
      "2021-09-07 17:18:47.345 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24792200326919556\n",
      "2021-09-07 17:18:47.346 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3172690868377686\n",
      "2021-09-07 17:18:47.347 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24792200326919556\n",
      "2021-09-07 17:18:47.348 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:47.350 | INFO     | src.policies:train:123 - Epoch 667 / 800\n",
      "2021-09-07 17:18:47.350 | INFO     | src.policies:collect_trajectories:221 - Episode 1507\n",
      "2021-09-07 17:18:47.365 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:47.366 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 104.0\n",
      "2021-09-07 17:18:47.366 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 104.0\n",
      "2021-09-07 17:18:47.367 | INFO     | src.policies:collect_trajectories:221 - Episode 1508\n",
      "2021-09-07 17:18:47.398 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:47.399 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:47.399 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 152.0\n",
      "2021-09-07 17:18:47.400 | WARNING  | src.policies:train:144 - The actual batch size is 304, instead of 200\n",
      "2021-09-07 17:18:47.402 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:47.405 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.21311689913272858, 'baseline_loss': 0.47457870841026306, 'total_loss': 0.024172455072402954}\n",
      "2021-09-07 17:18:47.406 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22903744876384735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:47.407 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9502004384994507\n",
      "2021-09-07 17:18:47.408 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22903744876384735\n",
      "2021-09-07 17:18:47.409 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:47.410 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:47.411 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.30934417247772217, 'baseline_loss': 0.4393107295036316, 'total_loss': -0.08968880772590637}\n",
      "2021-09-07 17:18:47.412 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3877227306365967\n",
      "2021-09-07 17:18:47.413 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8568661212921143\n",
      "2021-09-07 17:18:47.414 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3877227306365967\n",
      "2021-09-07 17:18:47.415 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:47.416 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:47.418 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.25487279891967773, 'baseline_loss': 0.44383084774017334, 'total_loss': -0.032957375049591064}\n",
      "2021-09-07 17:18:47.418 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11292512714862823\n",
      "2021-09-07 17:18:47.419 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2032444477081299\n",
      "2021-09-07 17:18:47.420 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11292512714862823\n",
      "2021-09-07 17:18:47.421 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:47.423 | INFO     | src.policies:train:123 - Epoch 668 / 800\n",
      "2021-09-07 17:18:47.423 | INFO     | src.policies:collect_trajectories:221 - Episode 1509\n",
      "2021-09-07 17:18:47.454 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:47.455 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:47.455 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:47.458 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:47.460 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.516758143901825, 'baseline_loss': 1.0239946842193604, 'total_loss': -0.004760801792144775}\n",
      "2021-09-07 17:18:47.461 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3811914920806885\n",
      "2021-09-07 17:18:47.462 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.628425657749176\n",
      "2021-09-07 17:18:47.463 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3811914920806885\n",
      "2021-09-07 17:18:47.464 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:47.466 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:47.467 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43176940083503723, 'baseline_loss': 0.9545007348060608, 'total_loss': 0.045480966567993164}\n",
      "2021-09-07 17:18:47.468 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10283273458480835\n",
      "2021-09-07 17:18:47.468 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5671460628509521\n",
      "2021-09-07 17:18:47.469 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10283273458480835\n",
      "2021-09-07 17:18:47.470 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:47.472 | INFO     | src.policies:train:123 - Epoch 669 / 800\n",
      "2021-09-07 17:18:47.472 | INFO     | src.policies:collect_trajectories:221 - Episode 1510\n",
      "2021-09-07 17:18:47.500 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:47.501 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:47.501 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:47.503 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:47.506 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24408988654613495, 'baseline_loss': 0.4182324707508087, 'total_loss': -0.03497365117073059}\n",
      "2021-09-07 17:18:47.506 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16348618268966675\n",
      "2021-09-07 17:18:47.507 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.088274598121643\n",
      "2021-09-07 17:18:47.509 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16348618268966675\n",
      "2021-09-07 17:18:47.510 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:47.511 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:47.512 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.25005772709846497, 'baseline_loss': 0.3782520294189453, 'total_loss': -0.06093171238899231}\n",
      "2021-09-07 17:18:47.513 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1729629784822464\n",
      "2021-09-07 17:18:47.513 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0244791507720947\n",
      "2021-09-07 17:18:47.514 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1729629784822464\n",
      "2021-09-07 17:18:47.516 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:47.517 | INFO     | src.policies:train:123 - Epoch 670 / 800\n",
      "2021-09-07 17:18:47.517 | INFO     | src.policies:collect_trajectories:221 - Episode 1511\n",
      "2021-09-07 17:18:47.556 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:47.557 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:47.558 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:47.559 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:47.561 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36243197321891785, 'baseline_loss': 0.6132079362869263, 'total_loss': -0.05582800507545471}\n",
      "2021-09-07 17:18:47.562 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4721708595752716\n",
      "2021-09-07 17:18:47.563 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.21763551235198975\n",
      "2021-09-07 17:18:47.565 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4721708595752716\n",
      "2021-09-07 17:18:47.566 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.21763551235198975\n",
      "2021-09-07 17:18:47.567 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:47.568 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.33075618743896484, 'baseline_loss': 0.4221749007701874, 'total_loss': -0.11966873705387115}\n",
      "2021-09-07 17:18:47.569 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.28490814566612244\n",
      "2021-09-07 17:18:47.570 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5691989064216614\n",
      "2021-09-07 17:18:47.570 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.28490814566612244\n",
      "2021-09-07 17:18:47.571 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:47.573 | INFO     | src.policies:train:123 - Epoch 671 / 800\n",
      "2021-09-07 17:18:47.573 | INFO     | src.policies:collect_trajectories:221 - Episode 1512\n",
      "2021-09-07 17:18:47.602 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:47.602 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:47.603 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:47.605 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:47.607 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4585162103176117, 'baseline_loss': 1.008881688117981, 'total_loss': 0.045924633741378784}\n",
      "2021-09-07 17:18:47.608 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3128708004951477\n",
      "2021-09-07 17:18:47.609 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.985119104385376\n",
      "2021-09-07 17:18:47.611 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3128708004951477\n",
      "2021-09-07 17:18:47.612 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:47.613 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:47.614 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5206769108772278, 'baseline_loss': 0.9354086518287659, 'total_loss': -0.05297258496284485}\n",
      "2021-09-07 17:18:47.615 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22141414880752563\n",
      "2021-09-07 17:18:47.616 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4354020357131958\n",
      "2021-09-07 17:18:47.617 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22141414880752563\n",
      "2021-09-07 17:18:47.618 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:47.619 | INFO     | src.policies:train:123 - Epoch 672 / 800\n",
      "2021-09-07 17:18:47.620 | INFO     | src.policies:collect_trajectories:221 - Episode 1513\n",
      "2021-09-07 17:18:47.752 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:47.753 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:47.753 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:47.756 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:47.758 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3664843440055847, 'baseline_loss': 0.5111629366874695, 'total_loss': -0.11090287566184998}\n",
      "2021-09-07 17:18:47.759 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5030162930488586\n",
      "2021-09-07 17:18:47.760 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9330848455429077\n",
      "2021-09-07 17:18:47.761 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999895691871643\n",
      "2021-09-07 17:18:47.762 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:47.763 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:47.764 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3364236056804657, 'baseline_loss': 0.5400827527046204, 'total_loss': -0.06638222932815552}\n",
      "2021-09-07 17:18:47.765 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10296778380870819\n",
      "2021-09-07 17:18:47.766 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5496927499771118\n",
      "2021-09-07 17:18:47.767 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10296778380870819\n",
      "2021-09-07 17:18:47.768 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:47.769 | INFO     | src.policies:train:123 - Epoch 673 / 800\n",
      "2021-09-07 17:18:47.770 | INFO     | src.policies:collect_trajectories:221 - Episode 1514\n",
      "2021-09-07 17:18:47.798 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:47.799 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:47.799 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:47.802 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:47.805 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6045511960983276, 'baseline_loss': 1.3535860776901245, 'total_loss': 0.07224184274673462}\n",
      "2021-09-07 17:18:47.806 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1871914267539978\n",
      "2021-09-07 17:18:47.807 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.1111292839050293\n",
      "2021-09-07 17:18:47.808 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1871914267539978\n",
      "2021-09-07 17:18:47.809 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:47.810 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:47.811 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6570184230804443, 'baseline_loss': 1.4359817504882812, 'total_loss': 0.06097245216369629}\n",
      "2021-09-07 17:18:47.842 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13807164132595062\n",
      "2021-09-07 17:18:47.929 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.195636749267578\n",
      "2021-09-07 17:18:47.930 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13807164132595062\n",
      "2021-09-07 17:18:47.932 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:47.934 | INFO     | src.policies:train:123 - Epoch 674 / 800\n",
      "2021-09-07 17:18:47.934 | INFO     | src.policies:collect_trajectories:221 - Episode 1515\n",
      "2021-09-07 17:18:47.972 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:47.972 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:47.973 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:47.975 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:47.977 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6812553405761719, 'baseline_loss': 2.302873134613037, 'total_loss': 0.4701812267303467}\n",
      "2021-09-07 17:18:47.978 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6652230024337769\n",
      "2021-09-07 17:18:47.979 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.9725687503814697\n",
      "2021-09-07 17:18:47.980 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:47.981 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:47.982 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:47.983 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8250569701194763, 'baseline_loss': 2.661935806274414, 'total_loss': 0.5059109330177307}\n",
      "2021-09-07 17:18:47.984 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.9719181656837463\n",
      "2021-09-07 17:18:47.985 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.156660079956055\n",
      "2021-09-07 17:18:47.986 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999995231628418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:47.987 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:47.989 | INFO     | src.policies:train:123 - Epoch 675 / 800\n",
      "2021-09-07 17:18:47.989 | INFO     | src.policies:collect_trajectories:221 - Episode 1516\n",
      "2021-09-07 17:18:48.020 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.021 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:48.022 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:48.024 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:48.026 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4650082290172577, 'baseline_loss': 0.8088168501853943, 'total_loss': -0.06059980392456055}\n",
      "2021-09-07 17:18:48.027 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1545438915491104\n",
      "2021-09-07 17:18:48.028 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1190826892852783\n",
      "2021-09-07 17:18:48.029 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1545438915491104\n",
      "2021-09-07 17:18:48.030 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:48.031 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:48.032 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4891042709350586, 'baseline_loss': 0.8437284827232361, 'total_loss': -0.06724002957344055}\n",
      "2021-09-07 17:18:48.033 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4278121292591095\n",
      "2021-09-07 17:18:48.034 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.856671154499054\n",
      "2021-09-07 17:18:48.035 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4278121292591095\n",
      "2021-09-07 17:18:48.036 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:48.037 | INFO     | src.policies:train:123 - Epoch 676 / 800\n",
      "2021-09-07 17:18:48.038 | INFO     | src.policies:collect_trajectories:221 - Episode 1517\n",
      "2021-09-07 17:18:48.066 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.067 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:48.067 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:48.069 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:48.071 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.37462589144706726, 'baseline_loss': 1.1517807245254517, 'total_loss': 0.20126447081565857}\n",
      "2021-09-07 17:18:48.072 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2534754276275635\n",
      "2021-09-07 17:18:48.074 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9642884731292725\n",
      "2021-09-07 17:18:48.075 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2534754276275635\n",
      "2021-09-07 17:18:48.076 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:48.077 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:48.078 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3620896637439728, 'baseline_loss': 1.0360276699066162, 'total_loss': 0.15592417120933533}\n",
      "2021-09-07 17:18:48.079 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.29793837666511536\n",
      "2021-09-07 17:18:48.079 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.597558617591858\n",
      "2021-09-07 17:18:48.080 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.29793837666511536\n",
      "2021-09-07 17:18:48.081 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:48.083 | INFO     | src.policies:train:123 - Epoch 677 / 800\n",
      "2021-09-07 17:18:48.083 | INFO     | src.policies:collect_trajectories:221 - Episode 1518\n",
      "2021-09-07 17:18:48.112 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.112 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:48.113 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:48.115 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:48.117 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7148140668869019, 'baseline_loss': 2.672544240951538, 'total_loss': 0.6214580535888672}\n",
      "2021-09-07 17:18:48.118 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.41391879320144653\n",
      "2021-09-07 17:18:48.119 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 5.299976825714111\n",
      "2021-09-07 17:18:48.120 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.41391879320144653\n",
      "2021-09-07 17:18:48.121 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:48.122 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:48.123 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.935130774974823, 'baseline_loss': 2.816248893737793, 'total_loss': 0.4729936718940735}\n",
      "2021-09-07 17:18:48.124 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.8418608903884888\n",
      "2021-09-07 17:18:48.125 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.926342487335205\n",
      "2021-09-07 17:18:48.126 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:48.127 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:48.129 | INFO     | src.policies:train:123 - Epoch 678 / 800\n",
      "2021-09-07 17:18:48.129 | INFO     | src.policies:collect_trajectories:221 - Episode 1519\n",
      "2021-09-07 17:18:48.157 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.157 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:48.158 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:48.160 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:48.163 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.447401225566864, 'baseline_loss': 0.7735862135887146, 'total_loss': -0.060608118772506714}\n",
      "2021-09-07 17:18:48.164 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.42147427797317505\n",
      "2021-09-07 17:18:48.165 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.682598888874054\n",
      "2021-09-07 17:18:48.166 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.42147427797317505\n",
      "2021-09-07 17:18:48.167 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:48.168 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:48.170 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32035118341445923, 'baseline_loss': 0.6785953044891357, 'total_loss': 0.018946468830108643}\n",
      "2021-09-07 17:18:48.170 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09769832342863083\n",
      "2021-09-07 17:18:48.171 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8691167235374451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:48.172 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09769832342863083\n",
      "2021-09-07 17:18:48.173 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:48.174 | INFO     | src.policies:train:123 - Epoch 679 / 800\n",
      "2021-09-07 17:18:48.175 | INFO     | src.policies:collect_trajectories:221 - Episode 1520\n",
      "2021-09-07 17:18:48.204 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.205 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:48.205 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:48.208 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:48.210 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6386047601699829, 'baseline_loss': 2.1643052101135254, 'total_loss': 0.4435478448867798}\n",
      "2021-09-07 17:18:48.211 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3774305284023285\n",
      "2021-09-07 17:18:48.212 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.904991388320923\n",
      "2021-09-07 17:18:48.213 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3774305284023285\n",
      "2021-09-07 17:18:48.214 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:48.215 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:48.216 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8286481499671936, 'baseline_loss': 2.7029306888580322, 'total_loss': 0.5228171944618225}\n",
      "2021-09-07 17:18:48.217 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.885124921798706\n",
      "2021-09-07 17:18:48.218 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.574107646942139\n",
      "2021-09-07 17:18:48.219 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:48.220 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:48.221 | INFO     | src.policies:train:123 - Epoch 680 / 800\n",
      "2021-09-07 17:18:48.222 | INFO     | src.policies:collect_trajectories:221 - Episode 1521\n",
      "2021-09-07 17:18:48.251 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.251 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:48.252 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:48.253 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:48.256 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29809731245040894, 'baseline_loss': 0.6172998547554016, 'total_loss': 0.01055261492729187}\n",
      "2021-09-07 17:18:48.257 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.49651166796684265\n",
      "2021-09-07 17:18:48.257 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5160037279129028\n",
      "2021-09-07 17:18:48.258 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49651166796684265\n",
      "2021-09-07 17:18:48.259 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:48.260 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:48.262 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.26619383692741394, 'baseline_loss': 0.6592593193054199, 'total_loss': 0.06343582272529602}\n",
      "2021-09-07 17:18:48.262 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08948018401861191\n",
      "2021-09-07 17:18:48.263 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6095011234283447\n",
      "2021-09-07 17:18:48.264 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08948018401861191\n",
      "2021-09-07 17:18:48.265 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:48.267 | INFO     | src.policies:train:123 - Epoch 681 / 800\n",
      "2021-09-07 17:18:48.267 | INFO     | src.policies:collect_trajectories:221 - Episode 1522\n",
      "2021-09-07 17:18:48.296 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.297 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:48.297 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:48.301 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:48.303 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5969091653823853, 'baseline_loss': 1.7782937288284302, 'total_loss': 0.29223769903182983}\n",
      "2021-09-07 17:18:48.304 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.41153421998023987\n",
      "2021-09-07 17:18:48.305 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.626317262649536\n",
      "2021-09-07 17:18:48.307 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.41153421998023987\n",
      "2021-09-07 17:18:48.308 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:48.309 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:48.310 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.569540798664093, 'baseline_loss': 1.9204645156860352, 'total_loss': 0.39069145917892456}\n",
      "2021-09-07 17:18:48.311 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32956093549728394\n",
      "2021-09-07 17:18:48.312 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.779279947280884\n",
      "2021-09-07 17:18:48.313 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32956093549728394\n",
      "2021-09-07 17:18:48.314 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:48.316 | INFO     | src.policies:train:123 - Epoch 682 / 800\n",
      "2021-09-07 17:18:48.316 | INFO     | src.policies:collect_trajectories:221 - Episode 1523\n",
      "2021-09-07 17:18:48.345 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.346 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:48.346 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:48.349 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:48.351 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8266016244888306, 'baseline_loss': 2.8642919063568115, 'total_loss': 0.6055443286895752}\n",
      "2021-09-07 17:18:48.352 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22077935934066772\n",
      "2021-09-07 17:18:48.353 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 6.191699981689453\n",
      "2021-09-07 17:18:48.355 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22077935934066772\n",
      "2021-09-07 17:18:48.356 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:48.357 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:48.358 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.8316811323165894, 'baseline_loss': 2.751666307449341, 'total_loss': 0.544152021408081}\n",
      "2021-09-07 17:18:48.359 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.756926953792572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:48.360 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 5.128989219665527\n",
      "2021-09-07 17:18:48.361 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:48.362 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:48.363 | INFO     | src.policies:train:123 - Epoch 683 / 800\n",
      "2021-09-07 17:18:48.364 | INFO     | src.policies:collect_trajectories:221 - Episode 1524\n",
      "2021-09-07 17:18:48.392 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.392 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:48.393 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:48.395 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:48.397 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3677183985710144, 'baseline_loss': 0.656464159488678, 'total_loss': -0.039486318826675415}\n",
      "2021-09-07 17:18:48.398 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.280294805765152\n",
      "2021-09-07 17:18:48.399 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8368023633956909\n",
      "2021-09-07 17:18:48.401 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.280294805765152\n",
      "2021-09-07 17:18:48.402 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:48.403 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:48.404 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.44957461953163147, 'baseline_loss': 0.6860199570655823, 'total_loss': -0.10656464099884033}\n",
      "2021-09-07 17:18:48.405 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21434636414051056\n",
      "2021-09-07 17:18:48.406 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1071434020996094\n",
      "2021-09-07 17:18:48.407 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21434636414051056\n",
      "2021-09-07 17:18:48.408 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:48.409 | INFO     | src.policies:train:123 - Epoch 684 / 800\n",
      "2021-09-07 17:18:48.410 | INFO     | src.policies:collect_trajectories:221 - Episode 1525\n",
      "2021-09-07 17:18:48.494 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.494 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:48.495 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:48.497 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:48.499 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3371894359588623, 'baseline_loss': 0.44519442319869995, 'total_loss': -0.11459222435951233}\n",
      "2021-09-07 17:18:48.500 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4099729359149933\n",
      "2021-09-07 17:18:48.502 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.0646886825561523\n",
      "2021-09-07 17:18:48.503 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4099729359149933\n",
      "2021-09-07 17:18:48.505 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:48.506 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:48.507 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2615421414375305, 'baseline_loss': 0.4120415449142456, 'total_loss': -0.055521368980407715}\n",
      "2021-09-07 17:18:48.508 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3490068018436432\n",
      "2021-09-07 17:18:48.509 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1044683456420898\n",
      "2021-09-07 17:18:48.510 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3490068018436432\n",
      "2021-09-07 17:18:48.511 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:48.513 | INFO     | src.policies:train:123 - Epoch 685 / 800\n",
      "2021-09-07 17:18:48.514 | INFO     | src.policies:collect_trajectories:221 - Episode 1526\n",
      "2021-09-07 17:18:48.523 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.523 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 56.0\n",
      "2021-09-07 17:18:48.524 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 56.0\n",
      "2021-09-07 17:18:48.524 | INFO     | src.policies:collect_trajectories:221 - Episode 1527\n",
      "2021-09-07 17:18:48.555 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.555 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:48.556 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 128.0\n",
      "2021-09-07 17:18:48.556 | WARNING  | src.policies:train:144 - The actual batch size is 256, instead of 200\n",
      "2021-09-07 17:18:48.559 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:48.562 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.38583603501319885, 'baseline_loss': 1.117173433303833, 'total_loss': 0.17275068163871765}\n",
      "2021-09-07 17:18:48.563 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5045365691184998\n",
      "2021-09-07 17:18:48.564 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.643556833267212\n",
      "2021-09-07 17:18:48.565 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999895691871643\n",
      "2021-09-07 17:18:48.566 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:48.567 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:48.568 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.558007001876831, 'baseline_loss': 1.2181065082550049, 'total_loss': 0.05104625225067139}\n",
      "2021-09-07 17:18:48.569 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21716882288455963\n",
      "2021-09-07 17:18:48.569 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.426933765411377\n",
      "2021-09-07 17:18:48.570 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21716882288455963\n",
      "2021-09-07 17:18:48.571 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:48.573 | INFO     | src.policies:train:123 - Epoch 686 / 800\n",
      "2021-09-07 17:18:48.573 | INFO     | src.policies:collect_trajectories:221 - Episode 1528\n",
      "2021-09-07 17:18:48.604 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.605 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:48.605 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:48.608 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:48.609 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2307157665491104, 'baseline_loss': 0.432219535112381, 'total_loss': -0.014605998992919922}\n",
      "2021-09-07 17:18:48.610 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.36569589376449585\n",
      "2021-09-07 17:18:48.611 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.1560544967651367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:48.613 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.36569589376449585\n",
      "2021-09-07 17:18:48.614 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:48.615 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:48.617 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.143715962767601, 'baseline_loss': 0.43199270963668823, 'total_loss': 0.0722803920507431}\n",
      "2021-09-07 17:18:48.618 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24173404276371002\n",
      "2021-09-07 17:18:48.619 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4369062185287476\n",
      "2021-09-07 17:18:48.620 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24173404276371002\n",
      "2021-09-07 17:18:48.621 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:48.623 | INFO     | src.policies:train:123 - Epoch 687 / 800\n",
      "2021-09-07 17:18:48.623 | INFO     | src.policies:collect_trajectories:221 - Episode 1529\n",
      "2021-09-07 17:18:48.652 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.653 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:48.653 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:48.655 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:48.657 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.517784059047699, 'baseline_loss': 1.411228060722351, 'total_loss': 0.18782997131347656}\n",
      "2021-09-07 17:18:48.658 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5963358879089355\n",
      "2021-09-07 17:18:48.660 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.8684895038604736\n",
      "2021-09-07 17:18:48.661 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:48.662 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:48.663 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:48.664 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5513278245925903, 'baseline_loss': 1.427727460861206, 'total_loss': 0.1625359058380127}\n",
      "2021-09-07 17:18:48.665 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.40356096625328064\n",
      "2021-09-07 17:18:48.666 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.9131412506103516\n",
      "2021-09-07 17:18:48.667 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.40356096625328064\n",
      "2021-09-07 17:18:48.668 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:48.670 | INFO     | src.policies:train:123 - Epoch 688 / 800\n",
      "2021-09-07 17:18:48.670 | INFO     | src.policies:collect_trajectories:221 - Episode 1530\n",
      "2021-09-07 17:18:48.700 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.701 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:48.701 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:48.704 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:48.706 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1788555383682251, 'baseline_loss': 0.6010813117027283, 'total_loss': 0.12168511748313904}\n",
      "2021-09-07 17:18:48.707 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32505863904953003\n",
      "2021-09-07 17:18:48.709 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.0260632038116455\n",
      "2021-09-07 17:18:48.710 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32505863904953003\n",
      "2021-09-07 17:18:48.712 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999985098838806\n",
      "2021-09-07 17:18:48.713 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:48.715 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.11751487106084824, 'baseline_loss': 0.47909945249557495, 'total_loss': 0.12203485518693924}\n",
      "2021-09-07 17:18:48.716 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1526947021484375\n",
      "2021-09-07 17:18:48.716 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.673610210418701\n",
      "2021-09-07 17:18:48.717 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1526947021484375\n",
      "2021-09-07 17:18:48.718 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:48.720 | INFO     | src.policies:train:123 - Epoch 689 / 800\n",
      "2021-09-07 17:18:48.720 | INFO     | src.policies:collect_trajectories:221 - Episode 1531\n",
      "2021-09-07 17:18:48.750 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.751 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:48.751 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:48.753 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:48.755 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3831063210964203, 'baseline_loss': 0.7351773977279663, 'total_loss': -0.015517622232437134}\n",
      "2021-09-07 17:18:48.756 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11395847052335739\n",
      "2021-09-07 17:18:48.758 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8577134609222412\n",
      "2021-09-07 17:18:48.759 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11395847052335739\n",
      "2021-09-07 17:18:48.760 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:48.761 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:48.762 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4145081639289856, 'baseline_loss': 0.8276599049568176, 'total_loss': -0.0006782114505767822}\n",
      "2021-09-07 17:18:48.763 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2731994092464447\n",
      "2021-09-07 17:18:48.764 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8046970367431641\n",
      "2021-09-07 17:18:48.765 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2731994092464447\n",
      "2021-09-07 17:18:48.767 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:48.768 | INFO     | src.policies:train:123 - Epoch 690 / 800\n",
      "2021-09-07 17:18:48.769 | INFO     | src.policies:collect_trajectories:221 - Episode 1532\n",
      "2021-09-07 17:18:48.798 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.799 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:48.799 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:48.802 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:48.804 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1922440528869629, 'baseline_loss': 0.38689398765563965, 'total_loss': 0.0012029409408569336}\n",
      "2021-09-07 17:18:48.805 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3134273290634155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:48.806 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9078142046928406\n",
      "2021-09-07 17:18:48.807 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3134273290634155\n",
      "2021-09-07 17:18:48.808 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:48.810 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:48.811 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.26315486431121826, 'baseline_loss': 0.4232656955718994, 'total_loss': -0.051522016525268555}\n",
      "2021-09-07 17:18:48.813 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10172299295663834\n",
      "2021-09-07 17:18:48.814 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6598833799362183\n",
      "2021-09-07 17:18:48.815 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10172299295663834\n",
      "2021-09-07 17:18:48.816 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:48.817 | INFO     | src.policies:train:123 - Epoch 691 / 800\n",
      "2021-09-07 17:18:48.817 | INFO     | src.policies:collect_trajectories:221 - Episode 1533\n",
      "2021-09-07 17:18:48.846 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.846 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:48.847 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:48.848 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:48.851 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24235893785953522, 'baseline_loss': 0.38225775957107544, 'total_loss': -0.0512300580739975}\n",
      "2021-09-07 17:18:48.852 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32697558403015137\n",
      "2021-09-07 17:18:48.853 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.057803988456726\n",
      "2021-09-07 17:18:48.854 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32697558403015137\n",
      "2021-09-07 17:18:48.855 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:48.856 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:48.857 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1246660128235817, 'baseline_loss': 0.4333101511001587, 'total_loss': 0.09198906272649765}\n",
      "2021-09-07 17:18:48.858 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10802377760410309\n",
      "2021-09-07 17:18:48.859 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3003493547439575\n",
      "2021-09-07 17:18:48.860 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10802377760410309\n",
      "2021-09-07 17:18:48.861 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:48.862 | INFO     | src.policies:train:123 - Epoch 692 / 800\n",
      "2021-09-07 17:18:48.863 | INFO     | src.policies:collect_trajectories:221 - Episode 1534\n",
      "2021-09-07 17:18:48.892 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.892 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:48.893 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:48.895 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:48.897 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24051019549369812, 'baseline_loss': 0.3818092346191406, 'total_loss': -0.04960557818412781}\n",
      "2021-09-07 17:18:48.898 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2734488546848297\n",
      "2021-09-07 17:18:48.899 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6739805936813354\n",
      "2021-09-07 17:18:48.900 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2734488546848297\n",
      "2021-09-07 17:18:48.901 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:48.902 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:48.903 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23236261308193207, 'baseline_loss': 0.37953802943229675, 'total_loss': -0.04259359836578369}\n",
      "2021-09-07 17:18:48.904 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07343325763940811\n",
      "2021-09-07 17:18:48.904 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1800880432128906\n",
      "2021-09-07 17:18:48.905 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07343325763940811\n",
      "2021-09-07 17:18:48.907 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:48.908 | INFO     | src.policies:train:123 - Epoch 693 / 800\n",
      "2021-09-07 17:18:48.909 | INFO     | src.policies:collect_trajectories:221 - Episode 1535\n",
      "2021-09-07 17:18:48.939 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:48.940 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:48.940 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:48.942 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:48.945 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.33607977628707886, 'baseline_loss': 0.5842738747596741, 'total_loss': -0.04394283890724182}\n",
      "2021-09-07 17:18:48.947 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17020714282989502\n",
      "2021-09-07 17:18:48.948 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8097609877586365\n",
      "2021-09-07 17:18:48.948 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17020714282989502\n",
      "2021-09-07 17:18:48.949 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:48.951 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:48.952 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3869559168815613, 'baseline_loss': 0.6399006843566895, 'total_loss': -0.06700557470321655}\n",
      "2021-09-07 17:18:48.953 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20121115446090698\n",
      "2021-09-07 17:18:48.954 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5905539393424988\n",
      "2021-09-07 17:18:48.955 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20121115446090698\n",
      "2021-09-07 17:18:48.956 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:48.957 | INFO     | src.policies:train:123 - Epoch 694 / 800\n",
      "2021-09-07 17:18:48.957 | INFO     | src.policies:collect_trajectories:221 - Episode 1536\n",
      "2021-09-07 17:18:48.987 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:49.005 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:49.018 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:49.046 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:49.049 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.13010664284229279, 'baseline_loss': 0.500175416469574, 'total_loss': 0.1199810653924942}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:49.050 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3069393038749695\n",
      "2021-09-07 17:18:49.051 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5704988241195679\n",
      "2021-09-07 17:18:49.052 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3069393038749695\n",
      "2021-09-07 17:18:49.053 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:49.054 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:49.055 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.07859165966510773, 'baseline_loss': 0.529151439666748, 'total_loss': 0.1859840601682663}\n",
      "2021-09-07 17:18:49.056 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23823602497577667\n",
      "2021-09-07 17:18:49.057 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9437576532363892\n",
      "2021-09-07 17:18:49.058 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23823602497577667\n",
      "2021-09-07 17:18:49.059 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:49.060 | INFO     | src.policies:train:123 - Epoch 695 / 800\n",
      "2021-09-07 17:18:49.060 | INFO     | src.policies:collect_trajectories:221 - Episode 1537\n",
      "2021-09-07 17:18:49.088 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:49.089 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:49.089 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:49.091 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:49.093 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41157084703445435, 'baseline_loss': 0.746509850025177, 'total_loss': -0.038315922021865845}\n",
      "2021-09-07 17:18:49.094 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.229707270860672\n",
      "2021-09-07 17:18:49.095 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6179730892181396\n",
      "2021-09-07 17:18:49.096 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.229707270860672\n",
      "2021-09-07 17:18:49.097 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:49.098 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:49.099 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40142822265625, 'baseline_loss': 0.7863280773162842, 'total_loss': -0.00826418399810791}\n",
      "2021-09-07 17:18:49.100 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1994650810956955\n",
      "2021-09-07 17:18:49.101 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9551315903663635\n",
      "2021-09-07 17:18:49.102 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1994650810956955\n",
      "2021-09-07 17:18:49.103 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:49.104 | INFO     | src.policies:train:123 - Epoch 696 / 800\n",
      "2021-09-07 17:18:49.105 | INFO     | src.policies:collect_trajectories:221 - Episode 1538\n",
      "2021-09-07 17:18:49.133 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:49.133 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:49.134 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:49.135 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:49.138 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20551925897598267, 'baseline_loss': 0.6304798126220703, 'total_loss': 0.10972064733505249}\n",
      "2021-09-07 17:18:49.139 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2490694373846054\n",
      "2021-09-07 17:18:49.140 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7708326578140259\n",
      "2021-09-07 17:18:49.141 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2490694373846054\n",
      "2021-09-07 17:18:49.142 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:49.143 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:49.144 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1508689671754837, 'baseline_loss': 0.524212121963501, 'total_loss': 0.11123709380626678}\n",
      "2021-09-07 17:18:49.145 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23569130897521973\n",
      "2021-09-07 17:18:49.146 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6558314561843872\n",
      "2021-09-07 17:18:49.147 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23569130897521973\n",
      "2021-09-07 17:18:49.148 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:49.149 | INFO     | src.policies:train:123 - Epoch 697 / 800\n",
      "2021-09-07 17:18:49.150 | INFO     | src.policies:collect_trajectories:221 - Episode 1539\n",
      "2021-09-07 17:18:49.180 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:49.180 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:49.181 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:49.184 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:49.186 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36266201734542847, 'baseline_loss': 0.5411298274993896, 'total_loss': -0.09209710359573364}\n",
      "2021-09-07 17:18:49.187 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.38022875785827637\n",
      "2021-09-07 17:18:49.188 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3430485129356384\n",
      "2021-09-07 17:18:49.189 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.38022875785827637\n",
      "2021-09-07 17:18:49.190 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3430485129356384\n",
      "2021-09-07 17:18:49.191 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:49.192 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.39651012420654297, 'baseline_loss': 0.5251359939575195, 'total_loss': -0.1339421272277832}\n",
      "2021-09-07 17:18:49.193 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.46979811787605286\n",
      "2021-09-07 17:18:49.194 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4223198890686035\n",
      "2021-09-07 17:18:49.195 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.46979811787605286\n",
      "2021-09-07 17:18:49.196 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4223198890686035\n",
      "2021-09-07 17:18:49.197 | INFO     | src.policies:train:123 - Epoch 698 / 800\n",
      "2021-09-07 17:18:49.197 | INFO     | src.policies:collect_trajectories:221 - Episode 1540\n",
      "2021-09-07 17:18:49.226 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:49.227 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:49.227 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:49.230 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:49.232 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31697970628738403, 'baseline_loss': 0.41786572337150574, 'total_loss': -0.10804684460163116}\n",
      "2021-09-07 17:18:49.233 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20085330307483673\n",
      "2021-09-07 17:18:49.234 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5135137438774109\n",
      "2021-09-07 17:18:49.235 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20085330307483673\n",
      "2021-09-07 17:18:49.236 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999889731407166\n",
      "2021-09-07 17:18:49.237 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:49.238 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3100651502609253, 'baseline_loss': 0.4149540066719055, 'total_loss': -0.10258814692497253}\n",
      "2021-09-07 17:18:49.239 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15750427544116974\n",
      "2021-09-07 17:18:49.240 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0073943138122559\n",
      "2021-09-07 17:18:49.241 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15750427544116974\n",
      "2021-09-07 17:18:49.242 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:49.243 | INFO     | src.policies:train:123 - Epoch 699 / 800\n",
      "2021-09-07 17:18:49.244 | INFO     | src.policies:collect_trajectories:221 - Episode 1541\n",
      "2021-09-07 17:18:49.273 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:49.274 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:49.274 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:49.276 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:49.279 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7008353471755981, 'baseline_loss': 1.6670303344726562, 'total_loss': 0.13267982006072998}\n",
      "2021-09-07 17:18:49.280 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.33567771315574646\n",
      "2021-09-07 17:18:49.281 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.5835113525390625\n",
      "2021-09-07 17:18:49.282 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.33567771315574646\n",
      "2021-09-07 17:18:49.283 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:49.284 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:49.285 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7699730396270752, 'baseline_loss': 2.120544910430908, 'total_loss': 0.2902994155883789}\n",
      "2021-09-07 17:18:49.286 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5819216370582581\n",
      "2021-09-07 17:18:49.286 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.5324482917785645\n",
      "2021-09-07 17:18:49.287 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:49.288 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:49.290 | INFO     | src.policies:train:123 - Epoch 700 / 800\n",
      "2021-09-07 17:18:49.290 | INFO     | src.policies:collect_trajectories:221 - Episode 1542\n",
      "2021-09-07 17:18:49.320 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:49.321 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:49.322 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:49.324 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:49.327 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5927398800849915, 'baseline_loss': 1.5426722764968872, 'total_loss': 0.17859625816345215}\n",
      "2021-09-07 17:18:49.328 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12980128824710846\n",
      "2021-09-07 17:18:49.328 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.124295234680176\n",
      "2021-09-07 17:18:49.329 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12980128824710846\n",
      "2021-09-07 17:18:49.330 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:49.332 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:49.333 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6597192287445068, 'baseline_loss': 1.6707491874694824, 'total_loss': 0.17565536499023438}\n",
      "2021-09-07 17:18:49.333 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.420545369386673\n",
      "2021-09-07 17:18:49.334 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.5361545085906982\n",
      "2021-09-07 17:18:49.335 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.420545369386673\n",
      "2021-09-07 17:18:49.336 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:49.338 | INFO     | src.policies:train:123 - Epoch 701 / 800\n",
      "2021-09-07 17:18:49.338 | INFO     | src.policies:collect_trajectories:221 - Episode 1543\n",
      "2021-09-07 17:18:49.367 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:49.368 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:49.368 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:49.370 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:49.373 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.25254151225090027, 'baseline_loss': 0.4091215431690216, 'total_loss': -0.047980740666389465}\n",
      "2021-09-07 17:18:49.374 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.48988544940948486\n",
      "2021-09-07 17:18:49.375 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8459456562995911\n",
      "2021-09-07 17:18:49.376 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.48988544940948486\n",
      "2021-09-07 17:18:49.377 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:49.378 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:49.380 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.28601494431495667, 'baseline_loss': 0.38005179166793823, 'total_loss': -0.09598904848098755}\n",
      "2021-09-07 17:18:49.381 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3316444754600525\n",
      "2021-09-07 17:18:49.382 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7217252254486084\n",
      "2021-09-07 17:18:49.383 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3316444754600525\n",
      "2021-09-07 17:18:49.384 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:49.385 | INFO     | src.policies:train:123 - Epoch 702 / 800\n",
      "2021-09-07 17:18:49.386 | INFO     | src.policies:collect_trajectories:221 - Episode 1544\n",
      "2021-09-07 17:18:49.415 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:49.415 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:49.415 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:49.418 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:49.420 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2290123552083969, 'baseline_loss': 0.41896918416023254, 'total_loss': -0.01952776312828064}\n",
      "2021-09-07 17:18:49.421 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.524260401725769\n",
      "2021-09-07 17:18:49.423 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8242222666740417\n",
      "2021-09-07 17:18:49.424 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:49.425 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:49.427 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:49.428 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.10894884169101715, 'baseline_loss': 0.42286327481269836, 'total_loss': 0.10248279571533203}\n",
      "2021-09-07 17:18:49.429 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1863510012626648\n",
      "2021-09-07 17:18:49.430 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4750723838806152\n",
      "2021-09-07 17:18:49.431 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1863510012626648\n",
      "2021-09-07 17:18:49.432 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:49.433 | INFO     | src.policies:train:123 - Epoch 703 / 800\n",
      "2021-09-07 17:18:49.434 | INFO     | src.policies:collect_trajectories:221 - Episode 1545\n",
      "2021-09-07 17:18:49.465 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:49.465 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:49.466 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:49.467 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:49.470 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27157697081565857, 'baseline_loss': 0.4411805272102356, 'total_loss': -0.05098670721054077}\n",
      "2021-09-07 17:18:49.471 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12960267066955566\n",
      "2021-09-07 17:18:49.472 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4359996318817139\n",
      "2021-09-07 17:18:49.473 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12960267066955566\n",
      "2021-09-07 17:18:49.474 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:49.475 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:49.476 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2886951267719269, 'baseline_loss': 0.41628387570381165, 'total_loss': -0.08055318892002106}\n",
      "2021-09-07 17:18:49.477 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16728003323078156\n",
      "2021-09-07 17:18:49.478 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9008969664573669\n",
      "2021-09-07 17:18:49.479 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16728003323078156\n",
      "2021-09-07 17:18:49.480 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:49.482 | INFO     | src.policies:train:123 - Epoch 704 / 800\n",
      "2021-09-07 17:18:49.482 | INFO     | src.policies:collect_trajectories:221 - Episode 1546\n",
      "2021-09-07 17:18:49.511 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:49.511 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:49.512 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:49.513 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:49.516 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4461408257484436, 'baseline_loss': 0.8936437964439392, 'total_loss': 0.000681072473526001}\n",
      "2021-09-07 17:18:49.517 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14716294407844543\n",
      "2021-09-07 17:18:49.518 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2056260108947754\n",
      "2021-09-07 17:18:49.519 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14716294407844543\n",
      "2021-09-07 17:18:49.520 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:49.522 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:49.523 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4918421804904938, 'baseline_loss': 0.869505763053894, 'total_loss': -0.05708929896354675}\n",
      "2021-09-07 17:18:49.524 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4704783260822296\n",
      "2021-09-07 17:18:49.525 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4567018747329712\n",
      "2021-09-07 17:18:49.526 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4704783260822296\n",
      "2021-09-07 17:18:49.527 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:49.528 | INFO     | src.policies:train:123 - Epoch 705 / 800\n",
      "2021-09-07 17:18:49.529 | INFO     | src.policies:collect_trajectories:221 - Episode 1547\n",
      "2021-09-07 17:18:49.614 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:49.615 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:49.615 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:49.617 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:49.620 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.331919401884079, 'baseline_loss': 0.4104408025741577, 'total_loss': -0.12669900059700012}\n",
      "2021-09-07 17:18:49.621 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3297383487224579\n",
      "2021-09-07 17:18:49.622 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4966445565223694\n",
      "2021-09-07 17:18:49.623 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3297383487224579\n",
      "2021-09-07 17:18:49.625 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4966445565223694\n",
      "2021-09-07 17:18:49.626 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:49.627 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.30401551723480225, 'baseline_loss': 0.42908501625061035, 'total_loss': -0.08947300910949707}\n",
      "2021-09-07 17:18:49.628 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18191084265708923\n",
      "2021-09-07 17:18:49.629 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.424955815076828\n",
      "2021-09-07 17:18:49.630 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18191084265708923\n",
      "2021-09-07 17:18:49.631 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.424955815076828\n",
      "2021-09-07 17:18:49.632 | INFO     | src.policies:train:123 - Epoch 706 / 800\n",
      "2021-09-07 17:18:49.633 | INFO     | src.policies:collect_trajectories:221 - Episode 1548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:49.663 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:49.663 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:49.664 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:49.666 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:49.668 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1440471112728119, 'baseline_loss': 0.450190007686615, 'total_loss': 0.0810478925704956}\n",
      "2021-09-07 17:18:49.669 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12156384438276291\n",
      "2021-09-07 17:18:49.670 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0245121717453003\n",
      "2021-09-07 17:18:49.671 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12156384438276291\n",
      "2021-09-07 17:18:49.673 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:49.674 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:49.675 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2725970149040222, 'baseline_loss': 0.41330137848854065, 'total_loss': -0.06594632565975189}\n",
      "2021-09-07 17:18:49.676 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.39854657649993896\n",
      "2021-09-07 17:18:49.677 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.911190390586853\n",
      "2021-09-07 17:18:49.678 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.39854657649993896\n",
      "2021-09-07 17:18:49.679 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:49.681 | INFO     | src.policies:train:123 - Epoch 707 / 800\n",
      "2021-09-07 17:18:49.681 | INFO     | src.policies:collect_trajectories:221 - Episode 1549\n",
      "2021-09-07 17:18:49.709 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:49.709 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:49.710 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:49.712 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:49.715 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1909886598587036, 'baseline_loss': 0.3600268065929413, 'total_loss': -0.010975256562232971}\n",
      "2021-09-07 17:18:49.716 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07316695153713226\n",
      "2021-09-07 17:18:49.717 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8913484215736389\n",
      "2021-09-07 17:18:49.718 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07316695153713226\n",
      "2021-09-07 17:18:49.719 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:49.720 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:49.722 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.28996190428733826, 'baseline_loss': 0.41115471720695496, 'total_loss': -0.08438454568386078}\n",
      "2021-09-07 17:18:49.722 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3434390723705292\n",
      "2021-09-07 17:18:49.724 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1769276857376099\n",
      "2021-09-07 17:18:49.725 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3434390723705292\n",
      "2021-09-07 17:18:49.727 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:49.728 | INFO     | src.policies:train:123 - Epoch 708 / 800\n",
      "2021-09-07 17:18:49.729 | INFO     | src.policies:collect_trajectories:221 - Episode 1550\n",
      "2021-09-07 17:18:49.873 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:49.873 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:49.874 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:49.876 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:49.879 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1682845652103424, 'baseline_loss': 0.425693154335022, 'total_loss': 0.04456201195716858}\n",
      "2021-09-07 17:18:49.880 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5425698161125183\n",
      "2021-09-07 17:18:49.881 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.1168301105499268\n",
      "2021-09-07 17:18:49.882 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:49.883 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:49.884 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:49.885 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.22156456112861633, 'baseline_loss': 0.44285818934440613, 'total_loss': -0.00013546645641326904}\n",
      "2021-09-07 17:18:49.886 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23248878121376038\n",
      "2021-09-07 17:18:49.887 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3039112091064453\n",
      "2021-09-07 17:18:49.888 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23248878121376038\n",
      "2021-09-07 17:18:49.889 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:49.890 | INFO     | src.policies:train:123 - Epoch 709 / 800\n",
      "2021-09-07 17:18:49.891 | INFO     | src.policies:collect_trajectories:221 - Episode 1551\n",
      "2021-09-07 17:18:49.925 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:49.927 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:49.928 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:49.930 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:49.932 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31383970379829407, 'baseline_loss': 0.4341297447681427, 'total_loss': -0.09677483141422272}\n",
      "2021-09-07 17:18:49.933 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14152184128761292\n",
      "2021-09-07 17:18:49.933 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6220369338989258\n",
      "2021-09-07 17:18:49.934 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14152184128761292\n",
      "2021-09-07 17:18:49.935 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:49.937 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:49.939 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3410628139972687, 'baseline_loss': 0.5484904646873474, 'total_loss': -0.06681758165359497}\n",
      "2021-09-07 17:18:49.941 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21199938654899597\n",
      "2021-09-07 17:18:49.942 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6759702563285828\n",
      "2021-09-07 17:18:49.943 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21199938654899597\n",
      "2021-09-07 17:18:49.944 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:49.946 | INFO     | src.policies:train:123 - Epoch 710 / 800\n",
      "2021-09-07 17:18:49.946 | INFO     | src.policies:collect_trajectories:221 - Episode 1552\n",
      "2021-09-07 17:18:49.977 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:49.978 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:49.978 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:49.981 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:49.984 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4538159966468811, 'baseline_loss': 0.833057701587677, 'total_loss': -0.0372871458530426}\n",
      "2021-09-07 17:18:49.985 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16567187011241913\n",
      "2021-09-07 17:18:49.986 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3530610799789429\n",
      "2021-09-07 17:18:49.987 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16567187011241913\n",
      "2021-09-07 17:18:49.988 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:49.989 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:49.990 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.34534695744514465, 'baseline_loss': 0.42124757170677185, 'total_loss': -0.13472317159175873}\n",
      "2021-09-07 17:18:49.991 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16086117923259735\n",
      "2021-09-07 17:18:49.993 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5859527587890625\n",
      "2021-09-07 17:18:49.994 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16086117923259735\n",
      "2021-09-07 17:18:49.995 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:49.997 | INFO     | src.policies:train:123 - Epoch 711 / 800\n",
      "2021-09-07 17:18:49.997 | INFO     | src.policies:collect_trajectories:221 - Episode 1553\n",
      "2021-09-07 17:18:50.029 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:50.030 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:50.030 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:50.032 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:50.034 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4045625627040863, 'baseline_loss': 0.5944970846176147, 'total_loss': -0.10731402039527893}\n",
      "2021-09-07 17:18:50.035 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1790153682231903\n",
      "2021-09-07 17:18:50.035 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6488033533096313\n",
      "2021-09-07 17:18:50.037 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1790153682231903\n",
      "2021-09-07 17:18:50.038 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:50.039 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:50.041 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36642733216285706, 'baseline_loss': 0.4820944666862488, 'total_loss': -0.12538009881973267}\n",
      "2021-09-07 17:18:50.042 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3908935785293579\n",
      "2021-09-07 17:18:50.043 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6806371808052063\n",
      "2021-09-07 17:18:50.044 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3908935785293579\n",
      "2021-09-07 17:18:50.045 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:50.046 | INFO     | src.policies:train:123 - Epoch 712 / 800\n",
      "2021-09-07 17:18:50.047 | INFO     | src.policies:collect_trajectories:221 - Episode 1554\n",
      "2021-09-07 17:18:50.078 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:50.078 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:50.079 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:50.081 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:50.083 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.437980055809021, 'baseline_loss': 0.7683750987052917, 'total_loss': -0.05379250645637512}\n",
      "2021-09-07 17:18:50.084 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32967913150787354\n",
      "2021-09-07 17:18:50.086 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9153311252593994\n",
      "2021-09-07 17:18:50.087 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32967913150787354\n",
      "2021-09-07 17:18:50.088 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:50.089 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:50.091 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.30645105242729187, 'baseline_loss': 0.42290636897087097, 'total_loss': -0.09499786794185638}\n",
      "2021-09-07 17:18:50.092 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13269177079200745\n",
      "2021-09-07 17:18:50.093 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4899141192436218\n",
      "2021-09-07 17:18:50.095 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13269177079200745\n",
      "2021-09-07 17:18:50.096 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4899141192436218\n",
      "2021-09-07 17:18:50.097 | INFO     | src.policies:train:123 - Epoch 713 / 800\n",
      "2021-09-07 17:18:50.098 | INFO     | src.policies:collect_trajectories:221 - Episode 1555\n",
      "2021-09-07 17:18:50.257 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:50.258 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:50.258 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:50.260 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:50.262 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.9941782355308533, 'baseline_loss': 2.928938865661621, 'total_loss': 0.4702911972999573}\n",
      "2021-09-07 17:18:50.263 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.8644344806671143\n",
      "2021-09-07 17:18:50.264 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 6.567916393280029\n",
      "2021-09-07 17:18:50.265 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:50.266 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999999403953552\n",
      "2021-09-07 17:18:50.268 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:50.269 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.7967397570610046, 'baseline_loss': 2.678831100463867, 'total_loss': 0.542675793170929}\n",
      "2021-09-07 17:18:50.270 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4683513641357422\n",
      "2021-09-07 17:18:50.271 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 6.029576301574707\n",
      "2021-09-07 17:18:50.272 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4683513641357422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:50.273 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999999403953552\n",
      "2021-09-07 17:18:50.275 | INFO     | src.policies:train:123 - Epoch 714 / 800\n",
      "2021-09-07 17:18:50.275 | INFO     | src.policies:collect_trajectories:221 - Episode 1556\n",
      "2021-09-07 17:18:50.310 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:50.311 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:50.311 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:50.313 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:50.315 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5379276871681213, 'baseline_loss': 0.9081854224205017, 'total_loss': -0.08383497595787048}\n",
      "2021-09-07 17:18:50.316 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18338549137115479\n",
      "2021-09-07 17:18:50.317 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9239726066589355\n",
      "2021-09-07 17:18:50.318 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18338549137115479\n",
      "2021-09-07 17:18:50.319 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:50.321 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:50.322 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5736520290374756, 'baseline_loss': 0.9944204688072205, 'total_loss': -0.07644179463386536}\n",
      "2021-09-07 17:18:50.323 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1729847639799118\n",
      "2021-09-07 17:18:50.324 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8025474548339844\n",
      "2021-09-07 17:18:50.325 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1729847639799118\n",
      "2021-09-07 17:18:50.326 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:50.328 | INFO     | src.policies:train:123 - Epoch 715 / 800\n",
      "2021-09-07 17:18:50.328 | INFO     | src.policies:collect_trajectories:221 - Episode 1557\n",
      "2021-09-07 17:18:50.357 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:50.357 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:50.358 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:50.361 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:50.362 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5207414627075195, 'baseline_loss': 1.061194896697998, 'total_loss': 0.009855985641479492}\n",
      "2021-09-07 17:18:50.363 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24012115597724915\n",
      "2021-09-07 17:18:50.364 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3977479934692383\n",
      "2021-09-07 17:18:50.365 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24012115597724915\n",
      "2021-09-07 17:18:50.366 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:50.368 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:50.369 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5624530911445618, 'baseline_loss': 1.0780669450759888, 'total_loss': -0.023419618606567383}\n",
      "2021-09-07 17:18:50.370 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5444664359092712\n",
      "2021-09-07 17:18:50.371 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6500492095947266\n",
      "2021-09-07 17:18:50.372 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:50.373 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:50.374 | INFO     | src.policies:train:123 - Epoch 716 / 800\n",
      "2021-09-07 17:18:50.374 | INFO     | src.policies:collect_trajectories:221 - Episode 1558\n",
      "2021-09-07 17:18:50.400 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:50.400 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 171.0\n",
      "2021-09-07 17:18:50.401 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 171.0\n",
      "2021-09-07 17:18:50.401 | INFO     | src.policies:collect_trajectories:221 - Episode 1559\n",
      "2021-09-07 17:18:50.432 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:50.432 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:50.433 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 185.5\n",
      "2021-09-07 17:18:50.433 | WARNING  | src.policies:train:144 - The actual batch size is 371, instead of 200\n",
      "2021-09-07 17:18:50.436 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:50.438 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.39717957377433777, 'baseline_loss': 0.510224461555481, 'total_loss': -0.1420673429965973}\n",
      "2021-09-07 17:18:50.439 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25854843854904175\n",
      "2021-09-07 17:18:50.440 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.37325024604797363\n",
      "2021-09-07 17:18:50.442 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25854843854904175\n",
      "2021-09-07 17:18:50.443 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.37325024604797363\n",
      "2021-09-07 17:18:50.445 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:50.446 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3180920481681824, 'baseline_loss': 0.5631988048553467, 'total_loss': -0.03649264574050903}\n",
      "2021-09-07 17:18:50.447 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2158185839653015\n",
      "2021-09-07 17:18:50.448 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3624812662601471\n",
      "2021-09-07 17:18:50.449 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2158185839653015\n",
      "2021-09-07 17:18:50.450 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3624812662601471\n",
      "2021-09-07 17:18:50.451 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:50.452 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.16082561016082764, 'baseline_loss': 0.38471779227256775, 'total_loss': 0.03153328597545624}\n",
      "2021-09-07 17:18:50.453 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1441711187362671\n",
      "2021-09-07 17:18:50.454 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8380916118621826\n",
      "2021-09-07 17:18:50.456 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1441711187362671\n",
      "2021-09-07 17:18:50.457 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:50.459 | INFO     | src.policies:train:123 - Epoch 717 / 800\n",
      "2021-09-07 17:18:50.459 | INFO     | src.policies:collect_trajectories:221 - Episode 1560\n",
      "2021-09-07 17:18:50.490 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:50.490 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 198.0\n",
      "2021-09-07 17:18:50.491 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 198.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:50.491 | INFO     | src.policies:collect_trajectories:221 - Episode 1561\n",
      "2021-09-07 17:18:50.522 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:50.522 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:50.523 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 199.0\n",
      "2021-09-07 17:18:50.523 | WARNING  | src.policies:train:144 - The actual batch size is 398, instead of 200\n",
      "2021-09-07 17:18:50.526 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:50.528 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.37649112939834595, 'baseline_loss': 0.5719707012176514, 'total_loss': -0.09050577878952026}\n",
      "2021-09-07 17:18:50.529 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5684343576431274\n",
      "2021-09-07 17:18:50.530 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4801543653011322\n",
      "2021-09-07 17:18:50.531 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:50.532 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4801543653011322\n",
      "2021-09-07 17:18:50.533 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:50.534 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.201121985912323, 'baseline_loss': 0.3768315613269806, 'total_loss': -0.012706205248832703}\n",
      "2021-09-07 17:18:50.535 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3904533088207245\n",
      "2021-09-07 17:18:50.536 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1795448064804077\n",
      "2021-09-07 17:18:50.537 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3904533088207245\n",
      "2021-09-07 17:18:50.538 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:50.539 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:50.540 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1973910927772522, 'baseline_loss': 0.3455333411693573, 'total_loss': -0.024624422192573547}\n",
      "2021-09-07 17:18:50.541 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3940599858760834\n",
      "2021-09-07 17:18:50.542 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1986782550811768\n",
      "2021-09-07 17:18:50.543 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3940599858760834\n",
      "2021-09-07 17:18:50.544 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:50.545 | INFO     | src.policies:train:123 - Epoch 718 / 800\n",
      "2021-09-07 17:18:50.546 | INFO     | src.policies:collect_trajectories:221 - Episode 1562\n",
      "2021-09-07 17:18:50.576 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:50.577 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:50.577 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:50.579 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:50.582 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3380713164806366, 'baseline_loss': 0.4607493281364441, 'total_loss': -0.10769665241241455}\n",
      "2021-09-07 17:18:50.582 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7336514592170715\n",
      "2021-09-07 17:18:50.583 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2533744275569916\n",
      "2021-09-07 17:18:50.584 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:50.585 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2533744275569916\n",
      "2021-09-07 17:18:50.586 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:50.587 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2958090901374817, 'baseline_loss': 0.4373480975627899, 'total_loss': -0.07713504135608673}\n",
      "2021-09-07 17:18:50.588 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.44150444865226746\n",
      "2021-09-07 17:18:50.589 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6060351729393005\n",
      "2021-09-07 17:18:50.590 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.44150444865226746\n",
      "2021-09-07 17:18:50.591 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:50.593 | INFO     | src.policies:train:123 - Epoch 719 / 800\n",
      "2021-09-07 17:18:50.593 | INFO     | src.policies:collect_trajectories:221 - Episode 1563\n",
      "2021-09-07 17:18:50.626 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:50.627 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:50.628 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:50.630 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:50.632 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36940011382102966, 'baseline_loss': 0.47780516743659973, 'total_loss': -0.1304975301027298}\n",
      "2021-09-07 17:18:50.633 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4116719663143158\n",
      "2021-09-07 17:18:50.634 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.39541786909103394\n",
      "2021-09-07 17:18:50.635 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4116719663143158\n",
      "2021-09-07 17:18:50.636 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.39541786909103394\n",
      "2021-09-07 17:18:50.637 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:50.639 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32194221019744873, 'baseline_loss': 0.4734767973423004, 'total_loss': -0.08520381152629852}\n",
      "2021-09-07 17:18:50.640 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1030966117978096\n",
      "2021-09-07 17:18:50.641 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.33319002389907837\n",
      "2021-09-07 17:18:50.642 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1030966117978096\n",
      "2021-09-07 17:18:50.643 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.33319002389907837\n",
      "2021-09-07 17:18:50.644 | INFO     | src.policies:train:123 - Epoch 720 / 800\n",
      "2021-09-07 17:18:50.644 | INFO     | src.policies:collect_trajectories:221 - Episode 1564\n",
      "2021-09-07 17:18:50.674 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:50.674 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:50.675 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:50.677 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:50.679 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.21253839135169983, 'baseline_loss': 0.40463683009147644, 'total_loss': -0.010219976305961609}\n",
      "2021-09-07 17:18:50.680 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2002769261598587\n",
      "2021-09-07 17:18:50.682 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9329359531402588\n",
      "2021-09-07 17:18:50.683 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2002769261598587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:50.684 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:50.685 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:50.686 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2180750072002411, 'baseline_loss': 0.3635489344596863, 'total_loss': -0.03630053997039795}\n",
      "2021-09-07 17:18:50.687 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19295372068881989\n",
      "2021-09-07 17:18:50.688 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3128821849822998\n",
      "2021-09-07 17:18:50.690 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19295372068881989\n",
      "2021-09-07 17:18:50.691 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:50.692 | INFO     | src.policies:train:123 - Epoch 721 / 800\n",
      "2021-09-07 17:18:50.693 | INFO     | src.policies:collect_trajectories:221 - Episode 1565\n",
      "2021-09-07 17:18:50.722 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:50.722 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:50.723 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:50.730 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:50.778 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20489637553691864, 'baseline_loss': 0.3465842008590698, 'total_loss': -0.03160427510738373}\n",
      "2021-09-07 17:18:50.779 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4708498418331146\n",
      "2021-09-07 17:18:50.780 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5399980545043945\n",
      "2021-09-07 17:18:50.782 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4708498418331146\n",
      "2021-09-07 17:18:50.783 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:50.784 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:50.785 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.28740403056144714, 'baseline_loss': 0.40471550822257996, 'total_loss': -0.08504627645015717}\n",
      "2021-09-07 17:18:50.786 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.35984861850738525\n",
      "2021-09-07 17:18:50.787 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.683682382106781\n",
      "2021-09-07 17:18:50.788 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.35984861850738525\n",
      "2021-09-07 17:18:50.789 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:50.790 | INFO     | src.policies:train:123 - Epoch 722 / 800\n",
      "2021-09-07 17:18:50.791 | INFO     | src.policies:collect_trajectories:221 - Episode 1566\n",
      "2021-09-07 17:18:50.815 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:50.816 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 162.0\n",
      "2021-09-07 17:18:50.816 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 162.0\n",
      "2021-09-07 17:18:50.816 | INFO     | src.policies:collect_trajectories:221 - Episode 1567\n",
      "2021-09-07 17:18:50.844 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:50.845 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 170.0\n",
      "2021-09-07 17:18:50.845 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 166.0\n",
      "2021-09-07 17:18:50.846 | WARNING  | src.policies:train:144 - The actual batch size is 332, instead of 200\n",
      "2021-09-07 17:18:50.850 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:50.852 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32946789264678955, 'baseline_loss': 0.5584537386894226, 'total_loss': -0.05024102330207825}\n",
      "2021-09-07 17:18:50.853 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.29613855481147766\n",
      "2021-09-07 17:18:50.854 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4184534251689911\n",
      "2021-09-07 17:18:50.855 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.29613855481147766\n",
      "2021-09-07 17:18:50.857 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4184534251689911\n",
      "2021-09-07 17:18:50.858 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:50.860 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3574902415275574, 'baseline_loss': 0.48003309965133667, 'total_loss': -0.11747369170188904}\n",
      "2021-09-07 17:18:50.861 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1939324140548706\n",
      "2021-09-07 17:18:50.862 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3370562195777893\n",
      "2021-09-07 17:18:50.863 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1939324140548706\n",
      "2021-09-07 17:18:50.864 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3370562195777893\n",
      "2021-09-07 17:18:50.865 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:50.867 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.481332927942276, 'baseline_loss': 0.7325571179389954, 'total_loss': -0.11505436897277832}\n",
      "2021-09-07 17:18:50.868 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19283053278923035\n",
      "2021-09-07 17:18:50.869 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6540125012397766\n",
      "2021-09-07 17:18:50.870 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19283053278923035\n",
      "2021-09-07 17:18:50.871 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:50.872 | INFO     | src.policies:train:123 - Epoch 723 / 800\n",
      "2021-09-07 17:18:50.873 | INFO     | src.policies:collect_trajectories:221 - Episode 1568\n",
      "2021-09-07 17:18:50.902 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:50.903 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:50.903 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:50.905 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:50.908 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45590588450431824, 'baseline_loss': 0.7322008013725281, 'total_loss': -0.0898054838180542}\n",
      "2021-09-07 17:18:50.909 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4922705292701721\n",
      "2021-09-07 17:18:50.910 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0155220031738281\n",
      "2021-09-07 17:18:50.911 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4922705292701721\n",
      "2021-09-07 17:18:50.912 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:50.913 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:50.914 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5059635639190674, 'baseline_loss': 0.8152123093605042, 'total_loss': -0.09835740923881531}\n",
      "2021-09-07 17:18:50.915 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.209433451294899\n",
      "2021-09-07 17:18:50.916 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9519329071044922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:50.917 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.209433451294899\n",
      "2021-09-07 17:18:50.918 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:50.920 | INFO     | src.policies:train:123 - Epoch 724 / 800\n",
      "2021-09-07 17:18:50.920 | INFO     | src.policies:collect_trajectories:221 - Episode 1569\n",
      "2021-09-07 17:18:50.927 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:50.928 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 32.0\n",
      "2021-09-07 17:18:50.928 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 32.0\n",
      "2021-09-07 17:18:50.929 | INFO     | src.policies:collect_trajectories:221 - Episode 1570\n",
      "2021-09-07 17:18:50.955 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:50.955 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 167.0\n",
      "2021-09-07 17:18:50.956 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 99.5\n",
      "2021-09-07 17:18:50.956 | INFO     | src.policies:collect_trajectories:221 - Episode 1571\n",
      "2021-09-07 17:18:50.989 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:50.990 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:50.990 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 133.0\n",
      "2021-09-07 17:18:50.991 | WARNING  | src.policies:train:144 - The actual batch size is 399, instead of 200\n",
      "2021-09-07 17:18:50.993 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:50.995 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4498233497142792, 'baseline_loss': 1.2104686498641968, 'total_loss': 0.1554109752178192}\n",
      "2021-09-07 17:18:50.996 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.27652743458747864\n",
      "2021-09-07 17:18:50.997 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4753241539001465\n",
      "2021-09-07 17:18:50.998 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.27652743458747864\n",
      "2021-09-07 17:18:50.999 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:51.001 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:51.002 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.536228597164154, 'baseline_loss': 1.4597512483596802, 'total_loss': 0.19364702701568604}\n",
      "2021-09-07 17:18:51.003 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.39780378341674805\n",
      "2021-09-07 17:18:51.004 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.680916428565979\n",
      "2021-09-07 17:18:51.005 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.39780378341674805\n",
      "2021-09-07 17:18:51.006 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:51.008 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:51.009 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5235496759414673, 'baseline_loss': 1.0874757766723633, 'total_loss': 0.020188212394714355}\n",
      "2021-09-07 17:18:51.010 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30343207716941833\n",
      "2021-09-07 17:18:51.010 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9625973105430603\n",
      "2021-09-07 17:18:51.011 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30343207716941833\n",
      "2021-09-07 17:18:51.012 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:51.014 | INFO     | src.policies:train:123 - Epoch 725 / 800\n",
      "2021-09-07 17:18:51.014 | INFO     | src.policies:collect_trajectories:221 - Episode 1572\n",
      "2021-09-07 17:18:51.044 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.044 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 179.0\n",
      "2021-09-07 17:18:51.045 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 179.0\n",
      "2021-09-07 17:18:51.046 | INFO     | src.policies:collect_trajectories:221 - Episode 1573\n",
      "2021-09-07 17:18:51.070 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.071 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 151.0\n",
      "2021-09-07 17:18:51.071 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 165.0\n",
      "2021-09-07 17:18:51.072 | WARNING  | src.policies:train:144 - The actual batch size is 330, instead of 200\n",
      "2021-09-07 17:18:51.075 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:51.077 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3452054560184479, 'baseline_loss': 0.5334414839744568, 'total_loss': -0.07848471403121948}\n",
      "2021-09-07 17:18:51.078 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6594453454017639\n",
      "2021-09-07 17:18:51.079 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4154475927352905\n",
      "2021-09-07 17:18:51.080 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:51.081 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4154475927352905\n",
      "2021-09-07 17:18:51.082 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:51.083 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.44087010622024536, 'baseline_loss': 0.7499848008155823, 'total_loss': -0.06587770581245422}\n",
      "2021-09-07 17:18:51.084 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24142314493656158\n",
      "2021-09-07 17:18:51.085 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7441815733909607\n",
      "2021-09-07 17:18:51.086 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24142314493656158\n",
      "2021-09-07 17:18:51.087 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:51.088 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:51.089 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5276206731796265, 'baseline_loss': 0.8939361572265625, 'total_loss': -0.08065259456634521}\n",
      "2021-09-07 17:18:51.090 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7751027345657349\n",
      "2021-09-07 17:18:51.091 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2463293075561523\n",
      "2021-09-07 17:18:51.092 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:51.094 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:51.095 | INFO     | src.policies:train:123 - Epoch 726 / 800\n",
      "2021-09-07 17:18:51.095 | INFO     | src.policies:collect_trajectories:221 - Episode 1574\n",
      "2021-09-07 17:18:51.124 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.125 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 155.0\n",
      "2021-09-07 17:18:51.125 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 155.0\n",
      "2021-09-07 17:18:51.126 | INFO     | src.policies:collect_trajectories:221 - Episode 1575\n",
      "2021-09-07 17:18:51.154 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:51.155 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 182.0\n",
      "2021-09-07 17:18:51.155 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 168.5\n",
      "2021-09-07 17:18:51.156 | WARNING  | src.policies:train:144 - The actual batch size is 337, instead of 200\n",
      "2021-09-07 17:18:51.159 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:51.161 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4157874584197998, 'baseline_loss': 0.5975469350814819, 'total_loss': -0.11701399087905884}\n",
      "2021-09-07 17:18:51.162 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31346216797828674\n",
      "2021-09-07 17:18:51.163 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4413022994995117\n",
      "2021-09-07 17:18:51.165 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31346216797828674\n",
      "2021-09-07 17:18:51.166 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4413022994995117\n",
      "2021-09-07 17:18:51.167 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:51.168 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43902119994163513, 'baseline_loss': 0.7083793878555298, 'total_loss': -0.08483150601387024}\n",
      "2021-09-07 17:18:51.169 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3076116442680359\n",
      "2021-09-07 17:18:51.171 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7406340837478638\n",
      "2021-09-07 17:18:51.172 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3076116442680359\n",
      "2021-09-07 17:18:51.173 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:51.174 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:51.175 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.49216777086257935, 'baseline_loss': 0.8406266570091248, 'total_loss': -0.07185444235801697}\n",
      "2021-09-07 17:18:51.176 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5862308740615845\n",
      "2021-09-07 17:18:51.177 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3151073455810547\n",
      "2021-09-07 17:18:51.178 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:51.179 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:51.181 | INFO     | src.policies:train:123 - Epoch 727 / 800\n",
      "2021-09-07 17:18:51.181 | INFO     | src.policies:collect_trajectories:221 - Episode 1576\n",
      "2021-09-07 17:18:51.205 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.205 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 153.0\n",
      "2021-09-07 17:18:51.206 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 153.0\n",
      "2021-09-07 17:18:51.206 | INFO     | src.policies:collect_trajectories:221 - Episode 1577\n",
      "2021-09-07 17:18:51.228 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.229 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 142.0\n",
      "2021-09-07 17:18:51.229 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 147.5\n",
      "2021-09-07 17:18:51.230 | WARNING  | src.policies:train:144 - The actual batch size is 295, instead of 200\n",
      "2021-09-07 17:18:51.232 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:51.234 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3822453022003174, 'baseline_loss': 0.5688450336456299, 'total_loss': -0.09782278537750244}\n",
      "2021-09-07 17:18:51.235 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3502276539802551\n",
      "2021-09-07 17:18:51.236 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.18952752649784088\n",
      "2021-09-07 17:18:51.237 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3502276539802551\n",
      "2021-09-07 17:18:51.238 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.18952752649784088\n",
      "2021-09-07 17:18:51.239 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:51.240 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.35178521275520325, 'baseline_loss': 0.5140456557273865, 'total_loss': -0.09476238489151001}\n",
      "2021-09-07 17:18:51.241 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1446284055709839\n",
      "2021-09-07 17:18:51.241 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.19900469481945038\n",
      "2021-09-07 17:18:51.242 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1446284055709839\n",
      "2021-09-07 17:18:51.244 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.19900469481945038\n",
      "2021-09-07 17:18:51.245 | INFO     | src.policies:train:123 - Epoch 728 / 800\n",
      "2021-09-07 17:18:51.246 | INFO     | src.policies:collect_trajectories:221 - Episode 1578\n",
      "2021-09-07 17:18:51.255 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.256 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 61.0\n",
      "2021-09-07 17:18:51.256 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 61.0\n",
      "2021-09-07 17:18:51.257 | INFO     | src.policies:collect_trajectories:221 - Episode 1579\n",
      "2021-09-07 17:18:51.340 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.340 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 172.0\n",
      "2021-09-07 17:18:51.341 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 116.5\n",
      "2021-09-07 17:18:51.343 | WARNING  | src.policies:train:144 - The actual batch size is 233, instead of 200\n",
      "2021-09-07 17:18:51.347 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:51.350 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5835064649581909, 'baseline_loss': 1.570205569267273, 'total_loss': 0.20159631967544556}\n",
      "2021-09-07 17:18:51.351 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22273103892803192\n",
      "2021-09-07 17:18:51.352 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8733962774276733\n",
      "2021-09-07 17:18:51.353 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22273103892803192\n",
      "2021-09-07 17:18:51.355 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:51.356 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:51.357 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5369107723236084, 'baseline_loss': 1.5546886920928955, 'total_loss': 0.24043357372283936}\n",
      "2021-09-07 17:18:51.358 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.258996844291687\n",
      "2021-09-07 17:18:51.358 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8062504529953003\n",
      "2021-09-07 17:18:51.359 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.258996844291687\n",
      "2021-09-07 17:18:51.361 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:51.362 | INFO     | src.policies:train:123 - Epoch 729 / 800\n",
      "2021-09-07 17:18:51.363 | INFO     | src.policies:collect_trajectories:221 - Episode 1580\n",
      "2021-09-07 17:18:51.383 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:51.383 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 128.0\n",
      "2021-09-07 17:18:51.384 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 128.0\n",
      "2021-09-07 17:18:51.384 | INFO     | src.policies:collect_trajectories:221 - Episode 1581\n",
      "2021-09-07 17:18:51.402 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.403 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 113.0\n",
      "2021-09-07 17:18:51.403 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 120.5\n",
      "2021-09-07 17:18:51.404 | WARNING  | src.policies:train:144 - The actual batch size is 241, instead of 200\n",
      "2021-09-07 17:18:51.406 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:51.410 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4991490840911865, 'baseline_loss': 0.7263426780700684, 'total_loss': -0.13597774505615234}\n",
      "2021-09-07 17:18:51.411 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.294089138507843\n",
      "2021-09-07 17:18:51.412 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.39199984073638916\n",
      "2021-09-07 17:18:51.413 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.294089138507843\n",
      "2021-09-07 17:18:51.414 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.39199984073638916\n",
      "2021-09-07 17:18:51.415 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:51.416 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42245060205459595, 'baseline_loss': 0.650026261806488, 'total_loss': -0.09743747115135193}\n",
      "2021-09-07 17:18:51.417 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7476402521133423\n",
      "2021-09-07 17:18:51.418 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.35992351174354553\n",
      "2021-09-07 17:18:51.419 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:51.420 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.35992351174354553\n",
      "2021-09-07 17:18:51.421 | INFO     | src.policies:train:123 - Epoch 730 / 800\n",
      "2021-09-07 17:18:51.422 | INFO     | src.policies:collect_trajectories:221 - Episode 1582\n",
      "2021-09-07 17:18:51.445 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.446 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 138.0\n",
      "2021-09-07 17:18:51.447 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 138.0\n",
      "2021-09-07 17:18:51.447 | INFO     | src.policies:collect_trajectories:221 - Episode 1583\n",
      "2021-09-07 17:18:51.476 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.477 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 168.0\n",
      "2021-09-07 17:18:51.477 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 153.0\n",
      "2021-09-07 17:18:51.478 | WARNING  | src.policies:train:144 - The actual batch size is 306, instead of 200\n",
      "2021-09-07 17:18:51.480 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:51.483 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3610689640045166, 'baseline_loss': 0.45566558837890625, 'total_loss': -0.13323616981506348}\n",
      "2021-09-07 17:18:51.484 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.12600290775299072\n",
      "2021-09-07 17:18:51.484 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.30942246317863464\n",
      "2021-09-07 17:18:51.485 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.12600290775299072\n",
      "2021-09-07 17:18:51.486 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.30942246317863464\n",
      "2021-09-07 17:18:51.488 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:51.489 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.22094492614269257, 'baseline_loss': 0.37486639618873596, 'total_loss': -0.033511728048324585}\n",
      "2021-09-07 17:18:51.490 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13903194665908813\n",
      "2021-09-07 17:18:51.490 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1660187244415283\n",
      "2021-09-07 17:18:51.491 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13903194665908813\n",
      "2021-09-07 17:18:51.492 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:51.493 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:51.495 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3274569809436798, 'baseline_loss': 0.43884900212287903, 'total_loss': -0.1080324798822403}\n",
      "2021-09-07 17:18:51.495 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20784734189510345\n",
      "2021-09-07 17:18:51.496 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0778533220291138\n",
      "2021-09-07 17:18:51.497 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20784734189510345\n",
      "2021-09-07 17:18:51.498 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:51.499 | INFO     | src.policies:train:123 - Epoch 731 / 800\n",
      "2021-09-07 17:18:51.500 | INFO     | src.policies:collect_trajectories:221 - Episode 1584\n",
      "2021-09-07 17:18:51.519 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.519 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 129.0\n",
      "2021-09-07 17:18:51.520 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 129.0\n",
      "2021-09-07 17:18:51.520 | INFO     | src.policies:collect_trajectories:221 - Episode 1585\n",
      "2021-09-07 17:18:51.538 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.539 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 115.0\n",
      "2021-09-07 17:18:51.539 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 122.0\n",
      "2021-09-07 17:18:51.540 | WARNING  | src.policies:train:144 - The actual batch size is 244, instead of 200\n",
      "2021-09-07 17:18:51.543 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:51.545 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.259581595659256, 'baseline_loss': 0.4413338899612427, 'total_loss': -0.038914650678634644}\n",
      "2021-09-07 17:18:51.546 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4593653082847595\n",
      "2021-09-07 17:18:51.547 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0914500951766968\n",
      "2021-09-07 17:18:51.548 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4593653082847595\n",
      "2021-09-07 17:18:51.549 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:51.550 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:51.551 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.419158399105072, 'baseline_loss': 0.4841586947441101, 'total_loss': -0.17707905173301697}\n",
      "2021-09-07 17:18:51.552 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6402816772460938\n",
      "2021-09-07 17:18:51.553 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5463505387306213\n",
      "2021-09-07 17:18:51.554 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999916553497314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:51.555 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:51.556 | INFO     | src.policies:train:123 - Epoch 732 / 800\n",
      "2021-09-07 17:18:51.557 | INFO     | src.policies:collect_trajectories:221 - Episode 1586\n",
      "2021-09-07 17:18:51.574 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.574 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 121.0\n",
      "2021-09-07 17:18:51.575 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 121.0\n",
      "2021-09-07 17:18:51.575 | INFO     | src.policies:collect_trajectories:221 - Episode 1587\n",
      "2021-09-07 17:18:51.599 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.600 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 155.0\n",
      "2021-09-07 17:18:51.600 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 138.0\n",
      "2021-09-07 17:18:51.601 | WARNING  | src.policies:train:144 - The actual batch size is 276, instead of 200\n",
      "2021-09-07 17:18:51.604 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:51.606 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2498771846294403, 'baseline_loss': 0.3742256462574005, 'total_loss': -0.06276436150074005}\n",
      "2021-09-07 17:18:51.607 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2490621954202652\n",
      "2021-09-07 17:18:51.608 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8280739784240723\n",
      "2021-09-07 17:18:51.610 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2490621954202652\n",
      "2021-09-07 17:18:51.612 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:51.613 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:51.615 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.25648510456085205, 'baseline_loss': 0.3810489773750305, 'total_loss': -0.06596061587333679}\n",
      "2021-09-07 17:18:51.617 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07635016739368439\n",
      "2021-09-07 17:18:51.618 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9307640194892883\n",
      "2021-09-07 17:18:51.619 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07635016739368439\n",
      "2021-09-07 17:18:51.620 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:51.621 | INFO     | src.policies:train:123 - Epoch 733 / 800\n",
      "2021-09-07 17:18:51.622 | INFO     | src.policies:collect_trajectories:221 - Episode 1588\n",
      "2021-09-07 17:18:51.645 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.645 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 164.0\n",
      "2021-09-07 17:18:51.646 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 164.0\n",
      "2021-09-07 17:18:51.646 | INFO     | src.policies:collect_trajectories:221 - Episode 1589\n",
      "2021-09-07 17:18:51.662 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.662 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 92.0\n",
      "2021-09-07 17:18:51.662 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 128.0\n",
      "2021-09-07 17:18:51.663 | WARNING  | src.policies:train:144 - The actual batch size is 256, instead of 200\n",
      "2021-09-07 17:18:51.667 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:51.669 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4447440803050995, 'baseline_loss': 0.7932180762290955, 'total_loss': -0.04813504219055176}\n",
      "2021-09-07 17:18:51.669 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6406574249267578\n",
      "2021-09-07 17:18:51.670 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.26802951097488403\n",
      "2021-09-07 17:18:51.671 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:51.672 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.26802951097488403\n",
      "2021-09-07 17:18:51.673 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:51.674 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.566440761089325, 'baseline_loss': 1.2149345874786377, 'total_loss': 0.041026532649993896}\n",
      "2021-09-07 17:18:51.675 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.45122671127319336\n",
      "2021-09-07 17:18:51.676 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5029431581497192\n",
      "2021-09-07 17:18:51.677 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.45122671127319336\n",
      "2021-09-07 17:18:51.678 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:51.679 | INFO     | src.policies:train:123 - Epoch 734 / 800\n",
      "2021-09-07 17:18:51.680 | INFO     | src.policies:collect_trajectories:221 - Episode 1590\n",
      "2021-09-07 17:18:51.704 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.705 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 161.0\n",
      "2021-09-07 17:18:51.705 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 161.0\n",
      "2021-09-07 17:18:51.705 | INFO     | src.policies:collect_trajectories:221 - Episode 1591\n",
      "2021-09-07 17:18:51.726 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.727 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 130.0\n",
      "2021-09-07 17:18:51.727 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 145.5\n",
      "2021-09-07 17:18:51.728 | WARNING  | src.policies:train:144 - The actual batch size is 291, instead of 200\n",
      "2021-09-07 17:18:51.731 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:51.733 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31258299946784973, 'baseline_loss': 0.38886529207229614, 'total_loss': -0.11815035343170166}\n",
      "2021-09-07 17:18:51.734 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.40525364875793457\n",
      "2021-09-07 17:18:51.735 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9814051985740662\n",
      "2021-09-07 17:18:51.736 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.40525364875793457\n",
      "2021-09-07 17:18:51.737 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:51.738 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:51.739 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3445250689983368, 'baseline_loss': 0.48936593532562256, 'total_loss': -0.09984210133552551}\n",
      "2021-09-07 17:18:51.740 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4561189115047455\n",
      "2021-09-07 17:18:51.741 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.36781755089759827\n",
      "2021-09-07 17:18:51.742 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4561189115047455\n",
      "2021-09-07 17:18:51.743 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.36781755089759827\n",
      "2021-09-07 17:18:51.745 | INFO     | src.policies:train:123 - Epoch 735 / 800\n",
      "2021-09-07 17:18:51.745 | INFO     | src.policies:collect_trajectories:221 - Episode 1592\n",
      "2021-09-07 17:18:51.771 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:51.772 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 183.0\n",
      "2021-09-07 17:18:51.772 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 183.0\n",
      "2021-09-07 17:18:51.773 | INFO     | src.policies:collect_trajectories:221 - Episode 1593\n",
      "2021-09-07 17:18:51.799 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.800 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 176.0\n",
      "2021-09-07 17:18:51.800 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 179.5\n",
      "2021-09-07 17:18:51.801 | WARNING  | src.policies:train:144 - The actual batch size is 359, instead of 200\n",
      "2021-09-07 17:18:51.804 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:51.806 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3652666211128235, 'baseline_loss': 0.49952232837677, 'total_loss': -0.11550545692443848}\n",
      "2021-09-07 17:18:51.807 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3471514880657196\n",
      "2021-09-07 17:18:51.808 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.25216394662857056\n",
      "2021-09-07 17:18:51.809 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3471514880657196\n",
      "2021-09-07 17:18:51.810 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.25216394662857056\n",
      "2021-09-07 17:18:51.811 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:51.812 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4206431210041046, 'baseline_loss': 0.5415163636207581, 'total_loss': -0.14988493919372559}\n",
      "2021-09-07 17:18:51.813 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.36121734976768494\n",
      "2021-09-07 17:18:51.814 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2104923129081726\n",
      "2021-09-07 17:18:51.815 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.36121734976768494\n",
      "2021-09-07 17:18:51.816 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2104923129081726\n",
      "2021-09-07 17:18:51.818 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:51.819 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3346278667449951, 'baseline_loss': 0.4181828796863556, 'total_loss': -0.12553642690181732}\n",
      "2021-09-07 17:18:51.820 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21596507728099823\n",
      "2021-09-07 17:18:51.820 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.456459641456604\n",
      "2021-09-07 17:18:51.821 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21596507728099823\n",
      "2021-09-07 17:18:51.822 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.456459641456604\n",
      "2021-09-07 17:18:51.824 | INFO     | src.policies:train:123 - Epoch 736 / 800\n",
      "2021-09-07 17:18:51.824 | INFO     | src.policies:collect_trajectories:221 - Episode 1594\n",
      "2021-09-07 17:18:51.995 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:51.996 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 149.0\n",
      "2021-09-07 17:18:51.997 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 149.0\n",
      "2021-09-07 17:18:51.997 | INFO     | src.policies:collect_trajectories:221 - Episode 1595\n",
      "2021-09-07 17:18:52.020 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.021 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 121.0\n",
      "2021-09-07 17:18:52.021 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 135.0\n",
      "2021-09-07 17:18:52.022 | WARNING  | src.policies:train:144 - The actual batch size is 270, instead of 200\n",
      "2021-09-07 17:18:52.024 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:52.026 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4969697892665863, 'baseline_loss': 0.7925997376441956, 'total_loss': -0.10066992044448853}\n",
      "2021-09-07 17:18:52.027 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19392941892147064\n",
      "2021-09-07 17:18:52.028 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9572291374206543\n",
      "2021-09-07 17:18:52.029 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19392941892147064\n",
      "2021-09-07 17:18:52.030 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:52.031 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:52.032 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5078957080841064, 'baseline_loss': 0.7414910793304443, 'total_loss': -0.13715016841888428}\n",
      "2021-09-07 17:18:52.033 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18577910959720612\n",
      "2021-09-07 17:18:52.034 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6518203616142273\n",
      "2021-09-07 17:18:52.035 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18577910959720612\n",
      "2021-09-07 17:18:52.036 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:52.037 | INFO     | src.policies:train:123 - Epoch 737 / 800\n",
      "2021-09-07 17:18:52.038 | INFO     | src.policies:collect_trajectories:221 - Episode 1596\n",
      "2021-09-07 17:18:52.062 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.063 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 174.0\n",
      "2021-09-07 17:18:52.064 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 174.0\n",
      "2021-09-07 17:18:52.064 | INFO     | src.policies:collect_trajectories:221 - Episode 1597\n",
      "2021-09-07 17:18:52.090 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.091 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 173.0\n",
      "2021-09-07 17:18:52.091 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 173.5\n",
      "2021-09-07 17:18:52.092 | WARNING  | src.policies:train:144 - The actual batch size is 347, instead of 200\n",
      "2021-09-07 17:18:52.095 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:52.097 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3048064708709717, 'baseline_loss': 0.38752567768096924, 'total_loss': -0.11104363203048706}\n",
      "2021-09-07 17:18:52.098 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.436993271112442\n",
      "2021-09-07 17:18:52.100 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.31844696402549744\n",
      "2021-09-07 17:18:52.102 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.436993271112442\n",
      "2021-09-07 17:18:52.103 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.31844696402549744\n",
      "2021-09-07 17:18:52.105 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:52.106 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.45869243144989014, 'baseline_loss': 0.5347316265106201, 'total_loss': -0.19132661819458008}\n",
      "2021-09-07 17:18:52.107 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.260756254196167\n",
      "2021-09-07 17:18:52.108 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.12641166150569916\n",
      "2021-09-07 17:18:52.109 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.260756254196167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:52.110 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.12641166150569916\n",
      "2021-09-07 17:18:52.111 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:52.112 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4592663049697876, 'baseline_loss': 0.6244103312492371, 'total_loss': -0.14706113934516907}\n",
      "2021-09-07 17:18:52.113 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19572122395038605\n",
      "2021-09-07 17:18:52.114 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.277847021818161\n",
      "2021-09-07 17:18:52.115 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19572122395038605\n",
      "2021-09-07 17:18:52.116 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.277847021818161\n",
      "2021-09-07 17:18:52.118 | INFO     | src.policies:train:123 - Epoch 738 / 800\n",
      "2021-09-07 17:18:52.118 | INFO     | src.policies:collect_trajectories:221 - Episode 1598\n",
      "2021-09-07 17:18:52.145 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.146 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 190.0\n",
      "2021-09-07 17:18:52.146 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 190.0\n",
      "2021-09-07 17:18:52.146 | INFO     | src.policies:collect_trajectories:221 - Episode 1599\n",
      "2021-09-07 17:18:52.173 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.174 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 178.0\n",
      "2021-09-07 17:18:52.174 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 184.0\n",
      "2021-09-07 17:18:52.175 | WARNING  | src.policies:train:144 - The actual batch size is 368, instead of 200\n",
      "2021-09-07 17:18:52.179 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:52.181 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5204145908355713, 'baseline_loss': 0.9153565168380737, 'total_loss': -0.06273633241653442}\n",
      "2021-09-07 17:18:52.182 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19369147717952728\n",
      "2021-09-07 17:18:52.182 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.256079077720642\n",
      "2021-09-07 17:18:52.184 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19369147717952728\n",
      "2021-09-07 17:18:52.185 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:52.186 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:52.187 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.439835786819458, 'baseline_loss': 0.6045722365379333, 'total_loss': -0.13754966855049133}\n",
      "2021-09-07 17:18:52.188 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2272261530160904\n",
      "2021-09-07 17:18:52.188 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.39581263065338135\n",
      "2021-09-07 17:18:52.189 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2272261530160904\n",
      "2021-09-07 17:18:52.190 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.39581263065338135\n",
      "2021-09-07 17:18:52.192 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:52.193 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2789561152458191, 'baseline_loss': 0.4229440987110138, 'total_loss': -0.0674840658903122}\n",
      "2021-09-07 17:18:52.194 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10687002539634705\n",
      "2021-09-07 17:18:52.195 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5440270900726318\n",
      "2021-09-07 17:18:52.196 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10687002539634705\n",
      "2021-09-07 17:18:52.197 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:52.198 | INFO     | src.policies:train:123 - Epoch 739 / 800\n",
      "2021-09-07 17:18:52.199 | INFO     | src.policies:collect_trajectories:221 - Episode 1600\n",
      "2021-09-07 17:18:52.227 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.228 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:52.228 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:52.230 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:52.232 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.38107830286026, 'baseline_loss': 0.5787470936775208, 'total_loss': -0.09170475602149963}\n",
      "2021-09-07 17:18:52.233 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3462514877319336\n",
      "2021-09-07 17:18:52.234 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.48478174209594727\n",
      "2021-09-07 17:18:52.236 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3462514877319336\n",
      "2021-09-07 17:18:52.237 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.48478174209594727\n",
      "2021-09-07 17:18:52.239 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:52.240 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4160688817501068, 'baseline_loss': 0.5720163583755493, 'total_loss': -0.13006070256233215}\n",
      "2021-09-07 17:18:52.241 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.34544214606285095\n",
      "2021-09-07 17:18:52.242 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5267704725265503\n",
      "2021-09-07 17:18:52.243 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.34544214606285095\n",
      "2021-09-07 17:18:52.244 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990463256836\n",
      "2021-09-07 17:18:52.246 | INFO     | src.policies:train:123 - Epoch 740 / 800\n",
      "2021-09-07 17:18:52.246 | INFO     | src.policies:collect_trajectories:221 - Episode 1601\n",
      "2021-09-07 17:18:52.276 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.276 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:52.277 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:52.279 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:52.281 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3826395273208618, 'baseline_loss': 0.5379735231399536, 'total_loss': -0.11365276575088501}\n",
      "2021-09-07 17:18:52.282 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1444508582353592\n",
      "2021-09-07 17:18:52.283 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2889287769794464\n",
      "2021-09-07 17:18:52.284 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1444508582353592\n",
      "2021-09-07 17:18:52.285 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2889287769794464\n",
      "2021-09-07 17:18:52.287 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:52.288 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4673682153224945, 'baseline_loss': 0.7665061950683594, 'total_loss': -0.08411511778831482}\n",
      "2021-09-07 17:18:52.289 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.23601418733596802\n",
      "2021-09-07 17:18:52.289 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9051110148429871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:52.290 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.23601418733596802\n",
      "2021-09-07 17:18:52.292 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:52.293 | INFO     | src.policies:train:123 - Epoch 741 / 800\n",
      "2021-09-07 17:18:52.294 | INFO     | src.policies:collect_trajectories:221 - Episode 1602\n",
      "2021-09-07 17:18:52.320 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.321 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 158.0\n",
      "2021-09-07 17:18:52.322 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 158.0\n",
      "2021-09-07 17:18:52.322 | INFO     | src.policies:collect_trajectories:221 - Episode 1603\n",
      "2021-09-07 17:18:52.348 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.348 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 164.0\n",
      "2021-09-07 17:18:52.349 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 161.0\n",
      "2021-09-07 17:18:52.349 | WARNING  | src.policies:train:144 - The actual batch size is 322, instead of 200\n",
      "2021-09-07 17:18:52.352 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:52.354 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2580479383468628, 'baseline_loss': 0.41818565130233765, 'total_loss': -0.04895511269569397}\n",
      "2021-09-07 17:18:52.355 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16878867149353027\n",
      "2021-09-07 17:18:52.356 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4222797751426697\n",
      "2021-09-07 17:18:52.357 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16878867149353027\n",
      "2021-09-07 17:18:52.358 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4222797751426697\n",
      "2021-09-07 17:18:52.359 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:52.360 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32483169436454773, 'baseline_loss': 0.4054206907749176, 'total_loss': -0.12212134897708893}\n",
      "2021-09-07 17:18:52.361 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16599224507808685\n",
      "2021-09-07 17:18:52.362 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.639060914516449\n",
      "2021-09-07 17:18:52.363 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16599224507808685\n",
      "2021-09-07 17:18:52.364 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:52.365 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:52.366 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.335295706987381, 'baseline_loss': 0.4337868392467499, 'total_loss': -0.11840228736400604}\n",
      "2021-09-07 17:18:52.367 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5483962893486023\n",
      "2021-09-07 17:18:52.368 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5786616802215576\n",
      "2021-09-07 17:18:52.369 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:52.370 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999916553497314\n",
      "2021-09-07 17:18:52.371 | INFO     | src.policies:train:123 - Epoch 742 / 800\n",
      "2021-09-07 17:18:52.372 | INFO     | src.policies:collect_trajectories:221 - Episode 1604\n",
      "2021-09-07 17:18:52.400 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.401 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 184.0\n",
      "2021-09-07 17:18:52.401 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 184.0\n",
      "2021-09-07 17:18:52.402 | INFO     | src.policies:collect_trajectories:221 - Episode 1605\n",
      "2021-09-07 17:18:52.429 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.429 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 179.0\n",
      "2021-09-07 17:18:52.430 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 181.5\n",
      "2021-09-07 17:18:52.430 | WARNING  | src.policies:train:144 - The actual batch size is 363, instead of 200\n",
      "2021-09-07 17:18:52.433 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:52.435 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4930843412876129, 'baseline_loss': 0.7008609175682068, 'total_loss': -0.14265388250350952}\n",
      "2021-09-07 17:18:52.436 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.36954888701438904\n",
      "2021-09-07 17:18:52.436 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5822632312774658\n",
      "2021-09-07 17:18:52.438 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.36954888701438904\n",
      "2021-09-07 17:18:52.439 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:52.440 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:52.441 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3282519280910492, 'baseline_loss': 0.4882926940917969, 'total_loss': -0.08410558104515076}\n",
      "2021-09-07 17:18:52.442 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.33386221528053284\n",
      "2021-09-07 17:18:52.443 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.14251656830310822\n",
      "2021-09-07 17:18:52.444 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.33386221528053284\n",
      "2021-09-07 17:18:52.446 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.14251656830310822\n",
      "2021-09-07 17:18:52.447 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:52.449 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41112077236175537, 'baseline_loss': 0.6308639049530029, 'total_loss': -0.0956888198852539}\n",
      "2021-09-07 17:18:52.450 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6738778948783875\n",
      "2021-09-07 17:18:52.451 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.45625585317611694\n",
      "2021-09-07 17:18:52.452 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:52.453 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.45625585317611694\n",
      "2021-09-07 17:18:52.454 | INFO     | src.policies:train:123 - Epoch 743 / 800\n",
      "2021-09-07 17:18:52.455 | INFO     | src.policies:collect_trajectories:221 - Episode 1606\n",
      "2021-09-07 17:18:52.485 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.498 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:52.538 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:52.541 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:52.542 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.47493264079093933, 'baseline_loss': 0.6825577020645142, 'total_loss': -0.13365378975868225}\n",
      "2021-09-07 17:18:52.544 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16264183819293976\n",
      "2021-09-07 17:18:52.545 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6600902676582336\n",
      "2021-09-07 17:18:52.546 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16264183819293976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:52.547 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:52.548 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:52.549 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3400154113769531, 'baseline_loss': 0.5419145822525024, 'total_loss': -0.0690581202507019}\n",
      "2021-09-07 17:18:52.550 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22335396707057953\n",
      "2021-09-07 17:18:52.551 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3963835537433624\n",
      "2021-09-07 17:18:52.552 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22335396707057953\n",
      "2021-09-07 17:18:52.553 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3963835537433624\n",
      "2021-09-07 17:18:52.555 | INFO     | src.policies:train:123 - Epoch 744 / 800\n",
      "2021-09-07 17:18:52.555 | INFO     | src.policies:collect_trajectories:221 - Episode 1607\n",
      "2021-09-07 17:18:52.584 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.585 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:52.585 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:52.587 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:52.589 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4774571359157562, 'baseline_loss': 0.8048014640808105, 'total_loss': -0.07505640387535095}\n",
      "2021-09-07 17:18:52.590 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.34436899423599243\n",
      "2021-09-07 17:18:52.591 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9055006504058838\n",
      "2021-09-07 17:18:52.592 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.34436899423599243\n",
      "2021-09-07 17:18:52.594 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:52.595 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:52.596 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4436695873737335, 'baseline_loss': 0.8322576880455017, 'total_loss': -0.027540743350982666}\n",
      "2021-09-07 17:18:52.597 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2693368196487427\n",
      "2021-09-07 17:18:52.598 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.21592116355896\n",
      "2021-09-07 17:18:52.598 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2693368196487427\n",
      "2021-09-07 17:18:52.600 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:52.601 | INFO     | src.policies:train:123 - Epoch 745 / 800\n",
      "2021-09-07 17:18:52.602 | INFO     | src.policies:collect_trajectories:221 - Episode 1608\n",
      "2021-09-07 17:18:52.626 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.627 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 176.0\n",
      "2021-09-07 17:18:52.627 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 176.0\n",
      "2021-09-07 17:18:52.628 | INFO     | src.policies:collect_trajectories:221 - Episode 1609\n",
      "2021-09-07 17:18:52.660 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.660 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:52.661 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 188.0\n",
      "2021-09-07 17:18:52.661 | WARNING  | src.policies:train:144 - The actual batch size is 376, instead of 200\n",
      "2021-09-07 17:18:52.664 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:52.666 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4646800756454468, 'baseline_loss': 0.6474089026451111, 'total_loss': -0.14097562432289124}\n",
      "2021-09-07 17:18:52.667 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4817562699317932\n",
      "2021-09-07 17:18:52.668 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.416963666677475\n",
      "2021-09-07 17:18:52.669 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4817562699317932\n",
      "2021-09-07 17:18:52.670 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.416963666677475\n",
      "2021-09-07 17:18:52.671 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:52.673 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.39766204357147217, 'baseline_loss': 0.7689415216445923, 'total_loss': -0.013191282749176025}\n",
      "2021-09-07 17:18:52.673 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22987128794193268\n",
      "2021-09-07 17:18:52.674 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9474122524261475\n",
      "2021-09-07 17:18:52.675 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22987128794193268\n",
      "2021-09-07 17:18:52.676 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999994933605194\n",
      "2021-09-07 17:18:52.677 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:52.678 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4329306483268738, 'baseline_loss': 0.661140501499176, 'total_loss': -0.10236039757728577}\n",
      "2021-09-07 17:18:52.679 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16519050300121307\n",
      "2021-09-07 17:18:52.680 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5767141580581665\n",
      "2021-09-07 17:18:52.681 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16519050300121307\n",
      "2021-09-07 17:18:52.682 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:52.683 | INFO     | src.policies:train:123 - Epoch 746 / 800\n",
      "2021-09-07 17:18:52.684 | INFO     | src.policies:collect_trajectories:221 - Episode 1610\n",
      "2021-09-07 17:18:52.710 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.710 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 183.0\n",
      "2021-09-07 17:18:52.711 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 183.0\n",
      "2021-09-07 17:18:52.711 | INFO     | src.policies:collect_trajectories:221 - Episode 1611\n",
      "2021-09-07 17:18:52.740 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.740 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:52.741 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 191.5\n",
      "2021-09-07 17:18:52.742 | WARNING  | src.policies:train:144 - The actual batch size is 383, instead of 200\n",
      "2021-09-07 17:18:52.744 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:52.747 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.594842255115509, 'baseline_loss': 1.0157990455627441, 'total_loss': -0.08694273233413696}\n",
      "2021-09-07 17:18:52.747 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5876465439796448\n",
      "2021-09-07 17:18:52.748 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1444947719573975\n",
      "2021-09-07 17:18:52.749 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:52.750 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:52.751 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:52.752 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3809751868247986, 'baseline_loss': 0.5614993572235107, 'total_loss': -0.10022550821304321}\n",
      "2021-09-07 17:18:52.754 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3881306052207947\n",
      "2021-09-07 17:18:52.754 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4239238202571869\n",
      "2021-09-07 17:18:52.755 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3881306052207947\n",
      "2021-09-07 17:18:52.756 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4239238202571869\n",
      "2021-09-07 17:18:52.758 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:52.759 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.41040074825286865, 'baseline_loss': 0.6926722526550293, 'total_loss': -0.064064621925354}\n",
      "2021-09-07 17:18:52.760 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24039679765701294\n",
      "2021-09-07 17:18:52.760 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8563861846923828\n",
      "2021-09-07 17:18:52.762 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24039679765701294\n",
      "2021-09-07 17:18:52.762 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:52.764 | INFO     | src.policies:train:123 - Epoch 747 / 800\n",
      "2021-09-07 17:18:52.764 | INFO     | src.policies:collect_trajectories:221 - Episode 1612\n",
      "2021-09-07 17:18:52.784 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.785 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 140.0\n",
      "2021-09-07 17:18:52.785 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 140.0\n",
      "2021-09-07 17:18:52.786 | INFO     | src.policies:collect_trajectories:221 - Episode 1613\n",
      "2021-09-07 17:18:52.815 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.815 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 176.0\n",
      "2021-09-07 17:18:52.816 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 158.0\n",
      "2021-09-07 17:18:52.816 | WARNING  | src.policies:train:144 - The actual batch size is 316, instead of 200\n",
      "2021-09-07 17:18:52.819 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:52.822 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.22672604024410248, 'baseline_loss': 0.3794551193714142, 'total_loss': -0.036998480558395386}\n",
      "2021-09-07 17:18:52.822 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1931038647890091\n",
      "2021-09-07 17:18:52.823 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.33503258228302\n",
      "2021-09-07 17:18:52.825 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1931038647890091\n",
      "2021-09-07 17:18:52.826 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:52.827 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:52.828 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27452558279037476, 'baseline_loss': 0.4198995530605316, 'total_loss': -0.06457580626010895}\n",
      "2021-09-07 17:18:52.829 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5975725054740906\n",
      "2021-09-07 17:18:52.829 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6972277760505676\n",
      "2021-09-07 17:18:52.830 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:52.831 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992847442627\n",
      "2021-09-07 17:18:52.833 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:52.834 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36490511894226074, 'baseline_loss': 0.4699111878871918, 'total_loss': -0.12994952499866486}\n",
      "2021-09-07 17:18:52.835 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4853592813014984\n",
      "2021-09-07 17:18:52.836 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8184680938720703\n",
      "2021-09-07 17:18:52.837 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4853592813014984\n",
      "2021-09-07 17:18:52.838 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:52.839 | INFO     | src.policies:train:123 - Epoch 748 / 800\n",
      "2021-09-07 17:18:52.840 | INFO     | src.policies:collect_trajectories:221 - Episode 1614\n",
      "2021-09-07 17:18:52.868 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.868 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 194.0\n",
      "2021-09-07 17:18:52.870 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 194.0\n",
      "2021-09-07 17:18:52.870 | INFO     | src.policies:collect_trajectories:221 - Episode 1615\n",
      "2021-09-07 17:18:52.897 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.898 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 177.0\n",
      "2021-09-07 17:18:52.898 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 185.5\n",
      "2021-09-07 17:18:52.899 | WARNING  | src.policies:train:144 - The actual batch size is 371, instead of 200\n",
      "2021-09-07 17:18:52.902 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:52.904 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42843613028526306, 'baseline_loss': 0.6012527942657471, 'total_loss': -0.12780973315238953}\n",
      "2021-09-07 17:18:52.905 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19886745512485504\n",
      "2021-09-07 17:18:52.906 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.29541894793510437\n",
      "2021-09-07 17:18:52.908 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19886745512485504\n",
      "2021-09-07 17:18:52.910 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.29541894793510437\n",
      "2021-09-07 17:18:52.911 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:52.913 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31589943170547485, 'baseline_loss': 0.4503065347671509, 'total_loss': -0.09074616432189941}\n",
      "2021-09-07 17:18:52.913 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19622685015201569\n",
      "2021-09-07 17:18:52.914 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.18826662003993988\n",
      "2021-09-07 17:18:52.915 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19622685015201569\n",
      "2021-09-07 17:18:52.916 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.18826662003993988\n",
      "2021-09-07 17:18:52.917 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:52.918 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3884517252445221, 'baseline_loss': 0.5049602389335632, 'total_loss': -0.13597160577774048}\n",
      "2021-09-07 17:18:52.919 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6716930866241455\n",
      "2021-09-07 17:18:52.920 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2720468044281006\n",
      "2021-09-07 17:18:52.921 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992549419403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:52.922 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2720468044281006\n",
      "2021-09-07 17:18:52.924 | INFO     | src.policies:train:123 - Epoch 749 / 800\n",
      "2021-09-07 17:18:52.924 | INFO     | src.policies:collect_trajectories:221 - Episode 1616\n",
      "2021-09-07 17:18:52.956 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.957 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 182.0\n",
      "2021-09-07 17:18:52.957 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 182.0\n",
      "2021-09-07 17:18:52.958 | INFO     | src.policies:collect_trajectories:221 - Episode 1617\n",
      "2021-09-07 17:18:52.988 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:52.989 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 196.0\n",
      "2021-09-07 17:18:52.990 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 189.0\n",
      "2021-09-07 17:18:52.990 | WARNING  | src.policies:train:144 - The actual batch size is 378, instead of 200\n",
      "2021-09-07 17:18:52.993 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:52.995 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2542995810508728, 'baseline_loss': 0.34849080443382263, 'total_loss': -0.08005417883396149}\n",
      "2021-09-07 17:18:52.996 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3449860215187073\n",
      "2021-09-07 17:18:52.997 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0380275249481201\n",
      "2021-09-07 17:18:52.998 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3449860215187073\n",
      "2021-09-07 17:18:52.999 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:53.000 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:53.002 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2809745669364929, 'baseline_loss': 0.387807697057724, 'total_loss': -0.08707071840763092}\n",
      "2021-09-07 17:18:53.003 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.07597552984952927\n",
      "2021-09-07 17:18:53.004 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4928419888019562\n",
      "2021-09-07 17:18:53.005 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.07597552984952927\n",
      "2021-09-07 17:18:53.007 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4928419888019562\n",
      "2021-09-07 17:18:53.008 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:53.009 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31849417090415955, 'baseline_loss': 0.4484589099884033, 'total_loss': -0.09426471590995789}\n",
      "2021-09-07 17:18:53.010 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2968593239784241\n",
      "2021-09-07 17:18:53.011 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4189172387123108\n",
      "2021-09-07 17:18:53.012 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2968593239784241\n",
      "2021-09-07 17:18:53.013 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4189172387123108\n",
      "2021-09-07 17:18:53.015 | INFO     | src.policies:train:123 - Epoch 750 / 800\n",
      "2021-09-07 17:18:53.015 | INFO     | src.policies:collect_trajectories:221 - Episode 1618\n",
      "2021-09-07 17:18:53.185 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:53.186 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 174.0\n",
      "2021-09-07 17:18:53.187 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 174.0\n",
      "2021-09-07 17:18:53.187 | INFO     | src.policies:collect_trajectories:221 - Episode 1619\n",
      "2021-09-07 17:18:53.206 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:53.206 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 105.0\n",
      "2021-09-07 17:18:53.207 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 139.5\n",
      "2021-09-07 17:18:53.208 | WARNING  | src.policies:train:144 - The actual batch size is 279, instead of 200\n",
      "2021-09-07 17:18:53.210 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:53.212 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.35450848937034607, 'baseline_loss': 0.5569663047790527, 'total_loss': -0.0760253369808197}\n",
      "2021-09-07 17:18:53.213 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7090912461280823\n",
      "2021-09-07 17:18:53.214 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6481573581695557\n",
      "2021-09-07 17:18:53.216 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:53.217 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:53.218 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:53.219 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3667469918727875, 'baseline_loss': 0.4412996172904968, 'total_loss': -0.14609718322753906}\n",
      "2021-09-07 17:18:53.220 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6718150973320007\n",
      "2021-09-07 17:18:53.221 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5902475118637085\n",
      "2021-09-07 17:18:53.222 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:53.223 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:53.225 | INFO     | src.policies:train:123 - Epoch 751 / 800\n",
      "2021-09-07 17:18:53.225 | INFO     | src.policies:collect_trajectories:221 - Episode 1620\n",
      "2021-09-07 17:18:53.253 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:53.253 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:53.254 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:53.255 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:53.258 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5042036771774292, 'baseline_loss': 1.333093285560608, 'total_loss': 0.16234296560287476}\n",
      "2021-09-07 17:18:53.259 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2484782487154007\n",
      "2021-09-07 17:18:53.260 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.757315158843994\n",
      "2021-09-07 17:18:53.261 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2484782487154007\n",
      "2021-09-07 17:18:53.262 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:53.263 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:53.264 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6571444869041443, 'baseline_loss': 1.4443707466125488, 'total_loss': 0.06504088640213013}\n",
      "2021-09-07 17:18:53.265 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2824966609477997\n",
      "2021-09-07 17:18:53.266 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.9933905601501465\n",
      "2021-09-07 17:18:53.267 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2824966609477997\n",
      "2021-09-07 17:18:53.268 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:53.269 | INFO     | src.policies:train:123 - Epoch 752 / 800\n",
      "2021-09-07 17:18:53.270 | INFO     | src.policies:collect_trajectories:221 - Episode 1621\n",
      "2021-09-07 17:18:53.298 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:53.298 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:53.299 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:53.301 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:53.303 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5964189171791077, 'baseline_loss': 1.3721433877944946, 'total_loss': 0.08965277671813965}\n",
      "2021-09-07 17:18:53.304 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.45487701892852783\n",
      "2021-09-07 17:18:53.306 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.761132001876831\n",
      "2021-09-07 17:18:53.308 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.45487701892852783\n",
      "2021-09-07 17:18:53.309 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:53.310 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:53.311 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.605672299861908, 'baseline_loss': 1.3438200950622559, 'total_loss': 0.06623774766921997}\n",
      "2021-09-07 17:18:53.312 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.33263856172561646\n",
      "2021-09-07 17:18:53.313 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.3723626136779785\n",
      "2021-09-07 17:18:53.314 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.33263856172561646\n",
      "2021-09-07 17:18:53.316 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:53.317 | INFO     | src.policies:train:123 - Epoch 753 / 800\n",
      "2021-09-07 17:18:53.318 | INFO     | src.policies:collect_trajectories:221 - Episode 1622\n",
      "2021-09-07 17:18:53.347 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:53.347 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:53.348 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:53.351 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:53.355 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2353261411190033, 'baseline_loss': 0.374767005443573, 'total_loss': -0.0479426383972168}\n",
      "2021-09-07 17:18:53.356 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18654446303844452\n",
      "2021-09-07 17:18:53.357 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.16615629196167\n",
      "2021-09-07 17:18:53.358 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18654446303844452\n",
      "2021-09-07 17:18:53.359 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:53.360 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:53.361 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1804666966199875, 'baseline_loss': 0.3693614602088928, 'total_loss': 0.004214033484458923}\n",
      "2021-09-07 17:18:53.362 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08031664043664932\n",
      "2021-09-07 17:18:53.363 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3567605018615723\n",
      "2021-09-07 17:18:53.364 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08031664043664932\n",
      "2021-09-07 17:18:53.365 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:53.366 | INFO     | src.policies:train:123 - Epoch 754 / 800\n",
      "2021-09-07 17:18:53.367 | INFO     | src.policies:collect_trajectories:221 - Episode 1623\n",
      "2021-09-07 17:18:53.399 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:53.399 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:53.400 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:53.402 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:53.404 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5261522531509399, 'baseline_loss': 0.8836174607276917, 'total_loss': -0.08434352278709412}\n",
      "2021-09-07 17:18:53.405 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22108113765716553\n",
      "2021-09-07 17:18:53.406 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5795097351074219\n",
      "2021-09-07 17:18:53.407 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22108113765716553\n",
      "2021-09-07 17:18:53.408 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:53.410 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:53.411 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5749107599258423, 'baseline_loss': 0.9974517822265625, 'total_loss': -0.07618486881256104}\n",
      "2021-09-07 17:18:53.412 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2282899022102356\n",
      "2021-09-07 17:18:53.413 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3271888494491577\n",
      "2021-09-07 17:18:53.414 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2282899022102356\n",
      "2021-09-07 17:18:53.415 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:53.416 | INFO     | src.policies:train:123 - Epoch 755 / 800\n",
      "2021-09-07 17:18:53.417 | INFO     | src.policies:collect_trajectories:221 - Episode 1624\n",
      "2021-09-07 17:18:53.445 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:53.446 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:53.446 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:53.450 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:53.452 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20952816307544708, 'baseline_loss': 0.41667068004608154, 'total_loss': -0.001192823052406311}\n",
      "2021-09-07 17:18:53.453 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22032777965068817\n",
      "2021-09-07 17:18:53.455 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2162450551986694\n",
      "2021-09-07 17:18:53.456 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22032777965068817\n",
      "2021-09-07 17:18:53.457 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:53.458 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:53.459 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.17537261545658112, 'baseline_loss': 0.36077821254730225, 'total_loss': 0.005016490817070007}\n",
      "2021-09-07 17:18:53.460 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1508060246706009\n",
      "2021-09-07 17:18:53.461 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.6352733373641968\n",
      "2021-09-07 17:18:53.462 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1508060246706009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:53.464 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:53.465 | INFO     | src.policies:train:123 - Epoch 756 / 800\n",
      "2021-09-07 17:18:53.465 | INFO     | src.policies:collect_trajectories:221 - Episode 1625\n",
      "2021-09-07 17:18:53.494 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:53.495 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:53.495 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:53.497 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:53.499 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2526150345802307, 'baseline_loss': 0.3795895278453827, 'total_loss': -0.06282027065753937}\n",
      "2021-09-07 17:18:53.500 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13133437931537628\n",
      "2021-09-07 17:18:53.501 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2711446285247803\n",
      "2021-09-07 17:18:53.502 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13133437931537628\n",
      "2021-09-07 17:18:53.503 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:53.505 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:53.506 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.14143192768096924, 'baseline_loss': 0.36123618483543396, 'total_loss': 0.03918616473674774}\n",
      "2021-09-07 17:18:53.507 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.28920236229896545\n",
      "2021-09-07 17:18:53.508 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.317928671836853\n",
      "2021-09-07 17:18:53.509 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.28920236229896545\n",
      "2021-09-07 17:18:53.510 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:53.512 | INFO     | src.policies:train:123 - Epoch 757 / 800\n",
      "2021-09-07 17:18:53.513 | INFO     | src.policies:collect_trajectories:221 - Episode 1626\n",
      "2021-09-07 17:18:53.544 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:53.545 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:53.545 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:53.547 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:53.549 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6917782425880432, 'baseline_loss': 1.932366967201233, 'total_loss': 0.27440524101257324}\n",
      "2021-09-07 17:18:53.550 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15718044340610504\n",
      "2021-09-07 17:18:53.551 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.584007263183594\n",
      "2021-09-07 17:18:53.552 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15718044340610504\n",
      "2021-09-07 17:18:53.553 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:53.554 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:53.555 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.64858078956604, 'baseline_loss': 1.242674469947815, 'total_loss': -0.02724355459213257}\n",
      "2021-09-07 17:18:53.556 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4010358452796936\n",
      "2021-09-07 17:18:53.557 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.691810131072998\n",
      "2021-09-07 17:18:53.558 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4010358452796936\n",
      "2021-09-07 17:18:53.559 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:53.560 | INFO     | src.policies:train:123 - Epoch 758 / 800\n",
      "2021-09-07 17:18:53.561 | INFO     | src.policies:collect_trajectories:221 - Episode 1627\n",
      "2021-09-07 17:18:53.589 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:53.590 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:53.590 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:53.593 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:53.595 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5430951714515686, 'baseline_loss': 0.9944626092910767, 'total_loss': -0.04586386680603027}\n",
      "2021-09-07 17:18:53.596 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3642665147781372\n",
      "2021-09-07 17:18:53.597 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8917701244354248\n",
      "2021-09-07 17:18:53.598 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3642665147781372\n",
      "2021-09-07 17:18:53.599 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:53.600 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:53.601 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5306437015533447, 'baseline_loss': 1.1756298542022705, 'total_loss': 0.05717122554779053}\n",
      "2021-09-07 17:18:53.602 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2146446704864502\n",
      "2021-09-07 17:18:53.603 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5442228317260742\n",
      "2021-09-07 17:18:53.604 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2146446704864502\n",
      "2021-09-07 17:18:53.605 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:53.606 | INFO     | src.policies:train:123 - Epoch 759 / 800\n",
      "2021-09-07 17:18:53.607 | INFO     | src.policies:collect_trajectories:221 - Episode 1628\n",
      "2021-09-07 17:18:53.636 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:53.636 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:53.637 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:53.639 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:53.641 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6036653518676758, 'baseline_loss': 1.1933412551879883, 'total_loss': -0.006994724273681641}\n",
      "2021-09-07 17:18:53.642 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21402201056480408\n",
      "2021-09-07 17:18:53.644 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.1778042316436768\n",
      "2021-09-07 17:18:53.645 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21402201056480408\n",
      "2021-09-07 17:18:53.646 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997913837433\n",
      "2021-09-07 17:18:53.647 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:53.648 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5382251143455505, 'baseline_loss': 1.043450117111206, 'total_loss': -0.01650005578994751}\n",
      "2021-09-07 17:18:53.649 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3246631622314453\n",
      "2021-09-07 17:18:53.649 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.2275021076202393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:53.650 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3246631622314453\n",
      "2021-09-07 17:18:53.651 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:53.653 | INFO     | src.policies:train:123 - Epoch 760 / 800\n",
      "2021-09-07 17:18:53.653 | INFO     | src.policies:collect_trajectories:221 - Episode 1629\n",
      "2021-09-07 17:18:53.682 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:53.683 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:53.683 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:53.715 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:53.746 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3866950273513794, 'baseline_loss': 0.5541887879371643, 'total_loss': -0.10960063338279724}\n",
      "2021-09-07 17:18:53.747 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22829370200634003\n",
      "2021-09-07 17:18:53.748 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.26738715171813965\n",
      "2021-09-07 17:18:53.749 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22829370200634003\n",
      "2021-09-07 17:18:53.751 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.26738715171813965\n",
      "2021-09-07 17:18:53.752 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:53.753 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3478400707244873, 'baseline_loss': 0.5043970346450806, 'total_loss': -0.09564155340194702}\n",
      "2021-09-07 17:18:53.754 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18804453313350677\n",
      "2021-09-07 17:18:53.755 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.23745369911193848\n",
      "2021-09-07 17:18:53.756 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18804453313350677\n",
      "2021-09-07 17:18:53.757 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.23745369911193848\n",
      "2021-09-07 17:18:53.758 | INFO     | src.policies:train:123 - Epoch 761 / 800\n",
      "2021-09-07 17:18:53.759 | INFO     | src.policies:collect_trajectories:221 - Episode 1630\n",
      "2021-09-07 17:18:53.786 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:53.787 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:53.787 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:53.789 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:53.793 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5864114761352539, 'baseline_loss': 1.0007354021072388, 'total_loss': -0.08604377508163452}\n",
      "2021-09-07 17:18:53.794 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.46043047308921814\n",
      "2021-09-07 17:18:53.795 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.2695106267929077\n",
      "2021-09-07 17:18:53.796 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.46043047308921814\n",
      "2021-09-07 17:18:53.797 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:53.798 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:53.800 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5723521113395691, 'baseline_loss': 0.9815098643302917, 'total_loss': -0.08159717917442322}\n",
      "2021-09-07 17:18:53.801 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6147031784057617\n",
      "2021-09-07 17:18:53.802 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0943732261657715\n",
      "2021-09-07 17:18:53.803 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:53.804 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:53.806 | INFO     | src.policies:train:123 - Epoch 762 / 800\n",
      "2021-09-07 17:18:53.806 | INFO     | src.policies:collect_trajectories:221 - Episode 1631\n",
      "2021-09-07 17:18:53.835 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:53.835 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:53.836 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:53.838 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:53.840 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4230755567550659, 'baseline_loss': 0.8122344017028809, 'total_loss': -0.01695835590362549}\n",
      "2021-09-07 17:18:53.841 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.375918984413147\n",
      "2021-09-07 17:18:53.842 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5989865064620972\n",
      "2021-09-07 17:18:53.844 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.375918984413147\n",
      "2021-09-07 17:18:53.845 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:53.846 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:53.847 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.515047550201416, 'baseline_loss': 0.7997514605522156, 'total_loss': -0.11517181992530823}\n",
      "2021-09-07 17:18:53.848 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32584938406944275\n",
      "2021-09-07 17:18:53.848 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8880078792572021\n",
      "2021-09-07 17:18:53.849 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32584938406944275\n",
      "2021-09-07 17:18:53.851 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:53.852 | INFO     | src.policies:train:123 - Epoch 763 / 800\n",
      "2021-09-07 17:18:53.853 | INFO     | src.policies:collect_trajectories:221 - Episode 1632\n",
      "2021-09-07 17:18:53.881 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:53.881 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:53.881 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:53.883 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:53.886 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2141842246055603, 'baseline_loss': 0.3670816123485565, 'total_loss': -0.030643418431282043}\n",
      "2021-09-07 17:18:53.887 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15016604959964752\n",
      "2021-09-07 17:18:53.887 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8616241216659546\n",
      "2021-09-07 17:18:53.889 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15016604959964752\n",
      "2021-09-07 17:18:53.890 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:53.891 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:53.892 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.23976215720176697, 'baseline_loss': 0.38073718547821045, 'total_loss': -0.04939356446266174}\n",
      "2021-09-07 17:18:53.893 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.26723575592041016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:53.894 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3990775346755981\n",
      "2021-09-07 17:18:53.895 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.26723575592041016\n",
      "2021-09-07 17:18:53.896 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:53.898 | INFO     | src.policies:train:123 - Epoch 764 / 800\n",
      "2021-09-07 17:18:53.898 | INFO     | src.policies:collect_trajectories:221 - Episode 1633\n",
      "2021-09-07 17:18:53.927 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:53.928 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:53.928 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:53.930 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:53.933 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4626205861568451, 'baseline_loss': 0.7016377449035645, 'total_loss': -0.11180171370506287}\n",
      "2021-09-07 17:18:53.934 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.210318461060524\n",
      "2021-09-07 17:18:53.936 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.46472111344337463\n",
      "2021-09-07 17:18:53.937 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.210318461060524\n",
      "2021-09-07 17:18:53.938 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.46472111344337463\n",
      "2021-09-07 17:18:53.940 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:53.941 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3115713894367218, 'baseline_loss': 0.425415575504303, 'total_loss': -0.09886360168457031}\n",
      "2021-09-07 17:18:53.942 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.04194001108407974\n",
      "2021-09-07 17:18:53.943 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4055778384208679\n",
      "2021-09-07 17:18:53.944 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.04194001108407974\n",
      "2021-09-07 17:18:53.945 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4055778384208679\n",
      "2021-09-07 17:18:53.947 | INFO     | src.policies:train:123 - Epoch 765 / 800\n",
      "2021-09-07 17:18:53.947 | INFO     | src.policies:collect_trajectories:221 - Episode 1634\n",
      "2021-09-07 17:18:54.090 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:54.091 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:54.091 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:54.094 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:54.096 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.9704368114471436, 'baseline_loss': 4.2483110427856445, 'total_loss': 1.1537187099456787}\n",
      "2021-09-07 17:18:54.097 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3911079168319702\n",
      "2021-09-07 17:18:54.098 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 6.687371253967285\n",
      "2021-09-07 17:18:54.100 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3911079168319702\n",
      "2021-09-07 17:18:54.101 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999999403953552\n",
      "2021-09-07 17:18:54.102 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:54.103 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.9293299317359924, 'baseline_loss': 4.27293586730957, 'total_loss': 1.2071380615234375}\n",
      "2021-09-07 17:18:54.104 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5137263536453247\n",
      "2021-09-07 17:18:54.105 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 7.3386101722717285\n",
      "2021-09-07 17:18:54.106 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:54.107 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999999403953552\n",
      "2021-09-07 17:18:54.108 | INFO     | src.policies:train:123 - Epoch 766 / 800\n",
      "2021-09-07 17:18:54.109 | INFO     | src.policies:collect_trajectories:221 - Episode 1635\n",
      "2021-09-07 17:18:54.137 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:54.137 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:54.138 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:54.139 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:54.142 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5273726582527161, 'baseline_loss': 1.116797685623169, 'total_loss': 0.031026184558868408}\n",
      "2021-09-07 17:18:54.143 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.28277942538261414\n",
      "2021-09-07 17:18:54.145 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.56480073928833\n",
      "2021-09-07 17:18:54.146 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.28277942538261414\n",
      "2021-09-07 17:18:54.147 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:54.148 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:54.149 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4570600390434265, 'baseline_loss': 0.8767264485359192, 'total_loss': -0.01869681477546692}\n",
      "2021-09-07 17:18:54.150 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.32543516159057617\n",
      "2021-09-07 17:18:54.151 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7905951738357544\n",
      "2021-09-07 17:18:54.152 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.32543516159057617\n",
      "2021-09-07 17:18:54.153 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:54.154 | INFO     | src.policies:train:123 - Epoch 767 / 800\n",
      "2021-09-07 17:18:54.155 | INFO     | src.policies:collect_trajectories:221 - Episode 1636\n",
      "2021-09-07 17:18:54.182 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:54.183 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:54.183 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:54.186 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:54.188 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6962487101554871, 'baseline_loss': 1.937291145324707, 'total_loss': 0.27239686250686646}\n",
      "2021-09-07 17:18:54.189 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30450794100761414\n",
      "2021-09-07 17:18:54.190 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.974186897277832\n",
      "2021-09-07 17:18:54.191 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30450794100761414\n",
      "2021-09-07 17:18:54.192 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999991059303284\n",
      "2021-09-07 17:18:54.194 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:54.195 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6804423332214355, 'baseline_loss': 1.9976792335510254, 'total_loss': 0.31839728355407715}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:54.196 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6871870160102844\n",
      "2021-09-07 17:18:54.197 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 4.204105377197266\n",
      "2021-09-07 17:18:54.198 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:54.199 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:54.200 | INFO     | src.policies:train:123 - Epoch 768 / 800\n",
      "2021-09-07 17:18:54.200 | INFO     | src.policies:collect_trajectories:221 - Episode 1637\n",
      "2021-09-07 17:18:54.228 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:54.228 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:54.229 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:54.231 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:54.233 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.942646861076355, 'baseline_loss': 3.419698715209961, 'total_loss': 0.7672024965286255}\n",
      "2021-09-07 17:18:54.234 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6827820539474487\n",
      "2021-09-07 17:18:54.234 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 5.820328235626221\n",
      "2021-09-07 17:18:54.236 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:54.237 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999999403953552\n",
      "2021-09-07 17:18:54.238 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:54.239 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.9397680163383484, 'baseline_loss': 3.4953811168670654, 'total_loss': 0.8079225420951843}\n",
      "2021-09-07 17:18:54.240 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3923324942588806\n",
      "2021-09-07 17:18:54.241 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 5.8880791664123535\n",
      "2021-09-07 17:18:54.242 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3923324942588806\n",
      "2021-09-07 17:18:54.243 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999988079071045\n",
      "2021-09-07 17:18:54.250 | INFO     | src.policies:train:123 - Epoch 769 / 800\n",
      "2021-09-07 17:18:54.290 | INFO     | src.policies:collect_trajectories:221 - Episode 1638\n",
      "2021-09-07 17:18:54.323 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:54.324 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:54.324 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:54.326 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:54.329 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.22540923953056335, 'baseline_loss': 0.41150447726249695, 'total_loss': -0.01965700089931488}\n",
      "2021-09-07 17:18:54.330 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4352988004684448\n",
      "2021-09-07 17:18:54.331 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8052643537521362\n",
      "2021-09-07 17:18:54.333 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4352988004684448\n",
      "2021-09-07 17:18:54.334 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:54.335 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:54.336 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.22690698504447937, 'baseline_loss': 0.3900264799594879, 'total_loss': -0.03189374506473541}\n",
      "2021-09-07 17:18:54.337 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10505679994821548\n",
      "2021-09-07 17:18:54.337 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.716086983680725\n",
      "2021-09-07 17:18:54.339 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10505679994821548\n",
      "2021-09-07 17:18:54.340 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997019767761\n",
      "2021-09-07 17:18:54.341 | INFO     | src.policies:train:123 - Epoch 770 / 800\n",
      "2021-09-07 17:18:54.342 | INFO     | src.policies:collect_trajectories:221 - Episode 1639\n",
      "2021-09-07 17:18:54.370 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:54.371 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:54.371 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:54.373 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:54.375 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32139354944229126, 'baseline_loss': 0.4484875798225403, 'total_loss': -0.09714975953102112}\n",
      "2021-09-07 17:18:54.376 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2244396209716797\n",
      "2021-09-07 17:18:54.377 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8535317182540894\n",
      "2021-09-07 17:18:54.379 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2244396209716797\n",
      "2021-09-07 17:18:54.380 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:54.381 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:54.382 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29928961396217346, 'baseline_loss': 0.4392939507961273, 'total_loss': -0.0796426385641098}\n",
      "2021-09-07 17:18:54.384 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.06919365376234055\n",
      "2021-09-07 17:18:54.385 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8414093255996704\n",
      "2021-09-07 17:18:54.386 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.06919365376234055\n",
      "2021-09-07 17:18:54.387 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:54.388 | INFO     | src.policies:train:123 - Epoch 771 / 800\n",
      "2021-09-07 17:18:54.389 | INFO     | src.policies:collect_trajectories:221 - Episode 1640\n",
      "2021-09-07 17:18:54.418 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:54.419 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:54.419 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:54.420 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:54.424 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4506166875362396, 'baseline_loss': 0.9118270874023438, 'total_loss': 0.005296856164932251}\n",
      "2021-09-07 17:18:54.425 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2688535451889038\n",
      "2021-09-07 17:18:54.426 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.361823320388794\n",
      "2021-09-07 17:18:54.428 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2688535451889038\n",
      "2021-09-07 17:18:54.429 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:54.430 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:54.431 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43598711490631104, 'baseline_loss': 0.9752878546714783, 'total_loss': 0.0516568124294281}\n",
      "2021-09-07 17:18:54.432 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3677012026309967\n",
      "2021-09-07 17:18:54.433 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1977490186691284\n",
      "2021-09-07 17:18:54.434 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3677012026309967\n",
      "2021-09-07 17:18:54.435 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:54.437 | INFO     | src.policies:train:123 - Epoch 772 / 800\n",
      "2021-09-07 17:18:54.438 | INFO     | src.policies:collect_trajectories:221 - Episode 1641\n",
      "2021-09-07 17:18:54.467 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:54.468 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:54.468 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:54.470 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:54.473 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.15967582166194916, 'baseline_loss': 0.4033583700656891, 'total_loss': 0.042003363370895386}\n",
      "2021-09-07 17:18:54.474 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2059931755065918\n",
      "2021-09-07 17:18:54.475 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.430699110031128\n",
      "2021-09-07 17:18:54.476 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2059931755065918\n",
      "2021-09-07 17:18:54.477 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:54.479 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:54.480 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.17751392722129822, 'baseline_loss': 0.38000527024269104, 'total_loss': 0.012488707900047302}\n",
      "2021-09-07 17:18:54.481 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20640826225280762\n",
      "2021-09-07 17:18:54.482 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4259989261627197\n",
      "2021-09-07 17:18:54.483 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20640826225280762\n",
      "2021-09-07 17:18:54.484 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n",
      "2021-09-07 17:18:54.485 | INFO     | src.policies:train:123 - Epoch 773 / 800\n",
      "2021-09-07 17:18:54.486 | INFO     | src.policies:collect_trajectories:221 - Episode 1642\n",
      "2021-09-07 17:18:54.516 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:54.517 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:54.518 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:54.521 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:54.522 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.094731405377388, 'baseline_loss': 0.5897437334060669, 'total_loss': 0.20014046132564545}\n",
      "2021-09-07 17:18:54.523 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.33547380566596985\n",
      "2021-09-07 17:18:54.524 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8720372915267944\n",
      "2021-09-07 17:18:54.525 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.33547380566596985\n",
      "2021-09-07 17:18:54.527 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:54.528 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:54.529 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.07086262106895447, 'baseline_loss': 0.5837429165840149, 'total_loss': 0.22100883722305298}\n",
      "2021-09-07 17:18:54.530 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3778965175151825\n",
      "2021-09-07 17:18:54.531 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8205498456954956\n",
      "2021-09-07 17:18:54.533 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3778965175151825\n",
      "2021-09-07 17:18:54.534 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:54.535 | INFO     | src.policies:train:123 - Epoch 774 / 800\n",
      "2021-09-07 17:18:54.536 | INFO     | src.policies:collect_trajectories:221 - Episode 1643\n",
      "2021-09-07 17:18:54.567 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:54.568 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:54.568 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:54.570 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:54.572 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.14739130437374115, 'baseline_loss': 0.38718345761299133, 'total_loss': 0.04620042443275452}\n",
      "2021-09-07 17:18:54.573 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19811737537384033\n",
      "2021-09-07 17:18:54.574 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4597060680389404\n",
      "2021-09-07 17:18:54.575 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19811737537384033\n",
      "2021-09-07 17:18:54.576 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:54.577 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:54.579 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.1966792494058609, 'baseline_loss': 0.3803795576095581, 'total_loss': -0.006489470601081848}\n",
      "2021-09-07 17:18:54.580 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10360096395015717\n",
      "2021-09-07 17:18:54.581 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5365530252456665\n",
      "2021-09-07 17:18:54.582 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10360096395015717\n",
      "2021-09-07 17:18:54.583 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:54.584 | INFO     | src.policies:train:123 - Epoch 775 / 800\n",
      "2021-09-07 17:18:54.585 | INFO     | src.policies:collect_trajectories:221 - Episode 1644\n",
      "2021-09-07 17:18:54.614 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:54.614 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:54.615 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:54.617 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:54.619 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.18039701879024506, 'baseline_loss': 0.3703499734401703, 'total_loss': 0.004777967929840088}\n",
      "2021-09-07 17:18:54.620 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6077757477760315\n",
      "2021-09-07 17:18:54.621 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.7346773147583008\n",
      "2021-09-07 17:18:54.622 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:54.623 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999967217445374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:54.624 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:54.625 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.18616394698619843, 'baseline_loss': 0.363148033618927, 'total_loss': -0.004589930176734924}\n",
      "2021-09-07 17:18:54.626 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.22245916724205017\n",
      "2021-09-07 17:18:54.627 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1853972673416138\n",
      "2021-09-07 17:18:54.628 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.22245916724205017\n",
      "2021-09-07 17:18:54.629 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:54.631 | INFO     | src.policies:train:123 - Epoch 776 / 800\n",
      "2021-09-07 17:18:54.631 | INFO     | src.policies:collect_trajectories:221 - Episode 1645\n",
      "2021-09-07 17:18:54.659 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:54.660 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:54.661 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:54.662 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:54.664 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.10459741950035095, 'baseline_loss': 0.49972420930862427, 'total_loss': 0.14526468515396118}\n",
      "2021-09-07 17:18:54.665 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18604256212711334\n",
      "2021-09-07 17:18:54.666 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.8787562847137451\n",
      "2021-09-07 17:18:54.668 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18604256212711334\n",
      "2021-09-07 17:18:54.669 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:54.670 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:54.671 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.08776545524597168, 'baseline_loss': 0.4340534210205078, 'total_loss': 0.12926125526428223}\n",
      "2021-09-07 17:18:54.672 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2713003158569336\n",
      "2021-09-07 17:18:54.673 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.5664366483688354\n",
      "2021-09-07 17:18:54.674 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2713003158569336\n",
      "2021-09-07 17:18:54.675 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:54.676 | INFO     | src.policies:train:123 - Epoch 777 / 800\n",
      "2021-09-07 17:18:54.676 | INFO     | src.policies:collect_trajectories:221 - Episode 1646\n",
      "2021-09-07 17:18:54.706 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:54.707 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:54.707 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:54.709 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:54.711 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3910788297653198, 'baseline_loss': 0.48470747470855713, 'total_loss': -0.14872509241104126}\n",
      "2021-09-07 17:18:54.712 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.43189647793769836\n",
      "2021-09-07 17:18:54.713 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5621318817138672\n",
      "2021-09-07 17:18:54.714 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.43189647793769836\n",
      "2021-09-07 17:18:54.715 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:54.716 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:54.717 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2828209400177002, 'baseline_loss': 0.37664690613746643, 'total_loss': -0.09449748694896698}\n",
      "2021-09-07 17:18:54.718 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4117249846458435\n",
      "2021-09-07 17:18:54.719 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8241357803344727\n",
      "2021-09-07 17:18:54.720 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4117249846458435\n",
      "2021-09-07 17:18:54.721 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:54.722 | INFO     | src.policies:train:123 - Epoch 778 / 800\n",
      "2021-09-07 17:18:54.723 | INFO     | src.policies:collect_trajectories:221 - Episode 1647\n",
      "2021-09-07 17:18:54.751 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:54.752 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:54.752 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:54.754 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:54.757 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6197373270988464, 'baseline_loss': 1.3248569965362549, 'total_loss': 0.042691171169281006}\n",
      "2021-09-07 17:18:54.758 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21894042193889618\n",
      "2021-09-07 17:18:54.759 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.06137752532959\n",
      "2021-09-07 17:18:54.760 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21894042193889618\n",
      "2021-09-07 17:18:54.761 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:54.762 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:54.763 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.6862683296203613, 'baseline_loss': 1.5693055391311646, 'total_loss': 0.09838443994522095}\n",
      "2021-09-07 17:18:54.764 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25382882356643677\n",
      "2021-09-07 17:18:54.765 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 3.316612720489502\n",
      "2021-09-07 17:18:54.766 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25382882356643677\n",
      "2021-09-07 17:18:54.767 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999998211860657\n",
      "2021-09-07 17:18:54.768 | INFO     | src.policies:train:123 - Epoch 779 / 800\n",
      "2021-09-07 17:18:54.769 | INFO     | src.policies:collect_trajectories:221 - Episode 1648\n",
      "2021-09-07 17:18:54.860 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:54.861 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:54.861 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:54.863 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:54.865 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3620867431163788, 'baseline_loss': 0.6355283856391907, 'total_loss': -0.04432255029678345}\n",
      "2021-09-07 17:18:54.866 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11052548885345459\n",
      "2021-09-07 17:18:54.867 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6479142904281616\n",
      "2021-09-07 17:18:54.868 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11052548885345459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:54.870 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:54.871 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:54.872 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.44053542613983154, 'baseline_loss': 0.7074931263923645, 'total_loss': -0.08678886294364929}\n",
      "2021-09-07 17:18:54.873 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15074780583381653\n",
      "2021-09-07 17:18:54.874 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5984836220741272\n",
      "2021-09-07 17:18:54.875 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15074780583381653\n",
      "2021-09-07 17:18:54.877 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:54.878 | INFO     | src.policies:train:123 - Epoch 780 / 800\n",
      "2021-09-07 17:18:54.879 | INFO     | src.policies:collect_trajectories:221 - Episode 1649\n",
      "2021-09-07 17:18:54.909 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:54.910 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:54.910 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:54.912 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:54.914 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4235251545906067, 'baseline_loss': 0.6319447159767151, 'total_loss': -0.10755279660224915}\n",
      "2021-09-07 17:18:54.915 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3464232087135315\n",
      "2021-09-07 17:18:54.917 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5629712343215942\n",
      "2021-09-07 17:18:54.918 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3464232087135315\n",
      "2021-09-07 17:18:54.919 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:54.920 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:54.922 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.38094720244407654, 'baseline_loss': 0.5845091938972473, 'total_loss': -0.08869260549545288}\n",
      "2021-09-07 17:18:54.922 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.35576674342155457\n",
      "2021-09-07 17:18:54.923 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5282782912254333\n",
      "2021-09-07 17:18:54.924 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.35576674342155457\n",
      "2021-09-07 17:18:54.925 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:54.927 | INFO     | src.policies:train:123 - Epoch 781 / 800\n",
      "2021-09-07 17:18:54.927 | INFO     | src.policies:collect_trajectories:221 - Episode 1650\n",
      "2021-09-07 17:18:54.956 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:54.957 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 193.0\n",
      "2021-09-07 17:18:54.957 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 193.0\n",
      "2021-09-07 17:18:54.957 | INFO     | src.policies:collect_trajectories:221 - Episode 1651\n",
      "2021-09-07 17:18:54.989 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:54.990 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:54.990 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 196.5\n",
      "2021-09-07 17:18:54.991 | WARNING  | src.policies:train:144 - The actual batch size is 393, instead of 200\n",
      "2021-09-07 17:18:54.994 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:54.996 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3750881850719452, 'baseline_loss': 0.6894446015357971, 'total_loss': -0.03036588430404663}\n",
      "2021-09-07 17:18:54.997 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4741706848144531\n",
      "2021-09-07 17:18:54.998 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.33536285161972046\n",
      "2021-09-07 17:18:54.999 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4741706848144531\n",
      "2021-09-07 17:18:55.000 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.33536285161972046\n",
      "2021-09-07 17:18:55.002 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:55.003 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3830760717391968, 'baseline_loss': 0.6442778706550598, 'total_loss': -0.06093713641166687}\n",
      "2021-09-07 17:18:55.004 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1587991565465927\n",
      "2021-09-07 17:18:55.005 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.22921554744243622\n",
      "2021-09-07 17:18:55.007 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1587991565465927\n",
      "2021-09-07 17:18:55.007 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.22921554744243622\n",
      "2021-09-07 17:18:55.009 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:55.010 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3069193661212921, 'baseline_loss': 0.5690369606018066, 'total_loss': -0.022400885820388794}\n",
      "2021-09-07 17:18:55.011 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1864539384841919\n",
      "2021-09-07 17:18:55.012 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5272554159164429\n",
      "2021-09-07 17:18:55.013 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1864539384841919\n",
      "2021-09-07 17:18:55.014 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999990165233612\n",
      "2021-09-07 17:18:55.016 | INFO     | src.policies:train:123 - Epoch 782 / 800\n",
      "2021-09-07 17:18:55.016 | INFO     | src.policies:collect_trajectories:221 - Episode 1652\n",
      "2021-09-07 17:18:55.044 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:55.045 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 196.0\n",
      "2021-09-07 17:18:55.045 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 196.0\n",
      "2021-09-07 17:18:55.045 | INFO     | src.policies:collect_trajectories:221 - Episode 1653\n",
      "2021-09-07 17:18:55.077 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:55.077 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:55.077 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 198.0\n",
      "2021-09-07 17:18:55.078 | WARNING  | src.policies:train:144 - The actual batch size is 396, instead of 200\n",
      "2021-09-07 17:18:55.081 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:55.083 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4325906038284302, 'baseline_loss': 0.6133447885513306, 'total_loss': -0.1259182095527649}\n",
      "2021-09-07 17:18:55.084 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15439727902412415\n",
      "2021-09-07 17:18:55.085 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.29234594106674194\n",
      "2021-09-07 17:18:55.086 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15439727902412415\n",
      "2021-09-07 17:18:55.087 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.29234594106674194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:55.088 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:55.089 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36925631761550903, 'baseline_loss': 0.5933966636657715, 'total_loss': -0.07255798578262329}\n",
      "2021-09-07 17:18:55.090 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.41722798347473145\n",
      "2021-09-07 17:18:55.091 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.535599946975708\n",
      "2021-09-07 17:18:55.092 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.41722798347473145\n",
      "2021-09-07 17:18:55.093 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999989867210388\n",
      "2021-09-07 17:18:55.094 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:55.095 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29808974266052246, 'baseline_loss': 0.4266568124294281, 'total_loss': -0.08476133644580841}\n",
      "2021-09-07 17:18:55.097 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.16434723138809204\n",
      "2021-09-07 17:18:55.098 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3446660339832306\n",
      "2021-09-07 17:18:55.099 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.16434723138809204\n",
      "2021-09-07 17:18:55.100 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3446660339832306\n",
      "2021-09-07 17:18:55.101 | INFO     | src.policies:train:123 - Epoch 783 / 800\n",
      "2021-09-07 17:18:55.101 | INFO     | src.policies:collect_trajectories:221 - Episode 1654\n",
      "2021-09-07 17:18:55.132 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:55.132 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:55.132 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:55.135 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:55.136 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4894483685493469, 'baseline_loss': 0.6912581920623779, 'total_loss': -0.14381927251815796}\n",
      "2021-09-07 17:18:55.137 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.21912692487239838\n",
      "2021-09-07 17:18:55.138 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6978304386138916\n",
      "2021-09-07 17:18:55.140 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.21912692487239838\n",
      "2021-09-07 17:18:55.141 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:55.143 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:55.144 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.31606531143188477, 'baseline_loss': 0.4203975200653076, 'total_loss': -0.10586655139923096}\n",
      "2021-09-07 17:18:55.145 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.08294487744569778\n",
      "2021-09-07 17:18:55.145 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7216308116912842\n",
      "2021-09-07 17:18:55.147 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.08294487744569778\n",
      "2021-09-07 17:18:55.148 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:55.149 | INFO     | src.policies:train:123 - Epoch 784 / 800\n",
      "2021-09-07 17:18:55.149 | INFO     | src.policies:collect_trajectories:221 - Episode 1655\n",
      "2021-09-07 17:18:55.176 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:55.176 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 185.0\n",
      "2021-09-07 17:18:55.177 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 185.0\n",
      "2021-09-07 17:18:55.177 | INFO     | src.policies:collect_trajectories:221 - Episode 1656\n",
      "2021-09-07 17:18:55.211 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:55.211 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 199.0\n",
      "2021-09-07 17:18:55.212 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 192.0\n",
      "2021-09-07 17:18:55.213 | WARNING  | src.policies:train:144 - The actual batch size is 384, instead of 200\n",
      "2021-09-07 17:18:55.215 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:55.217 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29770830273628235, 'baseline_loss': 0.35372665524482727, 'total_loss': -0.12084497511386871}\n",
      "2021-09-07 17:18:55.218 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.44154343008995056\n",
      "2021-09-07 17:18:55.219 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8874932527542114\n",
      "2021-09-07 17:18:55.220 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.44154343008995056\n",
      "2021-09-07 17:18:55.221 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:55.223 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:55.224 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.29367417097091675, 'baseline_loss': 0.3955147862434387, 'total_loss': -0.09591677784919739}\n",
      "2021-09-07 17:18:55.225 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30803143978118896\n",
      "2021-09-07 17:18:55.225 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6849631071090698\n",
      "2021-09-07 17:18:55.226 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30803143978118896\n",
      "2021-09-07 17:18:55.227 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:55.229 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:55.230 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2534720003604889, 'baseline_loss': 0.41690948605537415, 'total_loss': -0.04501725733280182}\n",
      "2021-09-07 17:18:55.231 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.13056033849716187\n",
      "2021-09-07 17:18:55.231 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6491111516952515\n",
      "2021-09-07 17:18:55.232 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.13056033849716187\n",
      "2021-09-07 17:18:55.233 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:55.235 | INFO     | src.policies:train:123 - Epoch 785 / 800\n",
      "2021-09-07 17:18:55.235 | INFO     | src.policies:collect_trajectories:221 - Episode 1657\n",
      "2021-09-07 17:18:55.263 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:55.263 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:55.264 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:55.266 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:55.268 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4663870632648468, 'baseline_loss': 0.7386965751647949, 'total_loss': -0.09703877568244934}\n",
      "2021-09-07 17:18:55.269 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.15294641256332397\n",
      "2021-09-07 17:18:55.270 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8271722793579102\n",
      "2021-09-07 17:18:55.272 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.15294641256332397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:55.273 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:55.274 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:55.275 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4267500042915344, 'baseline_loss': 0.6324383616447449, 'total_loss': -0.11053082346916199}\n",
      "2021-09-07 17:18:55.276 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3172348141670227\n",
      "2021-09-07 17:18:55.277 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5520754456520081\n",
      "2021-09-07 17:18:55.278 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3172348141670227\n",
      "2021-09-07 17:18:55.279 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:55.280 | INFO     | src.policies:train:123 - Epoch 786 / 800\n",
      "2021-09-07 17:18:55.281 | INFO     | src.policies:collect_trajectories:221 - Episode 1658\n",
      "2021-09-07 17:18:55.314 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:55.314 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:55.315 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:55.317 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:55.320 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20052331686019897, 'baseline_loss': 0.33813098073005676, 'total_loss': -0.03145782649517059}\n",
      "2021-09-07 17:18:55.321 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2800576686859131\n",
      "2021-09-07 17:18:55.322 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9590356349945068\n",
      "2021-09-07 17:18:55.323 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2800576686859131\n",
      "2021-09-07 17:18:55.324 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:55.326 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:55.327 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2981315851211548, 'baseline_loss': 0.41900262236595154, 'total_loss': -0.08863027393817902}\n",
      "2021-09-07 17:18:55.327 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3403044044971466\n",
      "2021-09-07 17:18:55.328 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6563383340835571\n",
      "2021-09-07 17:18:55.329 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3403044044971466\n",
      "2021-09-07 17:18:55.330 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992251396179\n",
      "2021-09-07 17:18:55.332 | INFO     | src.policies:train:123 - Epoch 787 / 800\n",
      "2021-09-07 17:18:55.332 | INFO     | src.policies:collect_trajectories:221 - Episode 1659\n",
      "2021-09-07 17:18:55.352 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:55.353 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 129.0\n",
      "2021-09-07 17:18:55.353 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 129.0\n",
      "2021-09-07 17:18:55.354 | INFO     | src.policies:collect_trajectories:221 - Episode 1660\n",
      "2021-09-07 17:18:55.528 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:55.528 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 195.0\n",
      "2021-09-07 17:18:55.529 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 162.0\n",
      "2021-09-07 17:18:55.529 | WARNING  | src.policies:train:144 - The actual batch size is 324, instead of 200\n",
      "2021-09-07 17:18:55.533 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:55.536 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.46484386920928955, 'baseline_loss': 0.8297848701477051, 'total_loss': -0.04995143413543701}\n",
      "2021-09-07 17:18:55.536 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20037798583507538\n",
      "2021-09-07 17:18:55.538 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7530487179756165\n",
      "2021-09-07 17:18:55.539 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20037798583507538\n",
      "2021-09-07 17:18:55.540 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999934434890747\n",
      "2021-09-07 17:18:55.541 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:55.542 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.38924112915992737, 'baseline_loss': 0.81174635887146, 'total_loss': 0.016632050275802612}\n",
      "2021-09-07 17:18:55.543 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.1535322070121765\n",
      "2021-09-07 17:18:55.544 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0888727903366089\n",
      "2021-09-07 17:18:55.545 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.1535322070121765\n",
      "2021-09-07 17:18:55.546 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:55.547 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:55.548 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4992269277572632, 'baseline_loss': 0.9679361581802368, 'total_loss': -0.015258848667144775}\n",
      "2021-09-07 17:18:55.549 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2252270132303238\n",
      "2021-09-07 17:18:55.550 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9925016760826111\n",
      "2021-09-07 17:18:55.551 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2252270132303238\n",
      "2021-09-07 17:18:55.551 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:55.553 | INFO     | src.policies:train:123 - Epoch 788 / 800\n",
      "2021-09-07 17:18:55.553 | INFO     | src.policies:collect_trajectories:221 - Episode 1661\n",
      "2021-09-07 17:18:55.579 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:55.580 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 171.0\n",
      "2021-09-07 17:18:55.580 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 171.0\n",
      "2021-09-07 17:18:55.581 | INFO     | src.policies:collect_trajectories:221 - Episode 1662\n",
      "2021-09-07 17:18:55.610 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:55.611 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 199.0\n",
      "2021-09-07 17:18:55.611 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 185.0\n",
      "2021-09-07 17:18:55.612 | WARNING  | src.policies:train:144 - The actual batch size is 370, instead of 200\n",
      "2021-09-07 17:18:55.615 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:55.617 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3079112768173218, 'baseline_loss': 0.4532071650028229, 'total_loss': -0.08130769431591034}\n",
      "2021-09-07 17:18:55.618 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.09965293109416962\n",
      "2021-09-07 17:18:55.619 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.37679797410964966\n",
      "2021-09-07 17:18:55.620 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.09965293109416962\n",
      "2021-09-07 17:18:55.621 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.37679797410964966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:55.622 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:55.623 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.20925326645374298, 'baseline_loss': 0.337091326713562, 'total_loss': -0.040707603096961975}\n",
      "2021-09-07 17:18:55.624 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.30465254187583923\n",
      "2021-09-07 17:18:55.625 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.017712950706482\n",
      "2021-09-07 17:18:55.626 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.30465254187583923\n",
      "2021-09-07 17:18:55.628 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995529651642\n",
      "2021-09-07 17:18:55.629 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:55.630 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.366127610206604, 'baseline_loss': 0.5763790011405945, 'total_loss': -0.07793810963630676}\n",
      "2021-09-07 17:18:55.631 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.25855594873428345\n",
      "2021-09-07 17:18:55.631 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.3231835663318634\n",
      "2021-09-07 17:18:55.633 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.25855594873428345\n",
      "2021-09-07 17:18:55.633 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.3231835663318634\n",
      "2021-09-07 17:18:55.635 | INFO     | src.policies:train:123 - Epoch 789 / 800\n",
      "2021-09-07 17:18:55.635 | INFO     | src.policies:collect_trajectories:221 - Episode 1663\n",
      "2021-09-07 17:18:55.655 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:55.656 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 142.0\n",
      "2021-09-07 17:18:55.656 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 142.0\n",
      "2021-09-07 17:18:55.656 | INFO     | src.policies:collect_trajectories:221 - Episode 1664\n",
      "2021-09-07 17:18:55.688 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:55.688 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:55.689 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 171.0\n",
      "2021-09-07 17:18:55.689 | WARNING  | src.policies:train:144 - The actual batch size is 342, instead of 200\n",
      "2021-09-07 17:18:55.692 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:55.694 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3672208786010742, 'baseline_loss': 0.4766092598438263, 'total_loss': -0.12891624867916107}\n",
      "2021-09-07 17:18:55.695 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4055176079273224\n",
      "2021-09-07 17:18:55.696 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.45653659105300903\n",
      "2021-09-07 17:18:55.697 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4055176079273224\n",
      "2021-09-07 17:18:55.698 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.45653659105300903\n",
      "2021-09-07 17:18:55.699 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:55.700 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.27581244707107544, 'baseline_loss': 0.5761232972145081, 'total_loss': 0.012249201536178589}\n",
      "2021-09-07 17:18:55.701 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3833167850971222\n",
      "2021-09-07 17:18:55.702 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8896389007568359\n",
      "2021-09-07 17:18:55.703 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3833167850971222\n",
      "2021-09-07 17:18:55.704 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:55.705 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:55.706 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3020660877227783, 'baseline_loss': 0.5210579037666321, 'total_loss': -0.04153713583946228}\n",
      "2021-09-07 17:18:55.707 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4776766002178192\n",
      "2021-09-07 17:18:55.708 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5936160683631897\n",
      "2021-09-07 17:18:55.709 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4776766002178192\n",
      "2021-09-07 17:18:55.710 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:55.711 | INFO     | src.policies:train:123 - Epoch 790 / 800\n",
      "2021-09-07 17:18:55.712 | INFO     | src.policies:collect_trajectories:221 - Episode 1665\n",
      "2021-09-07 17:18:55.739 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:55.740 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 180.0\n",
      "2021-09-07 17:18:55.741 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 180.0\n",
      "2021-09-07 17:18:55.741 | INFO     | src.policies:collect_trajectories:221 - Episode 1666\n",
      "2021-09-07 17:18:55.767 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:55.767 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 169.0\n",
      "2021-09-07 17:18:55.768 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 174.5\n",
      "2021-09-07 17:18:55.768 | WARNING  | src.policies:train:144 - The actual batch size is 349, instead of 200\n",
      "2021-09-07 17:18:55.772 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:55.774 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.25059500336647034, 'baseline_loss': 0.3375729024410248, 'total_loss': -0.08180855214595795}\n",
      "2021-09-07 17:18:55.775 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.11308380961418152\n",
      "2021-09-07 17:18:55.776 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.0709576606750488\n",
      "2021-09-07 17:18:55.777 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.11308380961418152\n",
      "2021-09-07 17:18:55.778 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995827674866\n",
      "2021-09-07 17:18:55.779 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:55.780 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2876838743686676, 'baseline_loss': 0.3809860646724701, 'total_loss': -0.09719084203243256}\n",
      "2021-09-07 17:18:55.781 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.31160134077072144\n",
      "2021-09-07 17:18:55.782 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7499452233314514\n",
      "2021-09-07 17:18:55.783 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.31160134077072144\n",
      "2021-09-07 17:18:55.784 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:55.785 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:55.786 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.26316747069358826, 'baseline_loss': 0.41888466477394104, 'total_loss': -0.05372513830661774}\n",
      "2021-09-07 17:18:55.787 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.19096946716308594\n",
      "2021-09-07 17:18:55.787 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9430139660835266\n",
      "2021-09-07 17:18:55.788 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.19096946716308594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:55.789 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999940395355225\n",
      "2021-09-07 17:18:55.791 | INFO     | src.policies:train:123 - Epoch 791 / 800\n",
      "2021-09-07 17:18:55.791 | INFO     | src.policies:collect_trajectories:221 - Episode 1667\n",
      "2021-09-07 17:18:55.822 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:55.823 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:55.824 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:55.826 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:55.828 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42970898747444153, 'baseline_loss': 0.5271084308624268, 'total_loss': -0.16615477204322815}\n",
      "2021-09-07 17:18:55.829 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24695204198360443\n",
      "2021-09-07 17:18:55.830 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.22488611936569214\n",
      "2021-09-07 17:18:55.831 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24695204198360443\n",
      "2021-09-07 17:18:55.832 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.22488611936569214\n",
      "2021-09-07 17:18:55.834 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:55.835 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.43141382932662964, 'baseline_loss': 0.6992794275283813, 'total_loss': -0.08177411556243896}\n",
      "2021-09-07 17:18:55.836 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.10917814821004868\n",
      "2021-09-07 17:18:55.837 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.8368818759918213\n",
      "2021-09-07 17:18:55.838 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.10917814821004868\n",
      "2021-09-07 17:18:55.839 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999943375587463\n",
      "2021-09-07 17:18:55.841 | INFO     | src.policies:train:123 - Epoch 792 / 800\n",
      "2021-09-07 17:18:55.842 | INFO     | src.policies:collect_trajectories:221 - Episode 1668\n",
      "2021-09-07 17:18:55.868 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:55.869 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 185.0\n",
      "2021-09-07 17:18:55.869 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 185.0\n",
      "2021-09-07 17:18:55.869 | INFO     | src.policies:collect_trajectories:221 - Episode 1669\n",
      "2021-09-07 17:18:55.905 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:55.906 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 177.0\n",
      "2021-09-07 17:18:55.907 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 181.0\n",
      "2021-09-07 17:18:55.908 | WARNING  | src.policies:train:144 - The actual batch size is 362, instead of 200\n",
      "2021-09-07 17:18:55.911 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:55.912 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.2958744764328003, 'baseline_loss': 0.3600284159183502, 'total_loss': -0.11586026847362518}\n",
      "2021-09-07 17:18:55.913 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.35014790296554565\n",
      "2021-09-07 17:18:55.914 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6580467820167542\n",
      "2021-09-07 17:18:55.915 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.35014790296554565\n",
      "2021-09-07 17:18:55.917 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:55.918 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:55.919 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.24734151363372803, 'baseline_loss': 0.38174015283584595, 'total_loss': -0.056471437215805054}\n",
      "2021-09-07 17:18:55.920 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5033706426620483\n",
      "2021-09-07 17:18:55.921 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7384227514266968\n",
      "2021-09-07 17:18:55.922 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999989867210388\n",
      "2021-09-07 17:18:55.923 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:55.924 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:55.926 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.36594897508621216, 'baseline_loss': 0.610455334186554, 'total_loss': -0.06072130799293518}\n",
      "2021-09-07 17:18:55.926 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5788900256156921\n",
      "2021-09-07 17:18:55.927 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5914227366447449\n",
      "2021-09-07 17:18:55.928 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:55.929 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999913573265076\n",
      "2021-09-07 17:18:55.931 | INFO     | src.policies:train:123 - Epoch 793 / 800\n",
      "2021-09-07 17:18:55.931 | INFO     | src.policies:collect_trajectories:221 - Episode 1670\n",
      "2021-09-07 17:18:55.962 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:55.962 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:55.963 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:55.965 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:55.967 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5279971361160278, 'baseline_loss': 1.112114429473877, 'total_loss': 0.028060078620910645}\n",
      "2021-09-07 17:18:55.968 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.20948466658592224\n",
      "2021-09-07 17:18:55.969 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 2.0206146240234375\n",
      "2021-09-07 17:18:55.970 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.20948466658592224\n",
      "2021-09-07 17:18:55.971 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997615814209\n",
      "2021-09-07 17:18:55.973 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:55.974 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5463463664054871, 'baseline_loss': 0.8008261322975159, 'total_loss': -0.14593330025672913}\n",
      "2021-09-07 17:18:55.975 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.7331186532974243\n",
      "2021-09-07 17:18:55.975 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.9716166853904724\n",
      "2021-09-07 17:18:55.976 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4999992549419403\n",
      "2021-09-07 17:18:55.977 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.499999463558197\n",
      "2021-09-07 17:18:55.979 | INFO     | src.policies:train:123 - Epoch 794 / 800\n",
      "2021-09-07 17:18:55.979 | INFO     | src.policies:collect_trajectories:221 - Episode 1671\n",
      "2021-09-07 17:18:56.060 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:56.060 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 187.0\n",
      "2021-09-07 17:18:56.061 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 187.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:56.061 | INFO     | src.policies:collect_trajectories:221 - Episode 1672\n",
      "2021-09-07 17:18:56.202 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:56.203 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 186.0\n",
      "2021-09-07 17:18:56.203 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 186.5\n",
      "2021-09-07 17:18:56.204 | WARNING  | src.policies:train:144 - The actual batch size is 373, instead of 200\n",
      "2021-09-07 17:18:56.206 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:56.209 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5090958476066589, 'baseline_loss': 0.8084837198257446, 'total_loss': -0.10485398769378662}\n",
      "2021-09-07 17:18:56.210 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.48964014649391174\n",
      "2021-09-07 17:18:56.211 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1354342699050903\n",
      "2021-09-07 17:18:56.212 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.48964014649391174\n",
      "2021-09-07 17:18:56.213 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:56.214 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:56.215 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3926840126514435, 'baseline_loss': 0.5510127544403076, 'total_loss': -0.11717763543128967}\n",
      "2021-09-07 17:18:56.216 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5305911302566528\n",
      "2021-09-07 17:18:56.217 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.35563957691192627\n",
      "2021-09-07 17:18:56.218 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.499999076128006\n",
      "2021-09-07 17:18:56.219 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.35563957691192627\n",
      "2021-09-07 17:18:56.220 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:56.221 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.40593165159225464, 'baseline_loss': 0.6025098562240601, 'total_loss': -0.10467672348022461}\n",
      "2021-09-07 17:18:56.222 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17862297594547272\n",
      "2021-09-07 17:18:56.223 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4275103509426117\n",
      "2021-09-07 17:18:56.224 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17862297594547272\n",
      "2021-09-07 17:18:56.225 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4275103509426117\n",
      "2021-09-07 17:18:56.226 | INFO     | src.policies:train:123 - Epoch 795 / 800\n",
      "2021-09-07 17:18:56.226 | INFO     | src.policies:collect_trajectories:221 - Episode 1673\n",
      "2021-09-07 17:18:56.254 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:56.255 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:56.255 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:56.258 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:56.261 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.42173099517822266, 'baseline_loss': 0.6186033487319946, 'total_loss': -0.11242932081222534}\n",
      "2021-09-07 17:18:56.262 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.24505700170993805\n",
      "2021-09-07 17:18:56.263 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.5955644845962524\n",
      "2021-09-07 17:18:56.264 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.24505700170993805\n",
      "2021-09-07 17:18:56.265 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:56.266 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:56.267 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4783828854560852, 'baseline_loss': 0.7984808087348938, 'total_loss': -0.0791424810886383}\n",
      "2021-09-07 17:18:56.268 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.27861496806144714\n",
      "2021-09-07 17:18:56.269 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1167293787002563\n",
      "2021-09-07 17:18:56.270 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.27861496806144714\n",
      "2021-09-07 17:18:56.271 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999961256980896\n",
      "2021-09-07 17:18:56.272 | INFO     | src.policies:train:123 - Epoch 796 / 800\n",
      "2021-09-07 17:18:56.273 | INFO     | src.policies:collect_trajectories:221 - Episode 1674\n",
      "2021-09-07 17:18:56.302 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:56.303 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:56.303 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:56.305 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:56.307 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5186598300933838, 'baseline_loss': 0.7062059044837952, 'total_loss': -0.1655568778514862}\n",
      "2021-09-07 17:18:56.309 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.3856325149536133\n",
      "2021-09-07 17:18:56.310 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.765611469745636\n",
      "2021-09-07 17:18:56.311 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.3856325149536133\n",
      "2021-09-07 17:18:56.312 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999993145465851\n",
      "2021-09-07 17:18:56.313 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:56.314 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5475607514381409, 'baseline_loss': 1.0207233428955078, 'total_loss': -0.03719907999038696}\n",
      "2021-09-07 17:18:56.315 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4017157554626465\n",
      "2021-09-07 17:18:56.316 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.9354203939437866\n",
      "2021-09-07 17:18:56.317 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4017157554626465\n",
      "2021-09-07 17:18:56.318 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999997317790985\n",
      "2021-09-07 17:18:56.319 | INFO     | src.policies:train:123 - Epoch 797 / 800\n",
      "2021-09-07 17:18:56.320 | INFO     | src.policies:collect_trajectories:221 - Episode 1675\n",
      "2021-09-07 17:18:56.347 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:56.348 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:56.349 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 200.0\n",
      "2021-09-07 17:18:56.351 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:56.353 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.52208012342453, 'baseline_loss': 0.9159141778945923, 'total_loss': -0.06412303447723389}\n",
      "2021-09-07 17:18:56.354 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.4850684106349945\n",
      "2021-09-07 17:18:56.355 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.4203643798828125\n",
      "2021-09-07 17:18:56.356 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.4850684106349945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:56.357 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:56.359 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:56.360 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5166565179824829, 'baseline_loss': 0.9258697032928467, 'total_loss': -0.05372166633605957}\n",
      "2021-09-07 17:18:56.360 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.14023199677467346\n",
      "2021-09-07 17:18:56.361 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3529202938079834\n",
      "2021-09-07 17:18:56.362 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.14023199677467346\n",
      "2021-09-07 17:18:56.363 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:56.365 | INFO     | src.policies:train:123 - Epoch 798 / 800\n",
      "2021-09-07 17:18:56.365 | INFO     | src.policies:collect_trajectories:221 - Episode 1676\n",
      "2021-09-07 17:18:56.390 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:56.390 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 178.0\n",
      "2021-09-07 17:18:56.391 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 178.0\n",
      "2021-09-07 17:18:56.391 | INFO     | src.policies:collect_trajectories:221 - Episode 1677\n",
      "2021-09-07 17:18:56.424 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:56.424 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 200.0\n",
      "2021-09-07 17:18:56.425 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 189.0\n",
      "2021-09-07 17:18:56.425 | WARNING  | src.policies:train:144 - The actual batch size is 378, instead of 200\n",
      "2021-09-07 17:18:56.429 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:56.431 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.355951189994812, 'baseline_loss': 0.43752339482307434, 'total_loss': -0.13718949258327484}\n",
      "2021-09-07 17:18:56.432 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2813093364238739\n",
      "2021-09-07 17:18:56.433 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.09704984724521637\n",
      "2021-09-07 17:18:56.434 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2813093364238739\n",
      "2021-09-07 17:18:56.435 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.09704984724521637\n",
      "2021-09-07 17:18:56.436 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:56.437 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.3517134189605713, 'baseline_loss': 0.48333457112312317, 'total_loss': -0.1100461333990097}\n",
      "2021-09-07 17:18:56.438 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.6089566946029663\n",
      "2021-09-07 17:18:56.439 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.6673921346664429\n",
      "2021-09-07 17:18:56.440 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:56.441 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999919533729553\n",
      "2021-09-07 17:18:56.442 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:56.444 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.32960984110832214, 'baseline_loss': 0.47700825333595276, 'total_loss': -0.09110571444034576}\n",
      "2021-09-07 17:18:56.445 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.17798446118831635\n",
      "2021-09-07 17:18:56.446 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.2107767015695572\n",
      "2021-09-07 17:18:56.447 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.17798446118831635\n",
      "2021-09-07 17:18:56.448 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.2107767015695572\n",
      "2021-09-07 17:18:56.449 | INFO     | src.policies:train:123 - Epoch 799 / 800\n",
      "2021-09-07 17:18:56.450 | INFO     | src.policies:collect_trajectories:221 - Episode 1678\n",
      "2021-09-07 17:18:56.480 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:56.481 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 192.0\n",
      "2021-09-07 17:18:56.481 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 192.0\n",
      "2021-09-07 17:18:56.482 | INFO     | src.policies:collect_trajectories:221 - Episode 1679\n",
      "2021-09-07 17:18:56.501 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:56.502 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 124.0\n",
      "2021-09-07 17:18:56.503 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 158.0\n",
      "2021-09-07 17:18:56.504 | WARNING  | src.policies:train:144 - The actual batch size is 316, instead of 200\n",
      "2021-09-07 17:18:56.506 | INFO     | src.policies:train:159 - Mini-batch 1 / 3\n",
      "2021-09-07 17:18:56.508 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.49501368403434753, 'baseline_loss': 0.8430556654930115, 'total_loss': -0.0734858512878418}\n",
      "2021-09-07 17:18:56.509 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.2431783229112625\n",
      "2021-09-07 17:18:56.510 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.1966025829315186\n",
      "2021-09-07 17:18:56.511 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.2431783229112625\n",
      "2021-09-07 17:18:56.512 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4999995231628418\n",
      "2021-09-07 17:18:56.514 | INFO     | src.policies:train:159 - Mini-batch 2 / 3\n",
      "2021-09-07 17:18:56.515 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4816906452178955, 'baseline_loss': 0.7322647571563721, 'total_loss': -0.11555826663970947}\n",
      "2021-09-07 17:18:56.516 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18325592577457428\n",
      "2021-09-07 17:18:56.517 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.7449561953544617\n",
      "2021-09-07 17:18:56.518 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18325592577457428\n",
      "2021-09-07 17:18:56.519 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999937415122986\n",
      "2021-09-07 17:18:56.520 | INFO     | src.policies:train:159 - Mini-batch 3 / 3\n",
      "2021-09-07 17:18:56.521 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5660153031349182, 'baseline_loss': 0.9694736003875732, 'total_loss': -0.08127850294113159}\n",
      "2021-09-07 17:18:56.522 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.5877996683120728\n",
      "2021-09-07 17:18:56.523 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 1.3771469593048096\n",
      "2021-09-07 17:18:56.524 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.49999910593032837\n",
      "2021-09-07 17:18:56.525 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.49999964237213135\n",
      "2021-09-07 17:18:56.526 | INFO     | src.policies:train:123 - Epoch 800 / 800\n",
      "2021-09-07 17:18:56.527 | INFO     | src.policies:collect_trajectories:221 - Episode 1680\n",
      "2021-09-07 17:18:56.544 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:56.545 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 108.0\n",
      "2021-09-07 17:18:56.545 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 108.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-07 17:18:56.546 | INFO     | src.policies:collect_trajectories:221 - Episode 1681\n",
      "2021-09-07 17:18:56.614 | DEBUG    | src.policies:execute_episode:413 - Early stopping, all agents done\n",
      "2021-09-07 17:18:56.614 | INFO     | src.policies:collect_trajectories:237 - Mean episode return: 172.0\n",
      "2021-09-07 17:18:56.615 | INFO     | src.policies:collect_trajectories:238 - Last 100 episodes mean return: 140.0\n",
      "2021-09-07 17:18:56.615 | WARNING  | src.policies:train:144 - The actual batch size is 280, instead of 200\n",
      "2021-09-07 17:18:56.618 | INFO     | src.policies:train:159 - Mini-batch 1 / 2\n",
      "2021-09-07 17:18:56.620 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.4010089039802551, 'baseline_loss': 0.7162554264068604, 'total_loss': -0.04288119077682495}\n",
      "2021-09-07 17:18:56.621 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.18547064065933228\n",
      "2021-09-07 17:18:56.622 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.4452331066131592\n",
      "2021-09-07 17:18:56.623 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.18547064065933228\n",
      "2021-09-07 17:18:56.624 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.4452331066131592\n",
      "2021-09-07 17:18:56.625 | INFO     | src.policies:train:159 - Mini-batch 2 / 2\n",
      "2021-09-07 17:18:56.626 | INFO     | src.policies:minibatch_update:281 - Losses: {'policy_loss': -0.5231190323829651, 'baseline_loss': 0.7085955739021301, 'total_loss': -0.16882124543190002}\n",
      "2021-09-07 17:18:56.627 | INFO     | src.policies:minibatch_update:287 - Policy network L2 gradient norm: 0.48017382621765137\n",
      "2021-09-07 17:18:56.628 | INFO     | src.policies:minibatch_update:291 - Baseline network L2 gradient norm: 0.32694733142852783\n",
      "2021-09-07 17:18:56.628 | INFO     | src.policies:minibatch_update:298 - Policy network L2 gradient norm after clipping: 0.48017382621765137\n",
      "2021-09-07 17:18:56.630 | INFO     | src.policies:minibatch_update:305 - Baseline network L2 gradient norm after clipping: 0.32694733142852783\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 39960<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/Users/jobs/Github/cpr-appropriation/notebooks/wandb/run-20210907_171749-fotj8n3u/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/Users/jobs/Github/cpr-appropriation/notebooks/wandb/run-20210907_171749-fotj8n3u/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>-0.2117</td></tr><tr><td>mean_return</td><td>140.0</td></tr><tr><td>_runtime</td><td>61</td></tr><tr><td>_timestamp</td><td>1631027936</td></tr><tr><td>_step</td><td>799</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>█▆▄▅▅▇▃▅▃▃▃█▃▄▃▅▅▃▂█▅▃▁▆▆▄▇▅▄▄▃▇▄▂▄▂▂▄▄▂</td></tr><tr><td>mean_return</td><td>▁▁▁▂▂█▆▅▄▇▅▆█▇▆█████▆██▇▆██▆████████▅██▆</td></tr><tr><td>_runtime</td><td>▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">peach-salad-89</strong>: <a href=\"https://wandb.ai/wadaboa/cpr-appropriation/runs/fotj8n3u\" target=\"_blank\">https://wandb.ai/wadaboa/cpr-appropriation/runs/fotj8n3u</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vpg_policy_nn = models.MLP(observation_space_size, hidden_sizes, action_space_size)\n",
    "vpg_baseline_nn = models.MLP(observation_space_size, hidden_sizes, 1, log_softmax=False)\n",
    "vpg_policy = policies.VPGPolicy(env, vpg_policy_nn, baseline_nn=vpg_baseline_nn)\n",
    "vpg_policy.train(\n",
    "    epochs,\n",
    "    steps_per_epoch,\n",
    "    minibatch_size,\n",
    "    enable_wandb=True,\n",
    "    wandb_config={**wandb_config, \"group\": \"VPG\"},\n",
    "    episodes_mean_return=episodes_mean_return\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ea7aa",
   "metadata": {},
   "source": [
    "## TRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5756d0",
   "metadata": {},
   "source": [
    "This section deals with training a Cartpole agent using our custom Trust Region Policy Optimization implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdddef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1.0\n",
    "kl_target = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f959403",
   "metadata": {},
   "outputs": [],
   "source": [
    "trpo_policy_nn = models.MLP(observation_space_size, hidden_sizes, action_space_size)\n",
    "trpo_baseline_nn = models.MLP(observation_space_size, hidden_sizes, 1, log_softmax=False)\n",
    "trpo_policy = policies.TRPOPolicy(env, trpo_policy_nn, trpo_baseline_nn, beta=beta, kl_target=kl_target)\n",
    "trpo_policy.train(\n",
    "    epochs,\n",
    "    steps_per_epoch,\n",
    "    minibatch_size,\n",
    "    enable_wandb=True,\n",
    "    wandb_config={**wandb_config, \"group\": \"TRPO\"},\n",
    "    episodes_mean_return=episodes_mean_return\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34632f1f",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc40880f",
   "metadata": {},
   "source": [
    "This section deals with training a Cartpole agent using our custom Proximal Policy Optimization implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51783ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "beta = 0.01\n",
    "eps = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e1efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_policy_nn = models.MLP(observation_space_size, hidden_sizes, action_space_size)\n",
    "ppo_baseline_nn = models.MLP(observation_space_size, hidden_sizes, 1, log_softmax=False)\n",
    "ppo_policy = policies.PPOPolicy(env, ppo_policy_nn, ppo_baseline_nn, alpha=alpha, beta=beta, eps=eps)\n",
    "ppo_policy.train(\n",
    "    epochs,\n",
    "    steps_per_epoch,\n",
    "    minibatch_size,\n",
    "    enable_wandb=True,\n",
    "    wandb_config={**wandb_config, \"group\": \"PPO\"},\n",
    "    episodes_mean_return=episodes_mean_return\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
